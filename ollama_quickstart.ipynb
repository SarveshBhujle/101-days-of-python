{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two numbers\n",
    "\n",
    "    Args:\n",
    "      a: The first integer number\n",
    "      b: The second integer number\n",
    "\n",
    "    Returns:\n",
    "      int: The sum of the two numbers\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "response = ollama.chat(\n",
    "    \"llama3.1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is 10 + 10?\"}],\n",
    "    # tools=[add_two_numbers],  # Actual function reference\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Listing available models...\")\n",
    "print(ollama.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You are an intelligent assistant. You are helping the user with their query.\"\n",
    ")\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 100\n",
    "DEFAULT_OLLAMA_MODEL = \"llama3.1\"\n",
    "DEFAULT_VERBOSE = True\n",
    "DEFAULT_DEBUG = True\n",
    "\n",
    "\n",
    "def generate_object(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=DEFAULT_SYSTEM_PROMPT,\n",
    "    model=DEFAULT_OLLAMA_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    "    max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    debug=DEFAULT_DEBUG,\n",
    "    verbose=DEFAULT_VERBOSE,\n",
    ") -> Union[BaseModel, None]:\n",
    "    \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "    try:\n",
    "        if verbose or debug:\n",
    "            print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "        prompt_with_structured_output = f\"\"\"\n",
    "            Prompt: {prompt} \n",
    "\n",
    "            Response Schema: '''{response_model.model_json_schema()}'''\n",
    "\n",
    "            INSTRUCTIONS:\n",
    "            - STRICTLY RESPOND IN THE GIVEN SCHEMA FORMAT.\n",
    "            - IT IS MANDATORY TO FOLLOW THE SCHEMA.\n",
    "            - MAKE SURE TO FILL IN ALL THE REQUIRED FIELDS.\n",
    "            - DON'T HALLUCINATE OR MAKE UP ANY INFORMATION.\n",
    "            - RESPOND ONLY WITH VALID JSON AND DON'T INCLUDE ANYTHING ELSE.\n",
    "        \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            params = {\n",
    "                \"prompt\": prompt_with_structured_output,\n",
    "                \"system\": system,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"model\": model,\n",
    "            }\n",
    "            params = json.dumps(params, indent=2)\n",
    "            print(f\"Params: {params}\")\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        response_json = response.message.content  # Get the response content\n",
    "        print(response_json)\n",
    "        response_obj = json.loads(\n",
    "            response_json\n",
    "        )  # Convert the response to a Python object\n",
    "\n",
    "        response_structured = response_model.model_validate(\n",
    "            response_obj\n",
    "        )  # Parse the response to the response model\n",
    "\n",
    "        if verbose or debug:\n",
    "            print(\"Object generated successfully. ðŸŽ‰\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"EasyLLM Response: {response_json}\")\n",
    "        return response_structured\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "        if debug:\n",
    "            traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str = (\n",
    "        \"Why did the scarecrow win an award? Because he was outstanding in his field.\"\n",
    "    )\n",
    "    author: str = \"Unknown\"\n",
    "\n",
    "\n",
    "response = generate_object(\"Tell me a joke\", response_model=Joke)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "    author: str\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\"Tell me a joke. \n",
    "Respond with the schema: {Joke.model_json_schema()}\"\n",
    "Include only the keys and values and not the schema itself.\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    \"phi4\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Tell me a joke. Respond with the schema: {Joke.model_json_schema()}\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str = \"A joke\"\n",
    "    author: str = \"An author\"\n",
    "\n",
    "\n",
    "def build_dummy_pydantic_object(schema: BaseModel) -> BaseModel:\n",
    "    \"\"\"\n",
    "    Build a dummy Pydantic object using the given schema.\n",
    "\n",
    "    Args:\n",
    "      schema: The Pydantic schema to build the object from\n",
    "\n",
    "    Returns:\n",
    "      BaseModel: The dummy Pydantic object\n",
    "    \"\"\"\n",
    "    return schema()\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\"Tell me a joke. \n",
    "Respond with the schema â€” Example object: {build_dummy_pydantic_object().model_dump_json()}\"\n",
    "Include only the keys and values and not the schema itself.\n",
    "Respond strictly in the given schema format.\n",
    "Respond only with valid JSON and don't include anything else.\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\"llama3.1\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
