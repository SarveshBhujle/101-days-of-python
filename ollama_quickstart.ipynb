{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set.\n",
      "ðŸš€ Setup complete. Continue to the next cell.\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"ðŸš€ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"llama3.1\" not found, try pulling it first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    Add two numbers\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m      int: The sum of the two numbers\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is 10 + 10?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43madd_two_numbers\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Actual function reference\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mtool_calls:\n\u001b[1;32m     26\u001b[0m     name \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mtool_calls[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtool_name\n",
      "File \u001b[0;32m~/thehackersplaybook/engineering/.venv/lib/python3.12/site-packages/ollama/_client.py:332\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    289\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    290\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    298\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thehackersplaybook/engineering/.venv/lib/python3.12/site-packages/ollama/_client.py:177\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/thehackersplaybook/engineering/.venv/lib/python3.12/site-packages/ollama/_client.py:122\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m   r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mResponseError\u001b[0m: model \"llama3.1\" not found, try pulling it first"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two numbers\n",
    "\n",
    "    Args:\n",
    "      a: The first integer number\n",
    "      b: The second integer number\n",
    "\n",
    "    Returns:\n",
    "      int: The sum of the two numbers\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "response = ollama.chat(\n",
    "    \"llama3.1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is 10 + 10?\"}],\n",
    "    tools=[add_two_numbers],  # Actual function reference\n",
    ")\n",
    "\n",
    "\n",
    "if response.message.tool_calls:\n",
    "    name = response.message.tool_calls[0].tool_name\n",
    "    print(f\"ðŸ¦™ The tool name is {name}\")\n",
    "    args = response.message.tool_calls[0].args\n",
    "\n",
    "    if args:\n",
    "        print(f\"ðŸ¦™ The arguments are {args}\")\n",
    "\n",
    "\n",
    "print(response, response.message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing available models...\n",
      "[('models', [Model(model='phi4:latest', modified_at=datetime.datetime(2025, 1, 10, 7, 37, 37, 424606, tzinfo=TzInfo(+05:30)), digest='ac896e5b8b34a1f4efa7b14d7520725140d5512484457fab45d2a4ea14c69dba', size=9053116391, details=ModelDetails(parent_model='', format='gguf', family='phi3', families=['phi3'], parameter_size='14.7B', quantization_level='Q4_K_M')), Model(model='llama3.1:70b', modified_at=datetime.datetime(2025, 1, 9, 13, 44, 41, 118933, tzinfo=TzInfo(+05:30)), digest='711a9e8463afd8edd580debd3b5c521521ebe55ba95bb80d576f4149969e07c6', size=42520412561, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='70.6B', quantization_level='Q4_K_M'))])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Listing available models...\")\n",
    "\n",
    "pretty_formatted_list = list(ollama.list())\n",
    "models = pretty_formatted_list[0][0]\n",
    "print(pretty_formatted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You are an intelligent assistant. You are helping the user with their query.\"\n",
    ")\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 100\n",
    "DEFAULT_OLLAMA_MODEL = \"phi4\"\n",
    "DEFAULT_VERBOSE = True\n",
    "DEFAULT_DEBUG = True\n",
    "\n",
    "\n",
    "def build_dummy_pydantic_object(schema: BaseModel) -> BaseModel:\n",
    "    \"\"\"\n",
    "    Build a dummy Pydantic object using the given schema.\n",
    "\n",
    "    Args:\n",
    "      schema: The Pydantic schema to build the object from\n",
    "\n",
    "    Returns:\n",
    "      BaseModel: The dummy Pydantic object\n",
    "    \"\"\"\n",
    "    return schema()\n",
    "\n",
    "\n",
    "def generate_object(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=DEFAULT_SYSTEM_PROMPT,\n",
    "    model=DEFAULT_OLLAMA_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    "    max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    debug=DEFAULT_DEBUG,\n",
    "    verbose=DEFAULT_VERBOSE,\n",
    ") -> Union[BaseModel, None]:\n",
    "    \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "    try:\n",
    "        if verbose or debug:\n",
    "            print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "        prompt_with_structured_output = f\"\"\"\n",
    "            Prompt: {prompt} \n",
    "            SCHEMA: {build_dummy_pydantic_object(response_model).model_dump_json()}\n",
    "            RESPOND IN JSON FORMAT\n",
    "        \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            params = {\n",
    "                \"prompt\": prompt_with_structured_output,\n",
    "                \"system\": system,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"model\": model,\n",
    "            }\n",
    "            params = json.dumps(params, indent=2)\n",
    "            print(f\"Params: {params}\")\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            format=\"json\",\n",
    "        )\n",
    "\n",
    "        response_json = response.message.content  # Get the response content\n",
    "        print(response_json)\n",
    "        response_obj = json.loads(response_json)\n",
    "        response_structured = response_model.model_validate(response_obj)\n",
    "\n",
    "        if verbose or debug:\n",
    "            print(\"Object generated successfully. ðŸŽ‰\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"EasyLLM Response: {response_json}\")\n",
    "        return response_structured\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "        if debug:\n",
    "            traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating object for prompt: Tell me a joke\n",
      "Params: {\n",
      "  \"prompt\": \"\\n            Prompt: Tell me a joke \\n            SCHEMA: {\\\"joke\\\":\\\"Why did the scarecrow win an award? Because he was outstanding in his field.\\\",\\\"author\\\":\\\"Unknown\\\"}\\n            RESPOND IN JSON FORMAT\\n        \",\n",
      "  \"system\": \"You are an intelligent assistant. You are helping the user with their query.\",\n",
      "  \"temperature\": 0.5,\n",
      "  \"max_tokens\": 100,\n",
      "  \"model\": \"phi4\"\n",
      "}\n",
      "{ \"joke\": \"Why don't scientists trust atoms? Because they make up everything!\" }\n",
      "\n",
      "\n",
      "Object generated successfully. ðŸŽ‰\n",
      "EasyLLM Response: { \"joke\": \"Why don't scientists trust atoms? Because they make up everything!\" }\n",
      "\n",
      "\n",
      "joke=\"Why don't scientists trust atoms? Because they make up everything!\" author='Unknown'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str = (\n",
    "        \"Why did the scarecrow win an award? Because he was outstanding in his field.\"\n",
    "    )\n",
    "    author: str = \"Unknown\"\n",
    "\n",
    "\n",
    "response = generate_object(\"Tell me a joke\", response_model=Joke)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating object for prompt: \n",
      "\"Tell me a joke. \n",
      "Respond with the schema: {'properties': {'joke': {'title': 'Joke', 'type': 'string'}, 'author': {'title': 'Author', 'type': 'string'}}, 'required': ['joke', 'author'], 'title': 'Joke', 'type': 'object'}\"\n",
      "Include only the keys and values and not the schema itself.\n",
      "\n",
      "Params: {\n",
      "  \"prompt\": \"\\n            Prompt: \\n\\\"Tell me a joke. \\nRespond with the schema: {'properties': {'joke': {'title': 'Joke', 'type': 'string'}, 'author': {'title': 'Author', 'type': 'string'}}, 'required': ['joke', 'author'], 'title': 'Joke', 'type': 'object'}\\\"\\nInclude only the keys and values and not the schema itself.\\n \\n            RESPOND IN JSON FORMAT\\n        \",\n",
      "  \"system\": \"You are an intelligent assistant. You are helping the user with their query.\",\n",
      "  \"temperature\": 0.5,\n",
      "  \"max_tokens\": 100,\n",
      "  \"model\": \"phi4\"\n",
      "}\n",
      "Failed to generate object. Error: Client.chat() got an unexpected keyword argument 'response_json'\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/sv/fnf9bf6x545974x7sdxqx8p80000gn/T/ipykernel_22671/3006045095.py\", line 60, in generate_object\n",
      "    response = ollama.chat(\n",
      "               ^^^^^^^^^^^^\n",
      "TypeError: Client.chat() got an unexpected keyword argument 'response_json'\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "    author: str\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\"Tell me a joke. \n",
    "Respond with the schema: {Joke.model_json_schema()}\"\n",
    "Include only the keys and values and not the schema itself.\n",
    "\"\"\"\n",
    "\n",
    "response = generate_object(prompt, response_model=Joke)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"joke\":\"Why don't scientists trust atoms? Because they make up everything!\",\"author\":\"Unknown\"}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str = \"A joke\"\n",
    "    author: str = \"An author\"\n",
    "\n",
    "\n",
    "def build_dummy_pydantic_object(schema: BaseModel) -> BaseModel:\n",
    "    \"\"\"\n",
    "    Build a dummy Pydantic object using the given schema.\n",
    "\n",
    "    Args:\n",
    "      schema: The Pydantic schema to build the object from\n",
    "\n",
    "    Returns:\n",
    "      BaseModel: The dummy Pydantic object\n",
    "    \"\"\"\n",
    "    return schema()\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\"Tell me a joke. \n",
    "Respond with the schema â€” Example object: {build_dummy_pydantic_object(Joke).model_dump_json()}\"\n",
    "Include only the keys and values and not the schema itself.\n",
    "Respond strictly in the given schema format.\n",
    "Respond only with valid JSON and don't include anything else.\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
