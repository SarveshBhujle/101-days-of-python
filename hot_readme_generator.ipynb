{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hot Readme Generator (WIP)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://languageicon.org/language-icon.png\"> \n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "## Description:\n",
    "\n",
    "This app automates the generation of structured README files by creating a table of contents, integrating badges, and providing customizable configurations for headers, sections, and taglines. It uses a large language model (LLM) to tailor the content to specific project needs and ensures optimal formatting and professionalism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Installation\n",
    "\n",
    "The first step in any project is setting up the environment. Here, we install the necessary requirements and check if the environment variables are properly configured.\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "- **install_requirements()**: This function ensures that all required Python packages are installed by attempting to install the packages listed in the `requirements.txt`. If the installation fails, it retries up to three times to ensure success.\n",
    "\n",
    "- **setup_env()**: This function loads environment variables from a `.env` file using the `load_dotenv()` method. It then checks if the required environment variables, such as `OPENAI_API_KEY`, are properly set. If any required variables are missing, the code prompts the user to set them.\n",
    "\n",
    "#### Key Libraries:\n",
    "\n",
    "- **dotenv**: Loads environment variables from a `.env` file to configure settings for the project.\n",
    "\n",
    "- **os**: Allows access to system functions and environment variables, helping the code to verify and load required configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"🚀 Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding the EasyLLM Class and Its Methods\n",
    "\n",
    "### 1. Import Statements\n",
    "\n",
    "- **os**: Allows interaction with the operating system, mainly used here for environment variables.\n",
    "\n",
    "- **openai**: The OpenAI Python client library, used to interact with OpenAI's GPT models.\n",
    "\n",
    "- **BaseModel from pydantic**: This is used to define data models with validation, making it easier to work with structured data.\n",
    "\n",
    "- **traceback**: Helps with printing detailed error stack traces for debugging.\n",
    "\n",
    "- **Union from typing**: A way to specify that a value could be one of several types.\n",
    "\n",
    "- **json**: Used to parse and serialize JSON data.\n",
    "\n",
    "- **re**: Provides regular expression matching operations, used here to estimate tokens.\n",
    "\n",
    "\n",
    "### 2. Constants Definition\n",
    "\n",
    "- **DEFAULT_OPENAI_MODEL**: Specifies the default OpenAI model to use (gpt-4o-mini).\n",
    "\n",
    "- **DEFAULT_SYSTEM_PROMPT**: The default system message that instructs the AI model on how to behave during a conversation.\n",
    "\n",
    "- **DEFAULT_TEMPERATURE**: Controls the randomness of the AI’s output (higher values make the output more random).\n",
    "\n",
    "- **DEFAULT_MAX_TOKENS**: Sets the maximum number of tokens (words or pieces of words) the model can generate in response.\n",
    "\n",
    "\n",
    "### 3. EasyLLM Class\n",
    "\n",
    "This is the main class that wraps the OpenAI API, simplifying its usage. The class provides methods to generate text and objects, interact with OpenAI’s models, and handle common tasks like estimating token usage.\n",
    "\n",
    "### 4. `__init__()` Method (Constructor)\n",
    "\n",
    "- **api_key**: The API key used to authenticate with OpenAI, fetched from environment variables (os.getenv).\n",
    "\n",
    "- **model**: The OpenAI model to use, with a default set to gpt-4o-mini.\n",
    "\n",
    "- **verbose**: Flag to control the level of logging output.\n",
    "\n",
    "- **debug**: Flag to enable or disable debugging information.\n",
    "\n",
    "\n",
    "This constructor initializes the class by setting up the OpenAI client and model, as well as handling logging based on the verbose and debug flags.\n",
    "\n",
    "**Key steps:**\n",
    "\n",
    "- Initializes the openai object using the provided api_key.\n",
    "- Sets the model to the provided value (or the default).\n",
    "- Prints log messages if verbose is True.\n",
    "\n",
    "### 5. `generate_text()` Method\n",
    "\n",
    "- **prompt**: The input text prompt to send to the model.\n",
    "\n",
    "- **system**: The system message that guides the AI's behavior (defaults to the predefined prompt).\n",
    "\n",
    "- **temperature**: Controls the randomness of the model’s response.\n",
    "\n",
    "- **max_tokens**: Specifies the maximum number of tokens to generate in the response.\n",
    "\n",
    "\n",
    "This method sends a request to OpenAI's API to generate a text response based on the provided prompt and settings.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "- Constructs a params dictionary with the input settings and converts it to JSON format if debug is enabled.\n",
    "\n",
    "- Calls OpenAI’s chat.completions.create() function to generate a response based on the provided parameters.\n",
    "\n",
    "- Returns the response text if successful, otherwise, it returns None if an error occurs.\n",
    "\n",
    "\n",
    "### 6. `generate_object()` Method\n",
    "\n",
    "- **prompt**: The input prompt to be processed by the model.\n",
    "\n",
    "- **response_model**: A Pydantic model used to parse and validate the response.\n",
    "\n",
    "- **system, temperature, max_tokens**: Same as in generate_text.\n",
    "\n",
    "\n",
    "This method is similar to generate_text() but returns the response in the form of a structured Python object based on the provided response_model.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "- Similar to generate_text(), it constructs the request and calls OpenAI's API.\n",
    "\n",
    "- Parses the response using response_model, which is a Pydantic model that validates the data.\n",
    "\n",
    "- Returns the parsed response or None in case of an error.\n",
    "\n",
    "\n",
    "### 7. `get_model()` Method\n",
    "\n",
    "This method simply returns the name of the current OpenAI model being used.\n",
    "\n",
    "### 8. `set_model()` Method\n",
    "\n",
    "- **model**: The new model to be used.\n",
    "\n",
    "This method allows you to change the model being used by the EasyLLM instance. It reinitializes the OpenAI client with the new model and updates the internal model attribute.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "- Attempts to set the new model and handle errors if any.\n",
    "\n",
    "- Prints logs if verbose or debug is enabled.\n",
    "\n",
    "\n",
    "### 9. `estimate_tokens()` Method\n",
    "\n",
    "- **text**: The input text whose tokens need to be estimated.\n",
    "\n",
    "This method estimates the number of tokens in a given string of text. It uses regular expressions to count the tokens by splitting the text at spaces and punctuation.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "- The `re.findall(r\"\\S+\", text)` function matches all non-whitespace sequences, effectively counting tokens in the text.\n",
    "\n",
    "- Returns the number of tokens found.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "This EasyLLM class is a Python wrapper around OpenAI's GPT API that simplifies interactions with the model. It includes methods for generating text, handling responses as objects, managing model selection, and estimating token usage.\n",
    "\n",
    "By using EasyLLM, you can easily interact with OpenAI's GPT models without needing to manually handle API requests, set up system prompts, or process model responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "import traceback\n",
    "from typing import Union\n",
    "import json\n",
    "import re\n",
    "\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are an intelligent AI assistant. The user will give you a prompt, respond appropriately.\"\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "class EasyLLM:\n",
    "    \"\"\"\n",
    "    A simple abstraction for the OpenAI API. It provides easy-to-use methods to generate text and objects using the OpenAI API.\n",
    "    A demonstration for the \"How to build an Abstaction with Open AI API\" blog post.\n",
    "    Author: Aditya Patange (AdiPat)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model=DEFAULT_OPENAI_MODEL,\n",
    "        verbose=True,\n",
    "        debug=True,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"EasyLLM: Powering up! 🚀\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.openai = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"EasyLLM: Model set to {model}.\")\n",
    "            print(\"EasyLLM: Ready for some Generative AI action! ⚡️\")\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[str, None]:\n",
    "        \"\"\"Generates text using the OpenAI API.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating text for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "            response = self.openai.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            response = response.choices[0].message.content\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Text generated successfully. 🎉\")\n",
    "\n",
    "            if self.debug:\n",
    "                response = json.dumps(response)\n",
    "                print(f\"EasyLLM Response: {response}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate text. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def generate_object(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_model: BaseModel,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[BaseModel, None]:\n",
    "        \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "\n",
    "            response = self.openai.beta.chat.completions.parse(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format=response_model,\n",
    "                model=self.model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Object generated successfully. 🎉\")\n",
    "\n",
    "            if self.debug:\n",
    "                response_json = response.model_dump_json()\n",
    "                print(f\"EasyLLM Response: {response_json}\")\n",
    "            return response.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def get_model(self) -> str:\n",
    "        \"\"\"Gets the current model.\"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def set_model(self, model: str) -> None:\n",
    "        \"\"\"Sets the model to the given model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Setting model to {model}\")\n",
    "            self.openai = openai.OpenAI(api_key=self.api_key)\n",
    "            self.model = model\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Model set to {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to set model.\\nError: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Estimate the number of tokens in a given string.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input string.\n",
    "\n",
    "        Returns:\n",
    "            int: Estimated token count.\n",
    "        \"\"\"\n",
    "        # Split text by whitespace and count tokens, including punctuation\n",
    "        tokens = re.findall(r\"\\S+\", text)\n",
    "        return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Classes, Models, and Functions for README Generation\n",
    "\n",
    "### 1. Classes and Models\n",
    "\n",
    "#### Project Class:\n",
    "\n",
    "The Project class is defined using Pydantic's BaseModel. This model is used to structure project-related data like:\n",
    "\n",
    "- repo_name: The repository name of the project.\n",
    "\n",
    "- title: The title of the project.\n",
    "\n",
    "- description: A short description of the project.\n",
    "\n",
    "- contributors: A list of contributors to the project.\n",
    "\n",
    "- references: External references for the project (e.g., articles, research papers).\n",
    "\n",
    "- tech_stack: A list of technologies used in the project.\n",
    "\n",
    "- tags: Keywords associated with the project.\n",
    "\n",
    "- license: The license type of the project.\n",
    "\n",
    "### Sections Class:\n",
    "\n",
    "The Sections class, also based on Pydantic, represents a list of sections in a README file:\n",
    "\n",
    "- sections: A list of strings representing the different sections of the README.\n",
    "\n",
    "### 2. Functions and Their Purpose\n",
    "\n",
    "#### `extract_badges_from_markdown`\n",
    "\n",
    "**Purpose**: This function extracts badges and their URLs from the given markdown content.\n",
    "\n",
    "**Arguments**:  \n",
    "- markdown_text (str): The markdown content (typically from a README file) containing badges.\n",
    "\n",
    "**Returns**:  \n",
    "- A dictionary where the keys are badge names and the values are URLs.\n",
    "\n",
    "**How it works**:  \n",
    "The function uses a regular expression to find all badge links in the markdown text. It matches the pattern of badges formatted as ![badge-name](url).  \n",
    "\n",
    "The function then returns a dictionary with badge names as keys and corresponding URLs as values.\n",
    "\n",
    "#### `get_badges`\n",
    "\n",
    "**Purpose**: Fetches the badges markdown from an online source.\n",
    "\n",
    "**Arguments**:  \n",
    "- None\n",
    "\n",
    "**Returns**:  \n",
    "- A dictionary of badge names and their URLs.\n",
    "\n",
    "**How it works**:  \n",
    "The function fetches markdown content from a public GitHub repository that contains badges.  \n",
    "\n",
    "The `extract_badges_from_markdown` function is then called to extract badges and their URLs from this markdown content.\n",
    "\n",
    "#### `get_table_of_contents`\n",
    "\n",
    "**Purpose**: Generates the table of contents (ToC) for a project's README.\n",
    "\n",
    "**Arguments**:  \n",
    "- llm (EasyLLM): An instance of the EasyLLM class for generating sections dynamically.\n",
    "\n",
    "- project (Project): An instance of the Project class containing project details.\n",
    "\n",
    "**Returns**:  \n",
    "- A list of sections that should be included in the README's table of contents.\n",
    "\n",
    "**How it works**:  \n",
    "The function starts by defining mandatory sections (e.g., \"About\", \"Problem Statement\", \"License\") that must be included in every README.  \n",
    "\n",
    "It then generates new, unique sections for the README by prompting the EasyLLM class to generate content based on the project details.  \n",
    "\n",
    "The generated sections are added to the table of contents, ensuring no duplication of mandatory sections.\n",
    "\n",
    "#### `get_readme_generator_config`\n",
    "\n",
    "**Purpose**: Returns the configuration for generating a README with specific visual sections.\n",
    "\n",
    "**Arguments**:  \n",
    "- title (str): The title of the project.\n",
    "\n",
    "**Returns**:  \n",
    "- A dictionary containing the configuration for generating the README header and table of contents.\n",
    "\n",
    "**How it works**:  \n",
    "This function defines a README_GENERATOR_CONFIG dictionary containing prompts for generating:  \n",
    "\n",
    "- A project header section with a title, badges, a message to encourage stars on GitHub, and a marketing tagline.\n",
    "\n",
    "- A table of contents with mandatory sections like \"About\", \"License\", \"Installation\", and more.\n",
    "\n",
    "The function returns this configuration to guide the README generation process.\n",
    "\n",
    "### 3. Integration and Workflow\n",
    "\n",
    "The entire setup leverages the EasyLLM instance to dynamically generate parts of the README (like the table of contents and specific sections) based on the project details.\n",
    "\n",
    "### How it all ties together:\n",
    "\n",
    "- Badges are fetched and extracted from an external markdown source (via the `get_badges` function).\n",
    "\n",
    "- Table of contents for the README is dynamically generated by calling the `get_table_of_contents` function, which relies on the EasyLLM instance to add relevant sections specific to the project.\n",
    "\n",
    "- README generation configuration is prepared using `get_readme_generator_config`, including the title and badges.\n",
    "\n",
    "The code provides a structured and modular approach for generating well-documented, rich README files that are customized based on project details and contributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import requests\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Project(BaseModel):\n",
    "    repo_name: str\n",
    "    title: str\n",
    "    description: str\n",
    "    contributors: list[str]\n",
    "    references: list[str]\n",
    "    tech_stack: list[str]\n",
    "    tags: list[str]\n",
    "    license: str\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: list[str]\n",
    "\n",
    "\n",
    "def extract_badges_from_markdown(markdown_text: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract badges and their URLs from the given markdown text.\n",
    "\n",
    "    Args:\n",
    "        markdown_text (str): The markdown content containing badges.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with badge names as keys and URLs as values.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\[!\\[([^\\]]+)\\]\\((https?://[^\\)]+)\\)\\]\"\n",
    "    matches = re.findall(pattern, markdown_text)\n",
    "    return {badge: url for badge, url in matches}\n",
    "\n",
    "\n",
    "def get_badges() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Get the markdown content of the README file containing badges.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with badge names as keys and URLs as values.\n",
    "\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/inttter/md-badges/refs/heads/main/README.md\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    badges_markdown = response.text\n",
    "    return extract_badges_from_markdown(badges_markdown)\n",
    "\n",
    "\n",
    "def get_table_of_contents(llm: EasyLLM, project: Project) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates the table of contents for the README.\n",
    "\n",
    "    Args:\n",
    "        llm (EasyLLM): The EasyLLM instance.\n",
    "        project (Project): The Project instance.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sections for the table of contents.\n",
    "    \"\"\"\n",
    "    mandatory_sections_first_half = [\n",
    "        \"About\",\n",
    "        \"Problem Statement\",\n",
    "        \"Research Areas\",\n",
    "        \"Installation\",\n",
    "        \"Usage\",\n",
    "    ]\n",
    "    mandatory_sections_second_half = [\n",
    "        \"Contributing\",\n",
    "        \"License\",\n",
    "        \"Acknowledgements\",\n",
    "        \"Authors\",\n",
    "        \"References\",\n",
    "    ]\n",
    "    table_of_contents = mandatory_sections_first_half\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Given the table of contents for the GitHub README of Project: '{project.title}' with the following sections:\n",
    "        Mandatory Sections: {json.dumps(mandatory_sections_first_half + mandatory_sections_second_half)}\n",
    "        These sections should be linked to the respective sections in the README.\n",
    "        These are mandatory sections for a good README.\n",
    "        You can add more sections if you want as per the project requirements.\n",
    "        For instance, a web scraping project would have \"AI & Web Scraping\" as a separate section. \n",
    "        Then, one of our projects, knowledge-grapher would have \"Knowledge Graphs in Practice\" and \"How Google uses Knowledge Graphs at scale\" as a separate section.\n",
    "\n",
    "        Generate any 3 new sections unique, and specific to the project.\n",
    "        Make sure you don't repeat the same sections and don't include the mandatory sections again.\n",
    "\n",
    "        Project Details: '''{project.model_dump_json()}''''\n",
    "    \"\"\"\n",
    "    sections = llm.generate_object(prompt, Sections)\n",
    "\n",
    "    if sections and len(sections.sections) > 0:\n",
    "        for section in sections.sections:\n",
    "            if section not in table_of_contents:\n",
    "                table_of_contents.append(section)\n",
    "\n",
    "    for section in mandatory_sections_second_half:\n",
    "        if section not in table_of_contents:\n",
    "            table_of_contents.append(section)\n",
    "\n",
    "    return table_of_contents\n",
    "\n",
    "\n",
    "def get_readme_generator_config(title: str) -> Dict[str, Any]:\n",
    "    README_GENERATOR_CONFIG = {\n",
    "        \"visual_config\": {\n",
    "            \"header\": {\n",
    "                \"prompt\": f\"\"\"\n",
    "                Generate a good header section for the README\n",
    "                1. TITLE: It should contain the title of the project with an emoji. \n",
    "                Example: `# 🚀 My Awesome Project`\n",
    "                Be creative with the emoji and title and try to ensure that the title is relevant to the project and README.\n",
    "                TITLE: {title}\n",
    "\n",
    "                2. BADGES: After the title add badges.\n",
    "                Available BadgesL {json.dumps(get_badges())}   \n",
    "\n",
    "                3. Message to star us on GitHub.\n",
    "                Example: Please give us a ⭐ on GitHub if this project helped you!\n",
    "\n",
    "                4. Marketing Tagline: Add a marketing tagline for the project.\n",
    "                Example: #### Simplify your ETL pipelines with LitETL 🔥 — the lightweight ETL framework!\n",
    "                \"\"\"\n",
    "            },\n",
    "            \"table_of_contents\": {\n",
    "                \"prompt\": \"\"\"\n",
    "                Generate a table of contents for the README.\n",
    "                This should include \"About\", \"Problem Statement\", \"Research Areas\", \"Installation\", \"Usage\", \"Contributing\", \"License\", \"Acknowledgements\", \"Authors\", \"References\".\n",
    "                These sections should be linked to the respective sections in the README.\n",
    "                These are mandatory sections for a good README.\n",
    "                You can add more sections if you want as per the project requirements.\n",
    "                For instance, a web scraping project would have \"AI & Web Scraping\" as a separate section. \n",
    "                Then, one of our projects, knowledge-grapher would have \"Knowledge Graphs in Practice\" and \"How Google uses Knowledge Graphs at scale\" as a separate section.\n",
    "                \"\"\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    return README_GENERATOR_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract and Process Badges for README\n",
    "\n",
    "### 1. Importing Dependencies\n",
    "\n",
    "- `import json`: The json module is used to handle JSON data, specifically for converting the badges dictionary into a well-formatted JSON string.\n",
    "\n",
    "### 2. Creating an Instance of EasyLLM\n",
    "\n",
    "- `llm = EasyLLM()`: An instance of the EasyLLM class is created, which will be used to estimate the number of tokens for the badges JSON data.\n",
    "\n",
    "### 3. Getting Badges\n",
    "\n",
    "- `badges = get_badges()`: This calls the `get_badges()` function defined earlier. It fetches the badge data from a specific markdown file and returns it as a dictionary, where each key is a badge name and each value is the corresponding URL.\n",
    "\n",
    "### 4. Converting Badges to JSON\n",
    "\n",
    "- `badges_json = json.dumps(badges, indent=2)`: The `json.dumps()` function converts the badges dictionary into a well-formatted JSON string. The `indent=2` argument ensures that the JSON is neatly indented for better readability.\n",
    "\n",
    "### 5. Displaying Badge Count\n",
    "\n",
    "- `print(f\"Badges found in the README: {len(badges)}\")`: This prints the number of badges extracted from the README file by displaying the length of the badges dictionary.\n",
    "\n",
    "### 6. Estimating Tokens\n",
    "\n",
    "- `print(f\"Tokens: {llm.estimate_tokens(badges_json)}\")`: The `estimate_tokens()` method of the EasyLLM instance is called to estimate how many tokens the badges JSON will consume in the context window of the language model. The result is printed.\n",
    "\n",
    "### 7. Token Estimation\n",
    "\n",
    "- The number of tokens represents the amount of processing the model needs to handle the data. Estimating tokens helps to avoid exceeding the model's context window, which could lead to truncation or errors.\n",
    "\n",
    "### 8. Printing the Badges in JSON Format\n",
    "\n",
    "- `print(badges_json)`: This prints the formatted JSON representation of the badges, allowing you to visually inspect the extracted badge data and their URLs.\n",
    "\n",
    "### **Expected Output**\n",
    "\n",
    "- **Badge Count**: The number of badges found in the README file will be displayed.\n",
    "\n",
    "- **Token Count**: The estimated number of tokens required to process the badges JSON data will be printed.\n",
    "\n",
    "- **Badges in JSON Format**: The badges, along with their respective URLs, will be displayed as a JSON string.\n",
    "\n",
    "### **Example Output**\n",
    "\n",
    "```plaintext\n",
    "Badges found in the README: 5\n",
    "Tokens: 120\n",
    "{\n",
    "  \"Build Status\": \"https://img.shields.io/badge/Build_Status-Passing-brightgreen\",\n",
    "  \n",
    "  \"License\": \"https://img.shields.io/badge/License-MIT-blue\",\n",
    "  \n",
    "  \"Version\": \"https://img.shields.io/badge/Version-1.0.0-blue\",\n",
    "  \n",
    "  \"Docs\": \"https://img.shields.io/badge/Docs-Available-brightgreen\",\n",
    "  \n",
    "  \"Downloads\": \"https://img.shields.io/badge/Downloads-1000%2B-orange\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "llm = EasyLLM()\n",
    "badges = get_badges()\n",
    "badges_json = json.dumps(badges, indent=2)\n",
    "\n",
    "\n",
    "print(f\"Badges found in the README: {len(badges)}\")\n",
    "# For estimating if it will fit in the LLM context window\n",
    "print(f\"Tokens: {llm.estimate_tokens(badges_json)}\")\n",
    "print(badges_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 5: Generate Table of Contents for README\n",
    "\n",
    "### 1. Importing clear_output from IPython\n",
    "\n",
    "- `from IPython.display import clear_output`: This import allows clearing the output of the Jupyter notebook cell. It's useful for cleaning up the console output before displaying new results.\n",
    "\n",
    "### 2. Generating Table of Contents\n",
    "\n",
    "- `table_of_contents = get_table_of_contents(...)`: The `get_table_of_contents()` function is called with the following parameters:\n",
    "\n",
    "  - `llm (EasyLLM instance)`: An instance of the EasyLLM class that helps generate sections for the ToC based on the project details.\n",
    "\n",
    "  - `Project`: A Project object created using the Project class (defined earlier). The Project is initialized with sample data such as the repository name, title, description, contributors, references, tech stack, tags, and license. This object is passed to the `get_table_of_contents()` function to help generate relevant sections for the table of contents.\n",
    "\n",
    "  - The `get_table_of_contents()` function uses this data to generate sections dynamically, considering both mandatory sections and additional sections specific to the project.\n",
    "\n",
    "### 3. Clearing Output\n",
    "\n",
    "- `clear_output()`: This function clears the output of the current Jupyter notebook cell. It is useful for resetting the output display before showing new or updated information, making the output look cleaner.\n",
    "\n",
    "### 4. Printing the Table of Contents\n",
    "\n",
    "- `print(f\"Table of Contents: {table_of_contents}\")`: After generating the table of contents, this line prints the resulting list of sections that will be included in the README file. These sections are typically linked in the document to facilitate navigation.\n",
    "\n",
    "### **Expected Output**\n",
    "\n",
    "- The code will generate a table of contents (ToC) that includes a combination of mandatory and project-specific sections. The output should look something like this:\n",
    "\n",
    "```plaintext\n",
    "Table of Contents: ['About', 'Problem Statement', 'Research Areas', 'Installation', 'Usage', 'Contributing', 'License', 'Acknowledgements', 'Authors', 'References', 'Additional Section 1', 'Additional Section 2', 'Additional Section 3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "table_of_contents = get_table_of_contents(\n",
    "    llm,\n",
    "    Project(\n",
    "        repo_name=\"test\",\n",
    "        title=\"Test\",\n",
    "        description=\"Test\",\n",
    "        contributors=[\"Adi\"],\n",
    "        references=[\"Ref\"],\n",
    "        tech_stack=[\"Python\"],\n",
    "        tags=[\"Test\"],\n",
    "        license=\"MIT\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"Table of Contents: {table_of_contents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Implementing the EasyLLM Class\n",
    "\n",
    "### Imports and Constants\n",
    "\n",
    "- ``import os, import openai, import traceback, import json, and import re``: These are necessary imports to handle environment variables, make requests to the OpenAI API, handle exceptions, process JSON data, and work with regular expressions.\n",
    "\n",
    "### DEFAULT Constants:\n",
    "\n",
    "- ``DEFAULT_OPENAI_MODEL``: Specifies the default model to use with the OpenAI API, in this case, \"gpt-4o-mini\".\n",
    "\n",
    "- ``DEFAULT_SYSTEM_PROMPT``: The system message that gives context to the AI model, which helps it understand the task. Here, it's set to guide the AI to behave as an intelligent assistant.\n",
    "\n",
    "- ``DEFAULT_TEMPERATURE``: Controls the randomness of the AI's responses. A temperature of 0.5 means it will generate moderately creative responses.\n",
    "\n",
    "- ``DEFAULT_MAX_TOKENS``: Sets the maximum number of tokens (words, punctuation, etc.) that the model will generate in its response, capped at 1024 tokens.\n",
    "\n",
    "### EasyLLM Class\n",
    "\n",
    "This class serves as an abstraction over the OpenAI API, making it easier to interact with the API and handle different tasks like generating text and estimating tokens.\n",
    "\n",
    "### Constructor (__init__)\n",
    "\n",
    "The constructor initializes the EasyLLM instance with several parameters, including the API key, model, and debug/verbose flags.\n",
    "\n",
    "- ``api_key=os.getenv(\"OPENAI_API_KEY\")``: Retrieves the OpenAI API key from the environment variable OPENAI_API_KEY.\n",
    "\n",
    "- ``self.openai = openai.OpenAI(api_key=api_key)``: Initializes the OpenAI object with the API key.\n",
    "\n",
    "- If ``verbose=True``, it prints messages to let the user know that the class is ready for use.\n",
    "\n",
    "### `generate_text` Method\n",
    "\n",
    "This method is responsible for generating text using the OpenAI API based on a prompt provided by the user.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- ``prompt``: The user’s input prompt that the model will respond to.\n",
    "\n",
    "- ``system``: A system prompt that sets the context for the model (default is ``DEFAULT_SYSTEM_PROMPT``).\n",
    "\n",
    "- ``temperature``: Determines the randomness of the response (default is 0.5).\n",
    "\n",
    "- ``max_tokens``: Maximum number of tokens for the model's response (default is 1024).\n",
    "\n",
    "### Flow:\n",
    "\n",
    "- First, it checks if ``verbose`` or ``debug`` is enabled to print the details of the prompt and parameters.\n",
    "\n",
    "- The ``openai.chat.completions.create()`` method is called to send the prompt to the API and generate a response.\n",
    "\n",
    "- The response content is extracted and returned.\n",
    "\n",
    "- If an error occurs, it catches the exception and prints an error message, as well as a traceback if debugging is enabled.\n",
    "\n",
    "### `generate_object` Method\n",
    "\n",
    "This method generates an object using the OpenAI API, but instead of raw text, it returns a parsed object according to a response model defined by the user.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- ``prompt``: The input prompt provided by the user.\n",
    "\n",
    "- ``response_model``: A Pydantic BaseModel class that defines the structure of the response object.\n",
    "\n",
    "- Other parameters like ``system``, ``temperature``, and ``max_tokens`` work similarly to the ``generate_text`` method.\n",
    "\n",
    "### Flow:\n",
    "\n",
    "- Similar to ``generate_text``, it sends the prompt to the OpenAI API.\n",
    "\n",
    "- The response is parsed into the defined Pydantic model and returned.\n",
    "\n",
    "- If an error occurs, it prints the error and traceback if debugging is enabled.\n",
    "\n",
    "### `get_model` and `set_model` Methods\n",
    "\n",
    "- `get_model`: Returns the currently set model.\n",
    "\n",
    "- `set_model`: Allows changing the model to a new one, using the ``model`` parameter. The OpenAI object is reinitialized with the new model, and the updated model is stored.\n",
    "\n",
    "### `estimate_tokens` Method\n",
    "\n",
    "This method estimates the number of tokens in a given text string. Tokens are units that the model processes, including words and punctuation.\n",
    "\n",
    "### Flow:\n",
    "\n",
    "- The function uses the regular expression ``re.findall(r\"\\S+\", text)`` to find non-whitespace characters, treating them as tokens.\n",
    "\n",
    "- It returns the number of tokens in the input text.\n",
    "\n",
    "# General Overview of the Flow\n",
    "\n",
    "1. ``Initialization (__init__)``: Sets up the instance with API keys and model configuration.\n",
    "\n",
    "2. ``Text Generation (generate_text)``: Generates text responses from the AI based on a prompt.\n",
    "\n",
    "3. ``Object Generation (generate_object)``: Similar to ``generate_text``, but parses the AI’s response into a structured object.\n",
    "\n",
    "4. ``Model Management (get_model, set_model)``: Allows getting or changing the AI model.\n",
    "\n",
    "5. ``Token Estimation (estimate_tokens)``: Estimates the number of tokens in a given string, which helps with understanding input/output limitations in the OpenAI context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "import traceback\n",
    "from typing import Union\n",
    "import json\n",
    "import re\n",
    "\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are an intelligent AI assistant. The user will give you a prompt, respond appropriately.\"\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "class EasyLLM:\n",
    "    \"\"\"\n",
    "    A simple abstraction for the OpenAI API. It provides easy-to-use methods to generate text and objects using the OpenAI API.\n",
    "    A demonstration for the \"How to build an Abstaction with Open AI API\" blog post.\n",
    "    Author: Aditya Patange (AdiPat)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model=DEFAULT_OPENAI_MODEL,\n",
    "        verbose=True,\n",
    "        debug=True,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"EasyLLM: Powering up! 🚀\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.openai = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"EasyLLM: Model set to {model}.\")\n",
    "            print(\"EasyLLM: Ready for some Generative AI action! ⚡️\")\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[str, None]:\n",
    "        \"\"\"Generates text using the OpenAI API.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating text for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "            response = self.openai.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            response = response.choices[0].message.content\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Text generated successfully. 🎉\")\n",
    "\n",
    "            if self.debug:\n",
    "                response = json.dumps(response)\n",
    "                print(f\"EasyLLM Response: {response}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate text. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def generate_object(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_model: BaseModel,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[BaseModel, None]:\n",
    "        \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "\n",
    "            response = self.openai.beta.chat.completions.parse(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format=response_model,\n",
    "                model=self.model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Object generated successfully. 🎉\")\n",
    "\n",
    "            if self.debug:\n",
    "                response_json = response.model_dump_json()\n",
    "                print(f\"EasyLLM Response: {response_json}\")\n",
    "            return response.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def get_model(self) -> str:\n",
    "        \"\"\"Gets the current model.\"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def set_model(self, model: str) -> None:\n",
    "        \"\"\"Sets the model to the given model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Setting model to {model}\")\n",
    "            self.openai = openai.OpenAI(api_key=self.api_key)\n",
    "            self.model = model\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Model set to {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to set model.\\nError: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Estimate the number of tokens in a given string.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input string.\n",
    "\n",
    "        Returns:\n",
    "            int: Estimated token count.\n",
    "        \"\"\"\n",
    "        # Split text by whitespace and count tokens, including punctuation\n",
    "        tokens = re.findall(r\"\\S+\", text)\n",
    "        return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate README Configuration and Content\n",
    "\n",
    "## Imports\n",
    "\n",
    "- ``Dict, Any, List, and BaseModel`` are imported from typing and pydantic for type annotations and validation.\n",
    "\n",
    "- ``requests`` is used for making HTTP requests, specifically to retrieve the content of a file from a URL.\n",
    "\n",
    "- ``re`` is imported for regular expressions, which are used to extract badges from markdown.\n",
    "\n",
    "- ``json`` is used for serializing data into JSON format.\n",
    "\n",
    "## Classes\n",
    "\n",
    "### Project Class (Pydantic BaseModel)\n",
    "\n",
    "This class defines the schema for a project and uses Pydantic's BaseModel to enforce validation.\n",
    "\n",
    "- ``repo_name``: The name of the repository.\n",
    "\n",
    "- ``title``: The title of the project.\n",
    "\n",
    "- ``description``: A description of the project.\n",
    "\n",
    "- ``contributors``: A list of contributors to the project (list of strings).\n",
    "\n",
    "- ``references``: A list of references related to the project (list of strings).\n",
    "\n",
    "- ``tech_stack``: The technologies used in the project (list of strings).\n",
    "\n",
    "- ``tags``: Tags associated with the project (list of strings).\n",
    "\n",
    "- ``license``: The license type for the project.\n",
    "\n",
    "### Sections Class (Pydantic BaseModel)\n",
    "\n",
    "This class defines a list of sections that are to be used in the table of contents. It's a simple schema with a single field ``sections``, which is a list of strings.\n",
    "\n",
    "## Functions\n",
    "\n",
    "### `extract_badges_from_markdown` (markdown_text: str) -> Dict[str, str]\n",
    "\n",
    "This function is responsible for extracting badges from a markdown text.\n",
    "\n",
    "- It uses a regular expression (``r\"\\[!\\[([^\\]]+)\\]\\((https?://[^\\)]+)\\)\\]``) to match the markdown syntax for badges, which typically appears as ``![badge_name](URL)``.\n",
    "\n",
    "- The ``re.findall()`` function is used to extract the badge names and URLs from the markdown text.\n",
    "\n",
    "- The function returns a dictionary where the badge names are the keys, and the URLs are the values.\n",
    "\n",
    "### `get_badges` () -> Dict[str, str]\n",
    "\n",
    "This function retrieves the markdown content for badges from a specific URL (``https://raw.githubusercontent.com/inttter/md-badges/refs/heads/main/README.md``). It calls ``requests.get()`` to fetch the README file from this URL, then passes the content to ``extract_badges_from_markdown()`` to extract the badge names and URLs.\n",
    "\n",
    "- The function returns a dictionary of badges and URLs.\n",
    "\n",
    "### `get_table_of_contents` (llm: EasyLLM, project: Project) -> List[str]\n",
    "\n",
    "This function generates a table of contents for the README, using a mandatory list of sections and adding custom ones specific to the project.\n",
    "\n",
    "- It starts by defining two lists of mandatory sections: one for the first half of the README (e.g., \"About\", \"Problem Statement\", \"Usage\") and the second half (e.g., \"Contributing\", \"License\", \"References\").\n",
    "\n",
    "- The function then uses an EasyLLM instance (which is presumably a custom class or wrapper for interacting with a large language model, although it’s not defined in the provided code) to generate additional project-specific sections.\n",
    "\n",
    "- A prompt is constructed, providing the mandatory sections and asking the LLM to suggest three new sections that are unique and specific to the project.\n",
    "\n",
    "- The sections generated by the LLM are added to the table of contents, ensuring no repetition of the mandatory sections.\n",
    "\n",
    "- Finally, the function returns the full table of contents.\n",
    "\n",
    "## `get_readme_generator_config` (title: str) -> Dict[str, Any]\n",
    "\n",
    "This function generates the configuration for the README generator, focusing on the visual aspects (header and table of contents).\n",
    "\n",
    "### Visual Configuration for Header:\n",
    "\n",
    "- The prompt generates a good header section for the README. It includes:\n",
    "\n",
    "  - A title with an emoji (e.g., ``# 🚀 My Awesome Project``).\n",
    "\n",
    "  - A list of badges to be included below the title (using the ``get_badges()`` function).\n",
    "\n",
    "  - A call to action to encourage users to star the project on GitHub.\n",
    "\n",
    "  - A marketing tagline that succinctly describes the project.\n",
    "\n",
    "### Table of Contents:\n",
    "\n",
    "- The prompt generates a table of contents with the mandatory sections and any additional sections generated by the LLM.\n",
    "\n",
    "- The function returns a dictionary with ``visual_config`` that includes the generated prompts for the header and table of contents.\n",
    "\n",
    "## Overall Workflow\n",
    "\n",
    "### Badges Extraction:\n",
    "\n",
    "- The code starts by retrieving badges from a specific URL (``get_badges()``) and parsing them using regular expressions (``extract_badges_from_markdown()``).\n",
    "\n",
    "### Generating Table of Contents:\n",
    "\n",
    "- The ``get_table_of_contents()`` function uses the LLM to generate the table of contents by combining predefined sections and custom sections specific to the project.\n",
    "\n",
    "### Generating README Configuration:\n",
    "\n",
    "- The ``get_readme_generator_config()`` function sets up the README header and table of contents configuration, which will be used to generate the final README file for the project.\n",
    "\n",
    "### Example of Usage\n",
    "\n",
    "- If you call ``get_table_of_contents()`` with an instance of ``Project`` and an LLM object, the function will return a list of sections that should be included in the project's README.\n",
    "\n",
    "- Calling ``get_readme_generator_config()`` with the title of the project will generate the visual configuration required to display the README header and its contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import requests\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Project(BaseModel):\n",
    "    repo_name: str\n",
    "    title: str\n",
    "    description: str\n",
    "    contributors: list[str]\n",
    "    references: list[str]\n",
    "    tech_stack: list[str]\n",
    "    tags: list[str]\n",
    "    license: str\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: list[str]\n",
    "\n",
    "\n",
    "def extract_badges_from_markdown(markdown_text: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract badges and their URLs from the given markdown text.\n",
    "\n",
    "    Args:\n",
    "        markdown_text (str): The markdown content containing badges.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with badge names as keys and URLs as values.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\[!\\[([^\\]]+)\\]\\((https?://[^\\)]+)\\)\\]\"\n",
    "    matches = re.findall(pattern, markdown_text)\n",
    "    return {badge: url for badge, url in matches}\n",
    "\n",
    "\n",
    "def get_badges() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Get the markdown content of the README file containing badges.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with badge names as keys and URLs as values.\n",
    "\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/inttter/md-badges/refs/heads/main/README.md\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    badges_markdown = response.text\n",
    "    return extract_badges_from_markdown(badges_markdown)\n",
    "\n",
    "\n",
    "def get_table_of_contents(llm: EasyLLM, project: Project) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates the table of contents for the README.\n",
    "\n",
    "    Args:\n",
    "        llm (EasyLLM): The EasyLLM instance.\n",
    "        project (Project): The Project instance.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sections for the table of contents.\n",
    "    \"\"\"\n",
    "    mandatory_sections_first_half = [\n",
    "        \"About\",\n",
    "        \"Problem Statement\",\n",
    "        \"Research Areas\",\n",
    "        \"Installation\",\n",
    "        \"Usage\",\n",
    "    ]\n",
    "    mandatory_sections_second_half = [\n",
    "        \"Contributing\",\n",
    "        \"License\",\n",
    "        \"Acknowledgements\",\n",
    "        \"Authors\",\n",
    "        \"References\",\n",
    "    ]\n",
    "    table_of_contents = mandatory_sections_first_half\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Given the table of contents for the GitHub README of Project: '{project.title}' with the following sections:\n",
    "        Mandatory Sections: {json.dumps(mandatory_sections_first_half + mandatory_sections_second_half)}\n",
    "        These sections should be linked to the respective sections in the README.\n",
    "        These are mandatory sections for a good README.\n",
    "        You can add more sections if you want as per the project requirements.\n",
    "        For instance, a web scraping project would have \"AI & Web Scraping\" as a separate section. \n",
    "        Then, one of our projects, knowledge-grapher would have \"Knowledge Graphs in Practice\" and \"How Google uses Knowledge Graphs at scale\" as a separate section.\n",
    "\n",
    "        Generate any 3 new sections unique, and specific to the project.\n",
    "        Make sure you don't repeat the same sections and don't include the mandatory sections again.\n",
    "\n",
    "        Project Details: '''{project.model_dump_json()}''''\n",
    "    \"\"\"\n",
    "    sections = llm.generate_object(prompt, Sections)\n",
    "\n",
    "    if sections and len(sections.sections) > 0:\n",
    "        for section in sections.sections:\n",
    "            if section not in table_of_contents:\n",
    "                table_of_contents.append(section)\n",
    "\n",
    "    for section in mandatory_sections_second_half:\n",
    "        if section not in table_of_contents:\n",
    "            table_of_contents.append(section)\n",
    "\n",
    "    return table_of_contents\n",
    "\n",
    "\n",
    "def get_readme_generator_config(title: str) -> Dict[str, Any]:\n",
    "    README_GENERATOR_CONFIG = {\n",
    "        \"visual_config\": {\n",
    "            \"header\": {\n",
    "                \"prompt\": f\"\"\"\n",
    "                Generate a good header section for the README\n",
    "                1. TITLE: It should contain the title of the project with an emoji. \n",
    "                Example: `# 🚀 My Awesome Project`\n",
    "                Be creative with the emoji and title and try to ensure that the title is relevant to the project and README.\n",
    "                TITLE: {title}\n",
    "\n",
    "                2. BADGES: After the title add badges.\n",
    "                Available BadgesL {json.dumps(get_badges())}   \n",
    "\n",
    "                3. Message to star us on GitHub.\n",
    "                Example: Please give us a ⭐ on GitHub if this project helped you!\n",
    "\n",
    "                4. Marketing Tagline: Add a marketing tagline for the project.\n",
    "                Example: #### Simplify your ETL pipelines with LitETL 🔥 — the lightweight ETL framework!\n",
    "                \"\"\"\n",
    "            },\n",
    "            \"table_of_contents\": {\n",
    "                \"prompt\": \"\"\"\n",
    "                Generate a table of contents for the README.\n",
    "                This should include \"About\", \"Problem Statement\", \"Research Areas\", \"Installation\", \"Usage\", \"Contributing\", \"License\", \"Acknowledgements\", \"Authors\", \"References\".\n",
    "                These sections should be linked to the respective sections in the README.\n",
    "                These are mandatory sections for a good README.\n",
    "                You can add more sections if you want as per the project requirements.\n",
    "                For instance, a web scraping project would have \"AI & Web Scraping\" as a separate section. \n",
    "                Then, one of our projects, knowledge-grapher would have \"Knowledge Graphs in Practice\" and \"How Google uses Knowledge Graphs at scale\" as a separate section.\n",
    "                \"\"\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    return README_GENERATOR_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Retrieve and Format Badges, Estimate Token Usage\n",
    "\n",
    "This code is designed to retrieve badges from a markdown file, format them as JSON, and estimate how many tokens the resulting JSON will consume in the context of a large language model (LLM).\n",
    "\n",
    "### Initializing EasyLLM Instance:\n",
    "\n",
    "- An instance of ``EasyLLM`` is created to interact with the LLM. This object likely has methods to help with tasks like token estimation or generating content based on prompts (though its exact implementation is not provided).\n",
    "\n",
    "### Fetching Badges:\n",
    "\n",
    "- The ``get_badges()`` function is called to retrieve the badges from a markdown file located at a specified URL. This function parses the markdown, extracting badge names and their corresponding URLs, returning them as a dictionary.\n",
    "\n",
    "### Converting Badges to JSON:\n",
    "\n",
    "- The ``json.dumps()`` function is used to convert the dictionary of badges into a JSON-formatted string. The badges are indented with 2 spaces for better readability. This string (``badges_json``) now represents the badges in a structured, machine-readable format.\n",
    "\n",
    "### Printing the Number of Badges:\n",
    "\n",
    "- The code prints the number of badges retrieved by calculating the length of the badges dictionary. This helps in understanding how many badges were extracted from the markdown file.\n",
    "\n",
    "### Estimating Token Usage:\n",
    "\n",
    "- The ``estimate_tokens()`` method of the ``EasyLLM`` instance is used to estimate the number of tokens the JSON string will consume if passed to the LLM. This is important because LLMs have a maximum token limit, and estimating token usage ensures the data won't exceed this limit.\n",
    "\n",
    "### Displaying the JSON Representation of Badges:\n",
    "\n",
    "- Finally, the formatted JSON string (``badges_json``) is printed to display the badge names and their URLs in a readable format.\n",
    "\n",
    "### Purpose:\n",
    "\n",
    "The overall purpose of this code is to extract badges from a README markdown file, format them as JSON for structured data handling, and ensure that the data can be processed by the LLM without exceeding token limits. The token estimation step is particularly useful when dealing with large datasets to avoid truncation or errors when passing the data to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "llm = EasyLLM()\n",
    "badges = get_badges()\n",
    "badges_json = json.dumps(badges, indent=2)\n",
    "\n",
    "\n",
    "print(f\"Badges found in the README: {len(badges)}\")\n",
    "# For estimating if it will fit in the LLM context window\n",
    "print(f\"Tokens: {llm.estimate_tokens(badges_json)}\")\n",
    "print(badges_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Generate and Display Table of Contents for README\n",
    "\n",
    "### Importing clear_output from IPython.display:\n",
    "\n",
    "- This imports the ``clear_output()`` function, which is used to clear the output of the cell in a Jupyter Notebook. This can be useful for keeping the notebook interface clean by removing previous outputs.\n",
    "\n",
    "### Calling get_table_of_contents() Function:\n",
    "\n",
    "- The ``get_table_of_contents()`` function is called to generate the table of contents for a README file.\n",
    "- A ``Project`` instance is created with the following attributes:\n",
    "\n",
    "  - ``repo_name``: \"test\"\n",
    "\n",
    "  - ``title``: \"Test\"\n",
    "\n",
    "  - ``description``: \"Test\"\n",
    "\n",
    "  - ``contributors``: [\"Adi\"]\n",
    "\n",
    "  - ``references``: [\"Ref\"]\n",
    "\n",
    "  - ``tech_stack``: [\"Python\"]\n",
    "\n",
    "  - ``tags``: [\"Test\"]\n",
    "\n",
    "  - ``license``: \"MIT\"\n",
    "\n",
    "- The ``llm`` instance is passed along with the ``Project`` instance to generate the table of contents. The ``llm`` is expected to interact with the LLM (likely a large language model) to suggest sections for the table of contents, combining predefined mandatory sections with custom sections based on the project.\n",
    "\n",
    "### Clearing the Output:\n",
    "\n",
    "- This clears any output that was previously displayed in the notebook cell. It is typically used to remove clutter, especially when running multiple iterations or operations in a notebook environment.\n",
    "\n",
    "### Printing the Table of Contents:\n",
    "\n",
    "- The table of contents, which was generated by the ``get_table_of_contents()`` function, is printed. This will show the list of sections that should be included in the README file for the \"Test\" project.\n",
    "\n",
    "### Purpose:\n",
    "\n",
    "The goal of this code is to generate and display the table of contents for a README file based on the ``Project`` instance and a large language model (LLM). The output is cleared beforehand to ensure that only the relevant table of contents is shown in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "table_of_contents = get_table_of_contents(\n",
    "    llm,\n",
    "    Project(\n",
    "        repo_name=\"test\",\n",
    "        title=\"Test\",\n",
    "        description=\"Test\",\n",
    "        contributors=[\"Adi\"],\n",
    "        references=[\"Ref\"],\n",
    "        tech_stack=[\"Python\"],\n",
    "        tags=[\"Test\"],\n",
    "        license=\"MIT\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"Table of Contents: {table_of_contents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "This application streamlines the process of generating structured and informative README files for projects by automating key components such as the table of contents, badge extraction, and visual configuration. By leveraging a large language model (LLM) and predefined templates, the app ensures that each README is comprehensive and aligned with best practices.\n",
    "\n",
    "- ### Key highlights of the app:\n",
    "\n",
    "    - `Automated Table of Contents Generation`: It dynamically creates a table of contents based on mandatory and custom sections, tailored to each project's specific needs.\n",
    "    \n",
    "    - `Badge Integration`: The app can fetch badges from external markdown sources and include them in the README for a more polished presentation.\n",
    "\n",
    "    - `Customizable Configuration`: Through configurable prompts, it generates headers, section titles, and marketing taglines, ensuring a professional and engaging README.\n",
    "\n",
    "    - `Token Estimation for LLM`: The app estimates token usage for large data to prevent issues with token limits when working with the LLM.\n",
    "    \n",
    "Overall, this app significantly reduces manual effort, enhances the quality of README files, and provides a consistent and automated way to document projects effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! 🌐\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
