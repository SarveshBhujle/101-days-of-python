{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üö® This Notebook is work in progress. \n",
    "\n",
    "# Groq: Generative Information Density Experiment üçû (GrID) \n",
    "\n",
    "This notebook services as our experiments in understanding \"information density\" ‚Äî as a concept, topic of research, practical tool and natural occurence. We start the experiment with no external knowledge, simply an intuition of what \"information density\" means to us. From there, we'll gradually evolve our approach by tapping into research literature and existing tools. At the end, we'll present one compact solution that compiles all the findings in the notebook. üëæ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "    load_dotenv()\n",
    "\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "    if GROQ_API_KEY is None:\n",
    "        print(\"Please set the GROQ_API_KEY environment variable.\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        print(\"GROQ_API_KEY is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "class Sentiment(Enum):\n",
    "    positive = \"positive\"\n",
    "    negative = \"negative\"\n",
    "    neutral = \"neutral\"\n",
    "\n",
    "class StrengthMetric(BaseModel):\n",
    "    \"\"\"\n",
    "    The strength of the metric in the text.\n",
    "    \"\"\"\n",
    "    strength: float\n",
    "    confidence: float\n",
    "\n",
    "class TopicScoreNode(StrengthMetric):\n",
    "    \"\"\"\n",
    "    Extracted topic with strength and confidence. \n",
    "    The topic strength indicates how strongly the topic is present in the text.\n",
    "    The confidence indicates how confident the model is in the topic extraction and strength.\n",
    "    \"\"\"\n",
    "    topic: str\n",
    "\n",
    "class QuestionNode(StrengthMetric):\n",
    "    \"\"\"\n",
    "    Extracted question with strength and confidence. \n",
    "    The question strength indicates how strongly the question is present in the text.\n",
    "    The confidence indicates how confident the model is in the question extraction and strength.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "\n",
    "class EmbeddedMeaningNode(StrengthMetric):\n",
    "    \"\"\"\n",
    "    Extracted embedded meaning with strength and confidence. \n",
    "    The embedded meaning strength indicates how strongly the embedded meaning is present in the text.\n",
    "    The confidence indicates how confident the model is in the embedded meaning extraction and strength.\n",
    "    \"\"\"\n",
    "    embedded_meaning: str\n",
    "\n",
    "class TangentThoughtNode(StrengthMetric):\n",
    "    \"\"\"\n",
    "    Extracted tangent thought with strength and confidence. \n",
    "    The tangent thought strength indicates how strongly the tangent thought is present in the text.\n",
    "    The confidence indicates how confident the model is in the tangent thought extraction and strength.\n",
    "    \"\"\"\n",
    "    tangent_thought: str\n",
    "\n",
    "class TopicScores(BaseModel):\n",
    "    \"\"\"\n",
    "    Extracted topics from the strength with their confidence and strengths. \n",
    "    \"\"\"\n",
    "    topic_scores: List[TopicScoreNode]\n",
    "\n",
    "class QuestionScores(BaseModel):\n",
    "    \"\"\"\n",
    "    Extracted questions from the strength with their confidence and strengths. \n",
    "    \"\"\"\n",
    "    question_scores: List[QuestionNode]\n",
    "\n",
    "class EmbeddedMeaningScores(BaseModel):\n",
    "    \"\"\"\n",
    "    Extracted embedded meanings from the strength with their confidence and strengths. \n",
    "    \"\"\"\n",
    "    embedded_meaning_scores: List[EmbeddedMeaningNode]\n",
    "\n",
    "class SentimentScore(BaseModel):\n",
    "    \"\"\"\n",
    "    The sentiment score of the text.\n",
    "    \"\"\"\n",
    "    sentiment: Sentiment\n",
    "    confidence: float\n",
    "\n",
    "\n",
    "class TangentThoughtScores(BaseModel):\n",
    "    \"\"\"\n",
    "    Extracted tangent thoughts from the strength with their confidence and strengths. \n",
    "    \"\"\"\n",
    "    tangent_thought_scores: List[TangentThoughtNode]\n",
    "\n",
    "\n",
    "class PreliminaryAnalysis(BaseModel):\n",
    "    \"\"\"\n",
    "    Preliminary analysis of the text.\n",
    "    \"\"\"\n",
    "    topic_scores: TopicScores\n",
    "    question_scores: QuestionScores\n",
    "    embedded_meaning_scores: EmbeddedMeaningScores\n",
    "    sentiment_score: SentimentScore\n",
    "    tangent_thought_scores: TangentThoughtScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from groq import Groq\n",
    "import traceback\n",
    "from pydantic import BaseModel\n",
    "from typing import Union\n",
    "from openai import OpenAI\n",
    "\n",
    "DEFAULT_MODEL = \"llama-3.3-70b-versatile\"\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o\"\n",
    "\n",
    "class LLMErrorResponse(BaseModel):\n",
    "    error: str\n",
    "\n",
    "\n",
    "def get_groq_client():\n",
    "    \"\"\"Returns an instance of the Groq class\"\"\"\n",
    "    groq = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    client = instructor.from_groq(groq, mode=instructor.Mode.JSON)\n",
    "    return client\n",
    "\n",
    "def get_openai_client():\n",
    "    \"\"\"Returns an instance of the OpenAI class\"\"\"\n",
    "    openai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    return openai\n",
    "\n",
    "\n",
    "def llm(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=\"You are a helpful AI assistant. The user will talk to you and its your job to provide detailed and clear responses.\",\n",
    "    model=DEFAULT_MODEL,\n",
    ") -> Union[BaseModel, LLMErrorResponse]:\n",
    "    \"\"\"Calls LLM API with the given prompt. Defaults to llama-3.3-70b-versatile\"\"\",\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    try:\n",
    "        client = get_groq_client()\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages, model=model, response_model=response_model\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            openai = get_openai_client()\n",
    "            completion = openai.beta.chat.completions.parse(messages=messages, model=DEFAULT_OPENAI_MODEL, response_format=response_model)\n",
    "            return completion.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return LLMErrorResponse(error=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrIDEngineNucleus():\n",
    "    \"\"\"GrID Engine: Computing information density.\"\"\"\n",
    "\n",
    "    def __init__(self, model = \"llama-3.3-70b-versatile\", verbose = False):\n",
    "        self.model = model \n",
    "        self.verbose = verbose\n",
    "        \n",
    "\n",
    "    def get_sentiment(self, text) -> SentimentScore:\n",
    "        prompt = f\"\"\"\n",
    "        Compute the sentiment of the text.\n",
    "        Respond with the sentiment and confidence.\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=SentimentScore)\n",
    "        return response\n",
    "    \n",
    "    def get_embedded_meaning(self, text) -> EmbeddedMeaningScores:\n",
    "        prompt = f\"\"\"\n",
    "        Extract the embedded meanings from the text.\n",
    "        Respond with the embedded meanings, their strengths and confidence.\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=EmbeddedMeaningScores)\n",
    "        return response\n",
    "    \n",
    "    def get_topic_scores(self, text) -> TopicScores:\n",
    "        prompt = f\"\"\"\n",
    "        Extract the topics from the text.\n",
    "        Respond with the topics, their strengths and confidence.\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=TopicScores)\n",
    "        return response\n",
    "    \n",
    "    def get_question_scores(self, text) -> QuestionScores:\n",
    "        prompt = f\"\"\"\n",
    "        Extract the questions from the text.\n",
    "        Respond with the questions, their strengths and confidence.\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=QuestionScores)\n",
    "        return response\n",
    "    \n",
    "    def get_tangent_thought_scores(self, text) -> TangentThoughtScores:\n",
    "        prompt = f\"\"\"\n",
    "        Extract the tangent thoughts from the text.\n",
    "        Respond with the tangent thoughts, their strengths and confidence.\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=TangentThoughtScores)\n",
    "        return response\n",
    "    \n",
    "    def get_preliminary_analysis(self, text: str, collapse_layers = True) -> PreliminaryAnalysis:\n",
    "        \"\"\"Computes the preliminary analysis of the text needed for the GrID engine.\"\"\"\n",
    "        if collapse_layers:\n",
    "            prompt = f\"\"\"\n",
    "                Generate the preliminary analysis of the text.\n",
    "                Respond with the topic scores, question scores, embedded meaning scores, sentiment score and tangent thought scores.\n",
    "                Topic Scores: Extract the topics from the text.\n",
    "                Question Scores: Extract the questions from the text.\n",
    "                Embedded Meaning Scores: Extract the embedded meanings from the text.\n",
    "                Sentiment Score: Compute the sentiment of the text.\n",
    "                Tangent Thought Scores: Extract the tangent thoughts from the text.\n",
    "                Text: {text}\n",
    "            \"\"\"\n",
    "            preliminary_analysis = llm(prompt=prompt, response_model=PreliminaryAnalysis)\n",
    "            return preliminary_analysis\n",
    "        sentiment_score = self.get_sentiment(text)\n",
    "        topic_scores = self.get_topic_scores(text)\n",
    "        question_scores = self.get_question_scores(text)\n",
    "        embedded_meaning_scores = self.get_embedded_meaning(text)\n",
    "        tangent_thought_scores = self.get_tangent_thought_scores(text)\n",
    "        return PreliminaryAnalysis(\n",
    "            topic_scores=topic_scores,\n",
    "            question_scores=question_scores,\n",
    "            embedded_meaning_scores=embedded_meaning_scores,\n",
    "            sentiment_score=sentiment_score,\n",
    "            tangent_thought_scores=tangent_thought_scores\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GrIDNaiveResult(BaseModel):\n",
    "    \"\"\"The result of the GrID Engine Naive.\"\"\"\n",
    "    information_density: float\n",
    "    explanation: str\n",
    "\n",
    "class GrIDEngineNaive(GrIDEngineNucleus):\n",
    "    \"\"\"GrID Engine: Computing information density using a naive approach.\"\"\"\n",
    "    \n",
    "    def __init__(self,  model = \"llama-3.3-70b-versatile\", verbose = False):\n",
    "        super().__init__(model, verbose=verbose)\n",
    "\n",
    "    def compute_information_density(self, text: str, collapse_layers = False) -> GrIDNaiveResult:\n",
    "        \"\"\"Compute the information density of the text.\"\"\"\n",
    "        preliminary_analysis = self.get_preliminary_analysis(text, collapse_layers=collapse_layers)\n",
    "        preliminary_analysis_json = preliminary_analysis.model_dump_json()\n",
    "        system = \"\"\"\n",
    "            You are GrID Engine Naive. \n",
    "            GrID Engine stands for 'Generative Information Density' Engine.\n",
    "            We have computed a preliminary analysis for a piece of text. \n",
    "            Your job is to understand that text, and compute the information density between 0 and 1.\n",
    "            To assist you, we have extracted the following information from the text:\n",
    "            - Topics: The main topics that the text discusses.\n",
    "            - Questions: The questions in the text which indicates the curiosity of the text.\n",
    "            - Embedded Meanings: The embedded meanings in the text which are not explicitly stated.\n",
    "            - Sentiment: The sentiment of the text which indicates the overall emotion of the text.\n",
    "            - Tangent Thoughts: The tangent thoughts in the text which are not directly related to the main topics.\n",
    "            These components include strengths and confidence levels.\n",
    "            Use this information to compute the information density.\n",
    "            Explain your reasoning for the information density in 4 paragraphs in markdown in detail.\n",
    "            Respond with the information density and the explanation in markdown.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "            Based on the given instructions, analyse the preliminary analysis and compute the information density of the text.\n",
    "            Text: {text}\n",
    "            Preliminary Analysis: {preliminary_analysis_json}\n",
    "        \"\"\"\n",
    "\n",
    "        response = llm(prompt=prompt, system=system, response_model=GrIDNaiveResult)\n",
    "        return response\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INPUT_TEXTS = [\n",
    "    \"\"\"\n",
    "When learning something new in Software Development, it's easy to get caught up in the \"consumption cycle\". If you're only consuming lectures, courses, blogs and learning material but spending little to no time building things, you won't get far. \n",
    "\n",
    "I personally like spending 80% to 90% of my time actively building and architecting / designing / implementing and the remaining 10% / 20% of my time consuming learning material, lecture, trainings. However, it's important that you strive for a balance that works the best for you. Generally 50% consuming, 50% implementing might be a good balance, but it depends on the subject or topic you're learning and the time you have on your hands.\n",
    "\n",
    "Sometimes, it's important to just learn enough so that you can get started with execution! There's that sweet spot of \"genius\" which lies between endless thought, planning, research, and the other extreme which is implementation without feedback or learnings or depth. \n",
    "\n",
    "It takes a while to figure this out, but as a rule of thumb; keep building, keep shipping and growth will become a natural outcome of the process!\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "Excited to be featured as an NVIDIA agents partner during Jensen's keynote at CES üî•\n",
    "\n",
    "2025 is the year of production knowledge agents. Everything from document research, to automated document extraction/business logic, to report generation. \n",
    "\n",
    "This is a great way to kick off the year. Huge shoutout to Laurie Voss + the NVIDIA team (including Daniel Glogowski) for pulling these resources together.\n",
    "\n",
    "Blueprint: https://lnkd.in/gHFFKrgC\n",
    "Video: https://lnkd.in/gQi7U6sq\n",
    "Notebook: https://lnkd.in/ggmkXiYA\n",
    "Blog: https://lnkd.in/ghfApUs9\n",
    "\"\"\",\n",
    "    \"\"\"If you are leading a project, your only responsibility is to ensure it is delivered, whatever it takes. Here are a few pointers that I have followed \n",
    "\n",
    "1. avoid being blocked, always find a way out\n",
    "2. if there is a chance of a delay, communicate early\n",
    "3. always look for trade-offs and make sure we pick the right one\n",
    "4. estimate timelines well; good estimation reduces chaos\n",
    "5. influence others so that they prioritize our tasks\n",
    "6. always reiterate key details to ensure alignment, there is no such thing as over-communication.\n",
    "\n",
    "On the technical and execution side, here's what I ensure\n",
    "\n",
    "1. form a deep understanding and high clarity about the project\n",
    "2. create a solid plan, reduce ambiguity, and keep the team focused \n",
    "3. be agile, monitor progress, revise plan if required\n",
    "4. make sure every single person involved in the project is aligned\n",
    "\n",
    "Delivering a project requires very high focus, clarity, and persistence. Keep the big picture in mind, but execute with attention to detail.\n",
    "\n",
    "Even if you are early in your career, follow the above, and earn some leadership brownie points.\n",
    "\n",
    "ps: enrollments open for sys design cohort - arpitbhayani.me/course\n",
    "\n",
    "hashtag#AsliEngineering hashtag#CareerGrowth\"\"\",\n",
    "    \"\"\"Yes, materials provide a chip‚Äôs physical foundation and the substance of more powerful and compact components. But they are also integral to the advanced fabrication methods and novel chip designs that underpin the industry‚Äôs rapid progress in recent decades.\n",
    "\n",
    "For this reason, materials science is taking on a heightened importance as we grapple with the limits of miniaturization. Advanced materials are needed more than ever for the industry to unlock the new designs and technologies capable of increasing chip efficiency, speed, and power. We are seeing novel chip architectures that embrace the third dimension and stack layers to optimize surface area usage while lowering energy consumption. The industry is harnessing advanced packaging techniques, where separate ‚Äúchiplets‚Äù are fused with varying functions into a more efficient, powerful single chip. This is called heterogeneous integration.\"\"\",\n",
    "    \"\"\"\n",
    "    Data science is a good field. \n",
    "    It has a lot of potential for career growth\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from json import dumps\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class FormattedResponse(BaseModel):\n",
    "    content: str\n",
    "\n",
    "\n",
    "def convert_markdown_to_text(markdown):\n",
    "    prompt = f\"\"\"\n",
    "    Convert the markdown to plain text.\n",
    "    Markdown: {markdown}\n",
    "    \"\"\"\n",
    "    response = llm(prompt=prompt, response_model=FormattedResponse)\n",
    "    return response.content\n",
    "\n",
    "def format_data_as_markdown_table(data):\n",
    "    \"\"\"Formats the data as a markdown table.\"\"\"\n",
    "    markdown = \"| Text | Information Density |\\n\"\n",
    "    markdown += \"| --- | --- |\\n\"\n",
    "    for item in data:\n",
    "        trimmed_text = item['text'][:50] + (item['text'][50:] and '...')\n",
    "        markdown += f\"| {trimmed_text} | {item['information_density']} |\\n\"\n",
    "    return markdown\n",
    "\n",
    "\n",
    "\n",
    "def run_naive_grid_engine():\n",
    "    grid_engine_naive = GrIDEngineNaive(verbose=True)\n",
    "    results = []\n",
    "    print(len(TEST_INPUT_TEXTS))\n",
    "    \n",
    "    for text in TEST_INPUT_TEXTS[0:2]:\n",
    "        response = grid_engine_naive.compute_information_density(text)\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"information_density\": response.information_density,\n",
    "            \"explanation\": response.explanation\n",
    "        })\n",
    "    formatted_tables_markdown = format_data_as_markdown_table(results)\n",
    "    return Markdown(formatted_tables_markdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_naive_grid_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Excited to be featured as an NVIDIA agents partner during Jensen's keynote at CES üî•\n",
    "\n",
    "2025 is the year of production knowledge agents. Everything from document research, to automated document extraction/business logic, to report generation. \n",
    "\n",
    "This is a great way to kick off the year. Huge shoutout to Laurie Voss + the NVIDIA team (including Daniel Glogowski) for pulling these resources together.\n",
    "\n",
    "Blueprint: https://lnkd.in/gHFFKrgC\n",
    "Video: https://lnkd.in/gQi7U6sq\n",
    "Notebook: https://lnkd.in/ggmkXiYA\n",
    "Blog: https://lnkd.in/ghfApUs9\n",
    "\"\"\"\n",
    "grid_engine_naive = GrIDEngineNaive(verbose=True)\n",
    "collapse_layers = False # Set to True to collapse the layers\n",
    "\n",
    "response = grid_engine_naive.compute_information_density(input_text, collapse_layers=True)\n",
    "\n",
    "print(f\"Text: {input_text}\")\n",
    "print(f\"Information Density: {response.information_density}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Excited to be featured as an NVIDIA agents partner during Jensen's keynote at CES üî•\n",
    "\n",
    "2025 is the year of production knowledge agents. Everything from document research, to automated document extraction/business logic, to report generation. \n",
    "\n",
    "This is a great way to kick off the year. Huge shoutout to Laurie Voss + the NVIDIA team (including Daniel Glogowski) for pulling these resources together.\n",
    "\n",
    "Blueprint: https://lnkd.in/gHFFKrgC\n",
    "Video: https://lnkd.in/gQi7U6sq\n",
    "Notebook: https://lnkd.in/ggmkXiYA\n",
    "Blog: https://lnkd.in/ghfApUs9\n",
    "\"\"\"\n",
    "grid_engine_naive = GrIDEngineNaive(verbose=True)\n",
    "collapse_layers = True # Set to True to collapse the layers\n",
    "\n",
    "response = grid_engine_naive.compute_information_density(input_text, collapse_layers=True)\n",
    "\n",
    "print(f\"Text: {input_text}\")\n",
    "print(f\"Information Density: {response.information_density}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_runtime_with_collapsing(input_text = TEST_INPUT_TEXTS[0]):\n",
    "    print(f\"Input Text: {input_text}\")\n",
    "    import time\n",
    "    grid_engine_naive = GrIDEngineNaive(verbose=True)\n",
    "    start_time = time.time()\n",
    "    response = grid_engine_naive.compute_information_density(input_text, collapse_layers=True)\n",
    "    print(f\"Information Density (no layer collapsing): {response.information_density}\")\n",
    "    end_time = time.time()\n",
    "    time_with_collapsing = end_time - start_time\n",
    "    print(f\"Time with collapsing: {time_with_collapsing}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = grid_engine_naive.compute_information_density(input_text, collapse_layers=False)\n",
    "    end_time = time.time()\n",
    "    time_without_collapsing = end_time - start_time\n",
    "    print(f\"Information Density (with layer collapsing): {response.information_density}\")\n",
    "    print(f\"Time without collapsing: {time_without_collapsing}\")\n",
    "\n",
    "    time_delta_percent = ((time_with_collapsing - time_without_collapsing) / time_with_collapsing) * 100\n",
    "    print(f\"Time difference (collapsing is better by?): {time_delta_percent}%\")\n",
    "    return time_with_collapsing, time_without_collapsing\n",
    "\n",
    "compare_runtime_with_collapsing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
