{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üö® This Notebook is work in progress. \n",
    "\n",
    "# Groq: Generative Information Density Experiment üçû (GrID) \n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "## Description: \n",
    "\n",
    "This notebook services as our experiments in understanding \"information density\" ‚Äî as a concept, topic of research, practical tool and natural occurence. We start the experiment with no external knowledge, simply an intuition of what \"information density\" means to us. From there, we'll gradually evolve our approach by tapping into research literature and existing tools. At the end, we'll present one compact solution that compiles all the findings in the notebook. üëæ\n",
    "\n",
    "GrID Analyzer is a text analysis tool that measures Generative Information Density (GrID) to evaluate coherence, redundancy, and informativeness.\n",
    "\n",
    "It helps assess the complexity of written content by detecting patterns in information distribution.\n",
    "This tool is useful for researchers, writers, and educators to improve text clarity and structure.\n",
    "\n",
    "It provides insights into how efficiently information is conveyed within a passage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Requirements\n",
    "\n",
    "This code checks if requirements are already installed. If not, it runs `pip install -r requirements.txt`. If installation fails, it retries up to max_retries times. If all attempts fail, the script exits with an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function installs packages from requirements.txt, checks if they are already installed, and retries up to max_retries times if the installation fails. If all retries fail, the program exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup Environment Variables\n",
    "\n",
    "The function loads environment variables from a .env file using load_dotenv(), checks if the GROQ_API_KEY is set, and exits with an error message if not. If the key is found, it confirms that the variable is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "    load_dotenv()\n",
    "\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "    if GROQ_API_KEY is None:\n",
    "        print(\"Please set the GROQ_API_KEY environment variable.\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        print(\"GROQ_API_KEY is set.\")\n",
    "        \n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Data Models Using Pydantic\n",
    "\n",
    "This code defines several Pydantic models to structure the results of a text analysis, capturing key insights such as topics, questions, sentiments, and embedded meanings, along with their associated strength and confidence metrics.\n",
    "\n",
    "### Key Classes:\n",
    "\n",
    "- **Sentiment Enum:**\n",
    "\n",
    "  - Represents the sentiment of the text with three possible values: \"positive\", \"negative\", and \"neutral\".\n",
    "\n",
    "- **StrengthMetric BaseModel:**\n",
    "\n",
    "  - A base model representing the strength and confidence of a metric in the text, used as a foundation for other models.\n",
    "\n",
    "- **TopicScoreNode:**\n",
    "\n",
    "  - Represents an extracted topic from the text, including its strength and confidence.\n",
    "\n",
    "- **QuestionNode:**\n",
    "\n",
    "  - Represents an extracted question from the text, with its strength and confidence values.\n",
    "\n",
    "- **EmbeddedMeaningNode:**\n",
    "\n",
    "  - Captures the extracted embedded meaning from the text, along with its strength and confidence.\n",
    "\n",
    "- **TangentThoughtNode:**\n",
    "\n",
    "  - Represents an extracted tangent thought, with strength and confidence values.\n",
    "\n",
    "- **TopicScores:**\n",
    "\n",
    "  - A collection of multiple `TopicScoreNode` objects representing extracted topics.\n",
    "\n",
    "- **QuestionScores:**\n",
    "\n",
    "  - A collection of `QuestionNode` objects representing extracted questions.\n",
    "\n",
    "- **EmbeddedMeaningScores:**\n",
    "\n",
    "  - A collection of `EmbeddedMeaningNode` objects representing extracted embedded meanings.\n",
    "\n",
    "- **SentimentScore:**\n",
    "\n",
    "  - Represents the overall sentiment of the text, along with its confidence.\n",
    "\n",
    "- **TangentThoughtScores:**\n",
    "\n",
    "  - A collection of `TangentThoughtNode` objects representing extracted tangent thoughts.\n",
    "\n",
    "- **PreliminaryAnalysis:**\n",
    "\n",
    "  - A comprehensive model that encapsulates the results of the preliminary analysis, including all topic, question, embedded meaning, sentiment, and tangent thought scores.\n",
    "\n",
    "### Purpose:\n",
    "- These models allow for structured representation and easy processing of various text analysis elements such as topics, sentiments, and extracted thoughts.\n",
    "\n",
    "- The inclusion of strength and confidence metrics provides context and accuracy for the extracted data.\n",
    "\n",
    "- The models enable organizing and grouping related data, facilitating deeper analysis and interpretation of text data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "class Sentiment(Enum):\n",
    "    positive = \"positive\"\n",
    "    negative = \"negative\"\n",
    "    neutral = \"neutral\"\n",
    "\n",
    "class StrengthMetric(BaseModel):\n",
    "    \"\"\"\n",
    "    The strength of the metric in the text.\n",
    "    \"\"\"\n",
    "    strength: float\n",
    "    confidence: float\n",
    "\n",
    "class TopicScoreNode(StrengthMetric):\n",
    "    \"\"\"\n",
    "    Extracted topic with strength and confidence. \n",
    "    The topic strength indicates how strongly the topic is present in the text.\n",
    "    The confidence indicates how confident the model is in the topic extraction and strength.\n",
    "    \"\"\"\n",
    "    topic: str\n",
    "\n",
    "class QuestionNode(StrengthMetric):\n",
    "    \"\"\"\n",
    "    Extracted question with strength and confidence. \n",
    "    The question strength indicates how strongly the question is present in the text.\n",
    "    The confidence indicates how confident the model is in the question extraction and strength.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "\n",
    "class EmbeddedMeaningNode(StrengthMetric):\n",
    "    \"\"\"\n",
    "    Extracted embedded meaning with strength and confidence. \n",
    "    The embedded meaning strength indicates how strongly the embedded meaning is present in the text.\n",
    "    The confidence indicates how confident the model is in the embedded meaning extraction and strength.\n",
    "    \"\"\"\n",
    "    embedded_meaning: str\n",
    "\n",
    "class TangentThoughtNode(StrengthMetric):\n",
    "    \"\"\"\n",
    "    Extracted tangent thought with strength and confidence. \n",
    "    The tangent thought strength indicates how strongly the tangent thought is present in the text.\n",
    "    The confidence indicates how confident the model is in the tangent thought extraction and strength.\n",
    "    \"\"\"\n",
    "    tangent_thought: str\n",
    "\n",
    "class TopicScores(BaseModel):\n",
    "    \"\"\"\n",
    "    Extracted topics from the strength with their confidence and strengths. \n",
    "    \"\"\"\n",
    "    topic_scores: List[TopicScoreNode]\n",
    "\n",
    "class QuestionScores(BaseModel):\n",
    "    \"\"\"\n",
    "    Extracted questions from the strength with their confidence and strengths. \n",
    "    \"\"\"\n",
    "    question_scores: List[QuestionNode]\n",
    "\n",
    "class EmbeddedMeaningScores(BaseModel):\n",
    "    \"\"\"\n",
    "    Extracted embedded meanings from the strength with their confidence and strengths. \n",
    "    \"\"\"\n",
    "    embedded_meaning_scores: List[EmbeddedMeaningNode]\n",
    "\n",
    "class SentimentScore(BaseModel):\n",
    "    \"\"\"\n",
    "    The sentiment score of the text.\n",
    "    \"\"\"\n",
    "    sentiment: Sentiment\n",
    "    confidence: float\n",
    "\n",
    "\n",
    "class TangentThoughtScores(BaseModel):\n",
    "    \"\"\"\n",
    "    Extracted tangent thoughts from the strength with their confidence and strengths. \n",
    "    \"\"\"\n",
    "    tangent_thought_scores: List[TangentThoughtNode]\n",
    "\n",
    "\n",
    "class PreliminaryAnalysis(BaseModel):\n",
    "    \"\"\"\n",
    "    Preliminary analysis of the text.\n",
    "    \"\"\"\n",
    "    topic_scores: TopicScores\n",
    "    question_scores: QuestionScores\n",
    "    embedded_meaning_scores: EmbeddedMeaningScores\n",
    "    sentiment_score: SentimentScore\n",
    "    tangent_thought_scores: TangentThoughtScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up and Call LLM APIs (Groq and OpenAI)\n",
    "\n",
    "This step defines functions to interact with the Groq and OpenAI language model APIs. These functions handle the initialization of the clients and the logic to call the respective APIs to generate responses based on a given prompt.\n",
    "\n",
    "### Key Functions:\n",
    "\n",
    "- **get_groq_client:**\n",
    "\n",
    "  - Initializes and configures the Groq API client using the provided Groq API key.\n",
    "  \n",
    "  - Returns the Groq client instance for further interaction.\n",
    "\n",
    "- **get_openai_client:**\n",
    "\n",
    "  - Initializes and configures the OpenAI API client using the provided OpenAI API key.\n",
    "  \n",
    "  - Returns the OpenAI client instance for further interaction.\n",
    "\n",
    "- **llm (main function):**\n",
    "\n",
    "  - Determines which language model (Groq or OpenAI) to use based on availability.\n",
    "  \n",
    "  - Structures the request with a system message and user prompt.\n",
    "  \n",
    "  - Sends the request to the Groq API first. If it fails, attempts to use the OpenAI API as a fallback.\n",
    "  \n",
    "  - Returns a structured error message if both APIs fail.\n",
    "\n",
    "### Purpose:\n",
    "\n",
    "- This step ensures that the application can communicate with both Groq and OpenAI APIs and handle responses seamlessly.\n",
    "\n",
    "- By attempting to use Groq first and falling back to OpenAI in case of failure, it maximizes the chances of successful API interaction while ensuring stability.\n",
    "\n",
    "- The use of structured error messages makes it easier to debug and identify any issues in the API interaction process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from groq import Groq\n",
    "import traceback\n",
    "from pydantic import BaseModel\n",
    "from typing import Union\n",
    "from openai import OpenAI\n",
    "\n",
    "DEFAULT_MODEL = \"llama-3.3-70b-versatile\"\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o\"\n",
    "\n",
    "class LLMErrorResponse(BaseModel):\n",
    "    error: str\n",
    "\n",
    "\n",
    "def get_groq_client():\n",
    "    \"\"\"Returns an instance of the Groq class\"\"\"\n",
    "    groq = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    client = instructor.from_groq(groq, mode=instructor.Mode.JSON)\n",
    "    return client\n",
    "\n",
    "def get_openai_client():\n",
    "    \"\"\"Returns an instance of the OpenAI class\"\"\"\n",
    "    openai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    return openai\n",
    "\n",
    "\n",
    "def llm(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=\"You are a helpful AI assistant. The user will talk to you and its your job to provide detailed and clear responses.\",\n",
    "    model=DEFAULT_MODEL,\n",
    ") -> Union[BaseModel, LLMErrorResponse]:\n",
    "    \"\"\"Calls LLM API with the given prompt. Defaults to llama-3.3-70b-versatile\"\"\",\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    try:\n",
    "        client = get_groq_client()\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages, model=model, response_model=response_model\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            openai = get_openai_client()\n",
    "            completion = openai.beta.chat.completions.parse(messages=messages, model=DEFAULT_OPENAI_MODEL, response_format=response_model)\n",
    "            return completion.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return LLMErrorResponse(error=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: GrID Engine for Text Analysis\n",
    "\n",
    "The `GrIDEngineNucleus` class is responsible for performing various text analysis tasks using a language model API (LLM). It includes methods that interact with the API to compute sentiment, extract embedded meanings, topics, questions, and tangent thoughts from the text. Additionally, the class can generate a comprehensive preliminary analysis of the text.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Methods for Text Analysis:**\n",
    "\n",
    "  - **Sentiment Analysis:** Computes the sentiment of the text (positive, negative, or neutral).\n",
    "  \n",
    "  - **Topic Extraction:** Extracts topics from the text, including their strength and confidence.\n",
    "  \n",
    "  - **Question Extraction:** Identifies and extracts questions from the text with associated strength and confidence.\n",
    "  \n",
    "  - **Embedded Meaning Extraction:** Extracts deeper meanings embedded within the text.\n",
    "  \n",
    "  - **Tangent Thought Extraction:** Identifies any tangent thoughts or digressions within the text.\n",
    "\n",
    "- **get_preliminary_analysis:**\n",
    "\n",
    "  - Aggregates the analysis results, either by collapsing all layers into a single API call or retrieving individual scores for each aspect.\n",
    "  \n",
    "  - Combines various analyses (e.g., topics, questions, sentiment) into a comprehensive report.\n",
    "\n",
    "### Purpose:\n",
    "\n",
    "- The `GrIDEngineNucleus` class uses the LLM API to extract meaningful insights from text, enabling more advanced text processing and understanding.\n",
    "\n",
    "- By calling specific methods, the class provides structured analysis that can be used for various applications such as sentiment analysis, topic discovery, and deeper content interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrIDEngineNucleus():\n",
    "    \"\"\"GrID Engine: Computing information density.\"\"\"\n",
    "\n",
    "    def __init__(self, model = \"llama-3.3-70b-versatile\", verbose = False):\n",
    "        self.model = model \n",
    "        self.verbose = verbose\n",
    "        \n",
    "\n",
    "    def get_sentiment(self, text) -> SentimentScore:\n",
    "        prompt = f\"\"\"\n",
    "        Compute the sentiment of the text.\n",
    "        Respond with the sentiment and confidence.\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=SentimentScore)\n",
    "        return response\n",
    "    \n",
    "    def get_embedded_meaning(self, text) -> EmbeddedMeaningScores:\n",
    "        prompt = f\"\"\"\n",
    "        Extract the embedded meanings from the text.\n",
    "        Respond with the embedded meanings, their strengths and confidence.\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=EmbeddedMeaningScores)\n",
    "        return response\n",
    "    \n",
    "    def get_topic_scores(self, text) -> TopicScores:\n",
    "        prompt = f\"\"\"\n",
    "        Extract the topics from the text.\n",
    "        Respond with the topics, their strengths and confidence.\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=TopicScores)\n",
    "        return response\n",
    "    \n",
    "    def get_question_scores(self, text) -> QuestionScores:\n",
    "        prompt = f\"\"\"\n",
    "        Extract the questions from the text.\n",
    "        Respond with the questions, their strengths and confidence.\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=QuestionScores)\n",
    "        return response\n",
    "    \n",
    "    def get_tangent_thought_scores(self, text) -> TangentThoughtScores:\n",
    "        prompt = f\"\"\"\n",
    "        Extract the tangent thoughts from the text.\n",
    "        Respond with the tangent thoughts, their strengths and confidence.\n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=TangentThoughtScores)\n",
    "        return response\n",
    "    \n",
    "    def get_preliminary_analysis(self, text: str, collapse_layers = True) -> PreliminaryAnalysis:\n",
    "        \"\"\"Computes the preliminary analysis of the text needed for the GrID engine.\"\"\"\n",
    "        if collapse_layers:\n",
    "            prompt = f\"\"\"\n",
    "                Generate the preliminary analysis of the text.\n",
    "                Respond with the topic scores, question scores, embedded meaning scores, sentiment score and tangent thought scores.\n",
    "                Topic Scores: Extract the topics from the text.\n",
    "                Question Scores: Extract the questions from the text.\n",
    "                Embedded Meaning Scores: Extract the embedded meanings from the text.\n",
    "                Sentiment Score: Compute the sentiment of the text.\n",
    "                Tangent Thought Scores: Extract the tangent thoughts from the text.\n",
    "                Text: {text}\n",
    "            \"\"\"\n",
    "            preliminary_analysis = llm(prompt=prompt, response_model=PreliminaryAnalysis)\n",
    "            return preliminary_analysis\n",
    "        sentiment_score = self.get_sentiment(text)\n",
    "        topic_scores = self.get_topic_scores(text)\n",
    "        question_scores = self.get_question_scores(text)\n",
    "        embedded_meaning_scores = self.get_embedded_meaning(text)\n",
    "        tangent_thought_scores = self.get_tangent_thought_scores(text)\n",
    "        return PreliminaryAnalysis(\n",
    "            topic_scores=topic_scores,\n",
    "            question_scores=question_scores,\n",
    "            embedded_meaning_scores=embedded_meaning_scores,\n",
    "            sentiment_score=sentiment_score,\n",
    "            tangent_thought_scores=tangent_thought_scores\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: GrID Engine Naive for Information Density\n",
    "\n",
    "The `GrIDEngineNaive` class extends the `GrIDEngineNucleus` class, adding functionality to compute the **information density** of a given text. This \"naive\" approach involves generating a preliminary analysis and then using it to calculate the density.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Inherits from `GrIDEngineNucleus`:** \n",
    "\n",
    "  - The class builds on the existing methods of the `GrIDEngineNucleus` class for sentiment, topic, and meaning analysis.\n",
    "\n",
    "- **compute_information_density Method:**\n",
    "\n",
    "  - **Purpose:** Computes the information density of the provided text. Information density is calculated as a score between 0 and 1.\n",
    "  \n",
    "  - **Process:** \n",
    "  \n",
    "    - It first calls `get_preliminary_analysis` to gather insights about the text.\n",
    "    \n",
    "    - Then, a detailed prompt is generated with the analysis and a request for the model to calculate the density.\n",
    "    \n",
    "    - The model returns both the information density score and a markdown explanation of how the density was computed.\n",
    "\n",
    "### Purpose:\n",
    "\n",
    "- The `GrIDEngineNaive` class provides an approach to assess how \"dense\" a given text is in terms of information.\n",
    "\n",
    "- By using this class, users can obtain a score that quantifies how much valuable information is packed into a piece of text, with an accompanying explanation of the reasoning behind the score.\n",
    "\n",
    "### Use Case:\n",
    "\n",
    "- **Information Density Analysis:** Helps determine how much content is packed into a text, which can be useful for text summarization, content optimization, or evaluating information-richness in documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GrIDNaiveResult(BaseModel):\n",
    "    \"\"\"The result of the GrID Engine Naive.\"\"\"\n",
    "    information_density: float\n",
    "    explanation: str\n",
    "\n",
    "class GrIDEngineNaive(GrIDEngineNucleus):\n",
    "    \"\"\"GrID Engine: Computing information density using a naive approach.\"\"\"\n",
    "    \n",
    "    def __init__(self,  model = \"llama-3.3-70b-versatile\", verbose = False):\n",
    "        super().__init__(model, verbose=verbose)\n",
    "\n",
    "    def compute_information_density(self, text: str, collapse_layers = False) -> GrIDNaiveResult:\n",
    "        \"\"\"Compute the information density of the text.\"\"\"\n",
    "        preliminary_analysis = self.get_preliminary_analysis(text, collapse_layers=collapse_layers)\n",
    "        preliminary_analysis_json = preliminary_analysis.model_dump_json()\n",
    "        system = \"\"\"\n",
    "            You are GrID Engine Naive. \n",
    "            GrID Engine stands for 'Generative Information Density' Engine.\n",
    "            We have computed a preliminary analysis for a piece of text. \n",
    "            Your job is to understand that text, and compute the information density between 0 and 1.\n",
    "            To assist you, we have extracted the following information from the text:\n",
    "            - Topics: The main topics that the text discusses.\n",
    "            - Questions: The questions in the text which indicates the curiosity of the text.\n",
    "            - Embedded Meanings: The embedded meanings in the text which are not explicitly stated.\n",
    "            - Sentiment: The sentiment of the text which indicates the overall emotion of the text.\n",
    "            - Tangent Thoughts: The tangent thoughts in the text which are not directly related to the main topics.\n",
    "            These components include strengths and confidence levels.\n",
    "            Use this information to compute the information density.\n",
    "            Explain your reasoning for the information density in 4 paragraphs in markdown in detail.\n",
    "            Respond with the information density and the explanation in markdown.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "            Based on the given instructions, analyse the preliminary analysis and compute the information density of the text.\n",
    "            Text: {text}\n",
    "            Preliminary Analysis: {preliminary_analysis_json}\n",
    "        \"\"\"\n",
    "\n",
    "        response = llm(prompt=prompt, system=system, response_model=GrIDNaiveResult)\n",
    "        return response\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Input Texts\n",
    "\n",
    "This code defines a list of five sample texts covering topics like software development, leadership, materials science, and data science. These texts are used for testing GrID Engine or analysis methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INPUT_TEXTS = [\n",
    "    \"\"\"\n",
    "When learning something new in Software Development, it's easy to get caught up in the \"consumption cycle\". If you're only consuming lectures, courses, blogs and learning material but spending little to no time building things, you won't get far. \n",
    "\n",
    "I personally like spending 80% to 90% of my time actively building and architecting / designing / implementing and the remaining 10% / 20% of my time consuming learning material, lecture, trainings. However, it's important that you strive for a balance that works the best for you. Generally 50% consuming, 50% implementing might be a good balance, but it depends on the subject or topic you're learning and the time you have on your hands.\n",
    "\n",
    "Sometimes, it's important to just learn enough so that you can get started with execution! There's that sweet spot of \"genius\" which lies between endless thought, planning, research, and the other extreme which is implementation without feedback or learnings or depth. \n",
    "\n",
    "It takes a while to figure this out, but as a rule of thumb; keep building, keep shipping and growth will become a natural outcome of the process!\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "Excited to be featured as an NVIDIA agents partner during Jensen's keynote at CES üî•\n",
    "\n",
    "2025 is the year of production knowledge agents. Everything from document research, to automated document extraction/business logic, to report generation. \n",
    "\n",
    "This is a great way to kick off the year. Huge shoutout to Laurie Voss + the NVIDIA team (including Daniel Glogowski) for pulling these resources together.\n",
    "\n",
    "Blueprint: https://lnkd.in/gHFFKrgC\n",
    "Video: https://lnkd.in/gQi7U6sq\n",
    "Notebook: https://lnkd.in/ggmkXiYA\n",
    "Blog: https://lnkd.in/ghfApUs9\n",
    "\"\"\",\n",
    "    \"\"\"If you are leading a project, your only responsibility is to ensure it is delivered, whatever it takes. Here are a few pointers that I have followed \n",
    "\n",
    "1. avoid being blocked, always find a way out\n",
    "2. if there is a chance of a delay, communicate early\n",
    "3. always look for trade-offs and make sure we pick the right one\n",
    "4. estimate timelines well; good estimation reduces chaos\n",
    "5. influence others so that they prioritize our tasks\n",
    "6. always reiterate key details to ensure alignment, there is no such thing as over-communication.\n",
    "\n",
    "On the technical and execution side, here's what I ensure\n",
    "\n",
    "1. form a deep understanding and high clarity about the project\n",
    "2. create a solid plan, reduce ambiguity, and keep the team focused \n",
    "3. be agile, monitor progress, revise plan if required\n",
    "4. make sure every single person involved in the project is aligned\n",
    "\n",
    "Delivering a project requires very high focus, clarity, and persistence. Keep the big picture in mind, but execute with attention to detail.\n",
    "\n",
    "Even if you are early in your career, follow the above, and earn some leadership brownie points.\n",
    "\n",
    "ps: enrollments open for sys design cohort - arpitbhayani.me/course\n",
    "\n",
    "hashtag#AsliEngineering hashtag#CareerGrowth\"\"\",\n",
    "    \"\"\"Yes, materials provide a chip‚Äôs physical foundation and the substance of more powerful and compact components. But they are also integral to the advanced fabrication methods and novel chip designs that underpin the industry‚Äôs rapid progress in recent decades.\n",
    "\n",
    "For this reason, materials science is taking on a heightened importance as we grapple with the limits of miniaturization. Advanced materials are needed more than ever for the industry to unlock the new designs and technologies capable of increasing chip efficiency, speed, and power. We are seeing novel chip architectures that embrace the third dimension and stack layers to optimize surface area usage while lowering energy consumption. The industry is harnessing advanced packaging techniques, where separate ‚Äúchiplets‚Äù are fused with varying functions into a more efficient, powerful single chip. This is called heterogeneous integration.\"\"\",\n",
    "    \"\"\"\n",
    "    Data science is a good field. \n",
    "    It has a lot of potential for career growth\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Running GrID Engine with Naive Approach\n",
    "\n",
    "This code computes the information density for a subset of `TEST_INPUT_TEXTS` using the `GrIDEngineNaive`. It formats the results (text and information density) as a markdown table and displays it. The text is processed by the GrID engine, and the data is returned in a table format, showing the text and corresponding information density values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from json import dumps\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class FormattedResponse(BaseModel):\n",
    "    content: str\n",
    "\n",
    "\n",
    "def convert_markdown_to_text(markdown):\n",
    "    prompt = f\"\"\"\n",
    "    Convert the markdown to plain text.\n",
    "    Markdown: {markdown}\n",
    "    \"\"\"\n",
    "    response = llm(prompt=prompt, response_model=FormattedResponse)\n",
    "    return response.content\n",
    "\n",
    "def format_data_as_markdown_table(data):\n",
    "    \"\"\"Formats the data as a markdown table.\"\"\"\n",
    "    markdown = \"| Text | Information Density |\\n\"\n",
    "    markdown += \"| --- | --- |\\n\"\n",
    "    for item in data:\n",
    "        trimmed_text = item['text'][:50] + (item['text'][50:] and '...')\n",
    "        markdown += f\"| {trimmed_text} | {item['information_density']} |\\n\"\n",
    "    return markdown\n",
    "\n",
    "\n",
    "\n",
    "def run_naive_grid_engine():\n",
    "    grid_engine_naive = GrIDEngineNaive(verbose=True)\n",
    "    results = []\n",
    "    print(len(TEST_INPUT_TEXTS))\n",
    "    \n",
    "    for text in TEST_INPUT_TEXTS[0:2]:\n",
    "        response = grid_engine_naive.compute_information_density(text)\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"information_density\": response.information_density,\n",
    "            \"explanation\": response.explanation\n",
    "        })\n",
    "    formatted_tables_markdown = format_data_as_markdown_table(results)\n",
    "    return Markdown(formatted_tables_markdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run_naive_grid_engine()` function processes a subset of texts to compute their information density using the `GrIDEngineNaive`, formats the results into a markdown table, and returns it for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_naive_grid_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compute Information Density using GrID Engine Naive\n",
    "\n",
    "This code computes the information density of the provided `input_text` using the GrIDEngineNaive model, with `collapse_layers=False` to keep the individual analysis layers separated. It then prints the original text and the calculated information density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Excited to be featured as an NVIDIA agents partner during Jensen's keynote at CES üî•\n",
    "\n",
    "2025 is the year of production knowledge agents. Everything from document research, to automated document extraction/business logic, to report generation. \n",
    "\n",
    "This is a great way to kick off the year. Huge shoutout to Laurie Voss + the NVIDIA team (including Daniel Glogowski) for pulling these resources together.\n",
    "\n",
    "Blueprint: https://lnkd.in/gHFFKrgC\n",
    "Video: https://lnkd.in/gQi7U6sq\n",
    "Notebook: https://lnkd.in/ggmkXiYA\n",
    "Blog: https://lnkd.in/ghfApUs9\n",
    "\"\"\"\n",
    "grid_engine_naive = GrIDEngineNaive(verbose=True)\n",
    "collapse_layers = False # Set to True to collapse the layers\n",
    "\n",
    "response = grid_engine_naive.compute_information_density(input_text, collapse_layers=True)\n",
    "\n",
    "print(f\"Text: {input_text}\")\n",
    "print(f\"Information Density: {response.information_density}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code computes the information density of the given `input_text` using the GrIDEngineNaive model, with `collapse_layers=True` to combine multiple analysis layers, and prints the result alongside the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Excited to be featured as an NVIDIA agents partner during Jensen's keynote at CES üî•\n",
    "\n",
    "2025 is the year of production knowledge agents. Everything from document research, to automated document extraction/business logic, to report generation. \n",
    "\n",
    "This is a great way to kick off the year. Huge shoutout to Laurie Voss + the NVIDIA team (including Daniel Glogowski) for pulling these resources together.\n",
    "\n",
    "Blueprint: https://lnkd.in/gHFFKrgC\n",
    "Video: https://lnkd.in/gQi7U6sq\n",
    "Notebook: https://lnkd.in/ggmkXiYA\n",
    "Blog: https://lnkd.in/ghfApUs9\n",
    "\"\"\"\n",
    "grid_engine_naive = GrIDEngineNaive(verbose=True)\n",
    "collapse_layers = True # Set to True to collapse the layers\n",
    "\n",
    "response = grid_engine_naive.compute_information_density(input_text, collapse_layers=True)\n",
    "\n",
    "print(f\"Text: {input_text}\")\n",
    "print(f\"Information Density: {response.information_density}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Compare Runtime with and without Layer Collapsing\n",
    "\n",
    "The code compares the runtime of computing information density with and without collapsing the layers in the `GrIDEngineNaive` class and prints the time difference between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_runtime_with_collapsing(input_text = TEST_INPUT_TEXTS[0]):\n",
    "    print(f\"Input Text: {input_text}\")\n",
    "    import time\n",
    "    grid_engine_naive = GrIDEngineNaive(verbose=True)\n",
    "    start_time = time.time()\n",
    "    response = grid_engine_naive.compute_information_density(input_text, collapse_layers=True)\n",
    "    print(f\"Information Density (no layer collapsing): {response.information_density}\")\n",
    "    end_time = time.time()\n",
    "    time_with_collapsing = end_time - start_time\n",
    "    print(f\"Time with collapsing: {time_with_collapsing}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = grid_engine_naive.compute_information_density(input_text, collapse_layers=False)\n",
    "    end_time = time.time()\n",
    "    time_without_collapsing = end_time - start_time\n",
    "    print(f\"Information Density (with layer collapsing): {response.information_density}\")\n",
    "    print(f\"Time without collapsing: {time_without_collapsing}\")\n",
    "\n",
    "    time_delta_percent = ((time_with_collapsing - time_without_collapsing) / time_with_collapsing) * 100\n",
    "    print(f\"Time difference (collapsing is better by?): {time_delta_percent}%\")\n",
    "    return time_with_collapsing, time_without_collapsing\n",
    "\n",
    "compare_runtime_with_collapsing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "\n",
    "This notebook explores the Generative Information Density (GrID) engine for understanding and calculating information density in text. The process begins with a preliminary analysis of various components such as sentiment, topics, questions, embedded meanings, and tangent thoughts. The system leverages multiple LLMs like Groq and OpenAI to extract and analyze these components.\n",
    "\n",
    "The GrID engine computes information density using two main methods: the nucleus and naive approaches. The naive method combines the preliminary analysis of text with predefined instructions to compute a density score between 0 and 1. The collapsed layers option optimizes the analysis by merging layers for faster computation, while the non-collapsed layers approach offers a more detailed breakdown.\n",
    "\n",
    "This framework helps quantify information density, which could be used in various research and real-world applications, such as summarization, content analysis, or improving model efficiency by understanding the depth of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! üåê\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
