{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FireCrawl x LangChain Documentation RAG üî•\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img  height=\"200px\" src=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/img/firecrawl_logo.png\"> \n",
    "</p>\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img  height=\"200px\" src=\"https://images.contentstack.io/v3/assets/bltf2fca5bf44f5e817/blt34d9fdb635976e4a/669e80a79fecd86c50d59f6d/Lang_Square.png\"> \n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "A simple and effective implementation of Naive RAG (Retrieval Augmented Generation) using FireCrawl and LangChain! üöÄ\n",
    "\n",
    "- Provide a link to your Python documentation.\n",
    "\n",
    "- FireCrawl crawls and scrapes the documentation.\n",
    "\n",
    "- Use LangChain to embed the documents and store it in an in-memory vector store.\n",
    "\n",
    "- Use prompt templates to use a RAG template which has the 'query' and 'context'.\n",
    "\n",
    "- Receive detailed answers from the LLM using your documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install Requirements ‚öôÔ∏è\n",
    "\n",
    "The requirements are already specified in the `requirements.txt` file. We simply use our utility to install the requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setup Environment Variables üèïÔ∏è\n",
    "\n",
    "Make sure you have added `FIRECRAWL_API_KEY` and `OPENAI_API_KEY` to your `.env` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Initialize FireCrawl Client üî•\n",
    "\n",
    "This method intitializes the FireCrawl App with the API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "\n",
    "def get_firecrawl_client():\n",
    "    return FirecrawlApp(api_key=os.getenv(\"FIRECRAWL_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Crawling & Scraping Utilities üï∑Ô∏è\n",
    "\n",
    "We fetch the documentation links and then scrape each page in markdown format. To achieve this, we define a set of utility methods that we will call in our runner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "cache = {}\n",
    "\n",
    "\n",
    "def get_doc_links(app: FirecrawlApp, input_url: str) -> List[str]:\n",
    "    \"\"\"Gets the documentation links from the given URL.\"\"\"\n",
    "    cache_key = f\"{input_url}_links\"\n",
    "    cached_links = cache.get(cache_key)\n",
    "    if cached_links:\n",
    "        print(f\"Using cached links for URL: {input_url}\")\n",
    "        return cached_links\n",
    "\n",
    "    app = get_firecrawl_client()\n",
    "    crawl_result = app.map_url(input_url)\n",
    "\n",
    "    success = crawl_result[\"success\"]\n",
    "\n",
    "    if not success:\n",
    "        raise RuntimeError(f\"Failed to get links from URL: {input_url}\")\n",
    "\n",
    "    links = crawl_result[\"links\"]\n",
    "    cache[cache_key] = links\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def get_single_doc_from_link(app: FirecrawlApp, link: str) -> str:\n",
    "    \"\"\"Gets the documentation from the given link.\"\"\"\n",
    "    cached_doc = cache.get(link)\n",
    "    if cached_doc:\n",
    "        print(f\"Using cached docs for URL: {link}\")\n",
    "        return cached_doc\n",
    "\n",
    "    scrape_result = None\n",
    "    try:\n",
    "        scrape_result = app.scrape_url(link, params={\"formats\": [\"markdown\"]})\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get docs from URL: {link}\")\n",
    "        print(e)\n",
    "\n",
    "    if not scrape_result:\n",
    "        return None\n",
    "\n",
    "    success = scrape_result[\"metadata\"][\"statusCode\"] == 200\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to get docs from URL: {link}\")\n",
    "        return None\n",
    "\n",
    "    markdown = scrape_result[\"markdown\"]\n",
    "    cache[link] = markdown\n",
    "\n",
    "    return markdown\n",
    "\n",
    "\n",
    "def get_docs_from_links(\n",
    "    app: FirecrawlApp, links: List[str], verbose=False\n",
    ") -> List[str]:\n",
    "    \"\"\"Gets the documentation from the given list of links.\"\"\"\n",
    "    docs = []\n",
    "\n",
    "    for link in links:\n",
    "        if verbose:\n",
    "            print(f\"Getting docs from URL: {link}\")\n",
    "        markdown = get_single_doc_from_link(app, link)\n",
    "        if markdown:\n",
    "            docs.append(markdown)\n",
    "            if verbose:\n",
    "                print(f\"Fetched docs from URL: {link}\")\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Vector Store & Embeddings üîÆ\n",
    "\n",
    "Setup the vector store and embeddings using Open AI embeddings. These methods will be used to embed our documentation and load it into the vector DB for retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "DEFAULT_EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "vector_store = None\n",
    "\n",
    "\n",
    "def get_embeddings() -> List[List[float]]:\n",
    "    \"\"\"Gets the embeddings for the given list of documents.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=DEFAULT_EMBEDDING_MODEL)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def build_vectorstore(docs: List[str]) -> InMemoryVectorStore:\n",
    "    \"\"\"Builds a vector store from the given list of documents.\"\"\"\n",
    "    global vector_store\n",
    "    if vector_store:\n",
    "        print(\"Vector store already built. Return pre-computed vector store.\")\n",
    "        return vector_store\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    vector_docs = text_splitter.create_documents(docs)\n",
    "    embeddings = get_embeddings()\n",
    "    vector_store = InMemoryVectorStore(embeddings)\n",
    "    vector_store.add_documents(vector_docs)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: LLM interface and RAG prompt üß†\n",
    "\n",
    "We dynamically return our LLM client based on the model and provider. If you wish to use anthropic instead of openai, accordingly supply the `ANTHROPIC_API_KEY` in your `.env` file. For defaults, we use Open AI and GPT-4o. Additionally, we define a simple RAG prompt which holds the 'query' and 'context'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import Anthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "DEFAULT_PROVIDER = \"openai\"\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o\"\n",
    "DEFAULT_ANTHROPIC_MODEL = \"claude-3-5-sonnet-latest\"\n",
    "\n",
    "\n",
    "def get_llm(provider=DEFAULT_PROVIDER, model=DEFAULT_OPENAI_MODEL) -> ChatOpenAI:\n",
    "    \"\"\"Gets the language model.\"\"\"\n",
    "    if provider == \"openai\":\n",
    "        return ChatOpenAI(model=model)\n",
    "    elif provider == \"anthropic\":\n",
    "        return Anthropic(model=model)\n",
    "\n",
    "\n",
    "def get_rag_prompt() -> ChatPromptTemplate:\n",
    "    \"\"\"Gets the RAG prompt.\"\"\"\n",
    "    system = \"\"\"\n",
    "        You are an AI agent that can answer questions about software development.\n",
    "        Given a query and context about the query, provide a factual, and detailed response. \n",
    "        The response should be relevant to the query and context.\n",
    "        Simply respond with the answer to the query.\n",
    "    \"\"\"\n",
    "\n",
    "    user = \"\"\"\n",
    "        Query: {query}\n",
    "        Context: {context}\n",
    "    \"\"\"\n",
    "\n",
    "    return ChatPromptTemplate.from_messages([(\"system\", system), (\"user\", user)])\n",
    "\n",
    "\n",
    "def rag_search(query: str, docs: List[str]) -> str:\n",
    "    \"\"\"Performs a RAG search.\"\"\"\n",
    "    llm = get_llm()\n",
    "    vector_store = build_vectorstore(docs)\n",
    "    matched_docs = vector_store.similarity_search(query, num_results=5)\n",
    "    matched_docs = [doc.page_content for doc in matched_docs]\n",
    "    context = \"\\n\".join(matched_docs)\n",
    "    prompt = get_rag_prompt()\n",
    "    prompt_formatted = prompt.invoke(input={\"query\": query, \"context\": context})\n",
    "    llm_response = llm.invoke(input=prompt_formatted)\n",
    "    return llm_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Runner! üèÉüèª\n",
    "\n",
    "Define our 'entry point' ‚Äî from this point all our previously defined methods will be called!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def render_output(markdown: str) -> None:\n",
    "    \"\"\"Renders the generated output file as markdown.\"\"\"\n",
    "    return Markdown(markdown)\n",
    "\n",
    "\n",
    "def run(input_url: str, query=None, verbose=False, user_input=False) -> str:\n",
    "    \"\"\"Runs the program.\"\"\"\n",
    "    print(\"Starting FireCrawl x LangChain Documentation RAG! üöÄ\")\n",
    "    app = get_firecrawl_client()\n",
    "    if verbose:\n",
    "        print(f\"Getting links from URL: {input_url}\")\n",
    "\n",
    "    if query and user_input:\n",
    "        print(\"User input is enabled. Ignoring the query parameter.\")\n",
    "\n",
    "    links = get_doc_links(app, input_url)\n",
    "    if verbose:\n",
    "        print(f\"Fetched {len(links)} links.\")\n",
    "        print(\"Fetching docs from links...\")\n",
    "    docs = get_docs_from_links(app, links, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(f\"Fetched {len(docs)} docs.\")\n",
    "    if verbose:\n",
    "        print(\"Built vector store.\")\n",
    "    if user_input:\n",
    "        query = input(\"Enter your query: \")\n",
    "    response = rag_search(query, docs)\n",
    "    print(\"Done! ‚ú®\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Run ‚ö°Ô∏è\n",
    "\n",
    "Run the program. Feel free to adjust the input parameters as per your liking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FireCrawl x LangChain Documentation RAG! üöÄ\n",
      "Getting links from URL: https://pyparsing-docs.readthedocs.io/en/latest/\n",
      "Using cached links for URL: https://pyparsing-docs.readthedocs.io/en/latest/\n",
      "Fetched 9 links.\n",
      "Fetching docs from links...\n",
      "Getting docs from URL: https://pyparsing-docs.readthedocs.io/en/latest\n",
      "Using cached docs for URL: https://pyparsing-docs.readthedocs.io/en/latest\n",
      "Fetched docs from URL: https://pyparsing-docs.readthedocs.io/en/latest\n",
      "Getting docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/modules.html\n",
      "Using cached docs for URL: https://pyparsing-docs.readthedocs.io/en/latest/modules.html\n",
      "Fetched docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/modules.html\n",
      "Getting docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/whats_new_in_3_0_0.html\n",
      "Using cached docs for URL: https://pyparsing-docs.readthedocs.io/en/latest/whats_new_in_3_0_0.html\n",
      "Fetched docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/whats_new_in_3_0_0.html\n",
      "Getting docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/pyparsing.html\n",
      "Using cached docs for URL: https://pyparsing-docs.readthedocs.io/en/latest/pyparsing.html\n",
      "Fetched docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/pyparsing.html\n",
      "Getting docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/HowToUsePyparsing.html\n",
      "Using cached docs for URL: https://pyparsing-docs.readthedocs.io/en/latest/HowToUsePyparsing.html\n",
      "Fetched docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/HowToUsePyparsing.html\n",
      "Getting docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/genindex.html\n",
      "Using cached docs for URL: https://pyparsing-docs.readthedocs.io/en/latest/genindex.html\n",
      "Fetched docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/genindex.html\n",
      "Getting docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/CODE_OF_CONDUCT.html\n",
      "Using cached docs for URL: https://pyparsing-docs.readthedocs.io/en/latest/CODE_OF_CONDUCT.html\n",
      "Fetched docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/CODE_OF_CONDUCT.html\n",
      "Getting docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/_static/sql_railroad.html\n",
      "Using cached docs for URL: https://pyparsing-docs.readthedocs.io/en/latest/_static/sql_railroad.html\n",
      "Fetched docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/_static/sql_railroad.html\n",
      "Getting docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/py-modindex.html\n",
      "Using cached docs for URL: https://pyparsing-docs.readthedocs.io/en/latest/py-modindex.html\n",
      "Fetched docs from URL: https://pyparsing-docs.readthedocs.io/en/latest/py-modindex.html\n",
      "Fetched 9 docs.\n",
      "Built vector store.\n",
      "Vector store already built. Return pre-computed vector store.\n",
      "Done! ‚ú®\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "-----\n",
       "# AI Response\n",
       "Pyparsing is useful for developers because it provides a powerful and flexible way to define and execute parsing grammars directly in Python. Unlike traditional methods like lex/yacc or regular expressions, pyparsing does not require learning a new syntax for grammar definition. Instead, it offers a library of classes and methods that allow developers to construct grammars in a more intuitive and readable way using Python code.\n",
       "\n",
       "Key benefits of pyparsing for developers include:\n",
       "\n",
       "1. **Ease of Use**: By leveraging Python's syntax and constructs, pyparsing allows developers to define grammars in a straightforward manner, reducing the learning curve and complexity associated with traditional parsing tools.\n",
       "\n",
       "2. **Flexibility**: Pyparsing supports a wide range of parsing needs, from simple to complex grammars, using its comprehensive set of classes and methods like `ParseResults`, `DelimitedList`, and `one_of`.\n",
       "\n",
       "3. **Extensibility**: Developers can easily extend and customize the parsing rules to fit specific requirements by using the provided subclasses and operators.\n",
       "\n",
       "4. **Rich Feature Set**: It offers various helper methods and parse actions that streamline common parsing tasks, making it efficient to handle different parsing scenarios.\n",
       "\n",
       "5. **No External Dependencies**: Being a pure Python library, pyparsing does not rely on external tools, making it easier to integrate into Python projects.\n",
       "\n",
       "Overall, pyparsing is a versatile tool for developers looking to implement custom parsers without the overhead of traditional parsing frameworks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Parameters: Adjust the query and user_input parameters as needed\n",
    "input_url = \"https://pyparsing-docs.readthedocs.io/en/latest/\"\n",
    "query = \"How is pyparser useful for developers?\"\n",
    "user_input = False  # If set to 'true' ignores 'query' parameter and takes user input\n",
    "verbose = True  # Set to 'true' to enable verbose mode\n",
    "\n",
    "# Run the program! üöÄ\n",
    "result_markdown = run(input_url=input_url, query=query, verbose=verbose, user_input=user_input)\n",
    "render_output(\"-----\\n# AI Response\\n\" + result_markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! üåê\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
