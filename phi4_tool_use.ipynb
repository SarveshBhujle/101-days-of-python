{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi4 Tool: Advanced Query and Data Processing Utility\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "## Description\n",
    "\n",
    "The Phi4 Tool is a powerful application designed for advanced query processing, designed to handle complex data interactions with precision and efficiency. Using sophisticated validation and dynamic query handling methods, it simplifies the management of user requests and external system integrations. Ideal for developers and businesses looking to optimize workflow automation and data processing, Phi4 Tool enhances accuracy and scalability while reducing the potential for errors in high-demand environments. This versatile tool integrates seamlessly into various systems to streamline operations and improve overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Boilerplate Setup\n",
    "\n",
    "This step sets up the boilerplate code for the project by importing necessary libraries like os, load_dotenv, and clear_output. \n",
    "\n",
    "It defines global variables to track installed dependencies and required environment variables. The install_requirements function handles installing dependencies with retry logic, while setup_env loads and validates environment variables. \n",
    "\n",
    "Finally, the code ensures that the setup is complete by clearing the output and confirming the successful configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"🚀 Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Imports and Default Constants  \n",
    "\n",
    "The code imports various libraries like pydantic for data validation, json for handling JSON data, and traceback for error handling. It defines configuration variables, such as default prompt instructions and model settings.\n",
    "\n",
    " The build_dummy_pydantic_object function generates a dummy instance of a Pydantic schema, while the generate_object function sends a prompt to the Ollama API, processes the response, and validates it against a provided model. \n",
    "\n",
    "The exception handling ensures any errors are logged with detailed stack traces, returning None if an issue arises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ollama: Generate Object\n",
    "\n",
    "from typing import Union\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import traceback\n",
    "import ollama\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You are an intelligent assistant. You are helping the user with their query.\"\n",
    ")\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 100\n",
    "DEFAULT_OLLAMA_MODEL = \"phi4\"\n",
    "DEFAULT_VERBOSE = True\n",
    "DEFAULT_DEBUG = True\n",
    "\n",
    "\n",
    "def build_dummy_pydantic_object(schema: BaseModel) -> BaseModel:\n",
    "    \"\"\"\n",
    "    Build a dummy Pydantic object using the given schema.\n",
    "\n",
    "    Args:\n",
    "      schema: The Pydantic schema to build the object from\n",
    "\n",
    "    Returns:\n",
    "      BaseModel: The dummy Pydantic object\n",
    "    \"\"\"\n",
    "    return schema()\n",
    "\n",
    "\n",
    "def generate_object(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=DEFAULT_SYSTEM_PROMPT,\n",
    "    model=DEFAULT_OLLAMA_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    "    max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    debug=DEFAULT_DEBUG,\n",
    "    verbose=DEFAULT_VERBOSE,\n",
    ") -> Union[BaseModel, None]:\n",
    "    \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "    try:\n",
    "        if verbose or debug:\n",
    "            print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "        prompt_with_structured_output = f\"\"\"\n",
    "            Prompt: {prompt} \n",
    "            SCHEMA: {build_dummy_pydantic_object(response_model).model_dump_json()}\n",
    "\n",
    "            INSTRUCTIONS: \n",
    "            - RESPOND IN JSON FORMAT. \n",
    "            - STRICTLY FOLLOW THE SCHEMA.\n",
    "            - MAKE SURE TO INCLUDE ALL THE REQUIRED FIELDS.\n",
    "            - DON'T INCLUDE ANY ADDITIONAL FIELDS.\n",
    "            - INCASE YOU DON'T HAVE AN ANSWER FOR A FIELD, LEAVE IT EMPTY.\n",
    "        \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            params = {\n",
    "                \"prompt\": prompt_with_structured_output,\n",
    "                \"system\": system,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"model\": model,\n",
    "            }\n",
    "            params = json.dumps(params, indent=2)\n",
    "            print(f\"Params: {params}\")\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": prompt_with_structured_output},\n",
    "            ],\n",
    "            format=\"json\",\n",
    "        )\n",
    "\n",
    "        response_json = response.message.content  # Get the response content\n",
    "\n",
    "        if verbose or debug:\n",
    "            print(f\"{model}: Response received successfully. 🎉\")\n",
    "            print(f\"Response: {response_json}\")\n",
    "\n",
    "        response_obj = json.loads(response_json)\n",
    "        response_structured = response_model.model_validate(response_obj)\n",
    "\n",
    "        if verbose or debug:\n",
    "            print(\"Object generated successfully. 🎉\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"EasyLLM Response: {response_json}\")\n",
    "        return response_structured\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "        if debug:\n",
    "            traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate and Validate Random Movie Data\n",
    "\n",
    "#### This step defines a Pydantic model for a movie, specifying fields like title, year, genre, director, and rating with default values. \n",
    "\n",
    "The `generate_object` function is called to generate a random movie object using the model, ensuring the result adheres to the schema. \n",
    "\n",
    "The generated movie is then printed in JSON format for readability and further use. \n",
    "\n",
    "This process demonstrates how structured data can be generated and validated dynamically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title: str = \"Movie name\"\n",
    "    year: int = 0\n",
    "    genre: str = \"movie genre\"\n",
    "    director: str = \"movie director\"\n",
    "    rating: float = 0.0\n",
    "\n",
    "\n",
    "response = generate_object(\n",
    "    prompt=\"Generate a random movie\",\n",
    "    response_model=Movie,\n",
    "    verbose=False,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "print(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Tool Execution and Response Generation\n",
    "\n",
    "This step sets up a system where tools, like fetching a stock price, are executed based on user queries. It defines a ToolUseRequest schema using Pydantic to structure the request. \n",
    "\n",
    "The process involves generating the tool's input from the query, executing the corresponding function, and then handling the output. \n",
    "\n",
    "The system ensures strict adherence to the defined schema, generating a clean and formatted response with the executed tool’s result, such as the stock price for a company like \"AAPL\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "def get_stock_price(symbol: str) -> str:\n",
    "    price = random.randint(1, 200)\n",
    "    return f\"The current price of {symbol} is {str(price)}.\"\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"Get Stock Price\",\n",
    "        \"description\": \"Get the current price of a stock.\",\n",
    "        \"function\": get_stock_price,\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"name\": \"symbol\",\n",
    "                \"type\": \"str\",\n",
    "                \"description\": \"The stock symbol.\",\n",
    "            }\n",
    "        ],\n",
    "        \"output\": {\n",
    "            \"return_type\": \"str\",\n",
    "            \"description\": \"The current price of the stock.\",\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "query = \"Get the current price of AAPL.\"\n",
    "\n",
    "\n",
    "class ToolUseRequest(BaseModel):\n",
    "    tool_name: str = \"tool name\"\n",
    "    tool_input: dict = {\n",
    "        \"arg1\": \"val1\",\n",
    "        \"arg2\": \"val2\",\n",
    "        \"info\": \"This object should contain the args for the tool and the arg names should correspond to the provided tool args.\",\n",
    "    }\n",
    "\n",
    "\n",
    "prompt = f\"\"\"For the given query: {query}. \n",
    "    Tell me if you need any tools to be used and provide the input for the tool.\"\n",
    "    These are the available tools: {tools}\".\n",
    "    If no tool needs to be used respond with null values.\n",
    "    Respond strictly in the given schema format.\n",
    "    Don't make up field names or types.\n",
    "    Use the exact field names and types as given in the schema.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def call_tool(tool_name: str, tool_input: dict):\n",
    "    for tool in tools:\n",
    "        if tool[\"name\"] == tool_name:\n",
    "            return tool[\"function\"](**tool_input)\n",
    "    return \"Tool not found.\"\n",
    "\n",
    "\n",
    "response = generate_object(\n",
    "    prompt=prompt,\n",
    "    response_model=ToolUseRequest,\n",
    "    verbose=True,\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "tool_output = None\n",
    "\n",
    "if not (\n",
    "    not response.tool_name\n",
    "    or response.tool_name == \"null\"\n",
    "    or response.tool_name == \"None\"\n",
    "    or response.tool_name == \"\"\n",
    "):\n",
    "    result = call_tool(response.tool_name, response.tool_input)\n",
    "    print(\"Tool Output:\", result)\n",
    "    tool_output = result\n",
    "\n",
    "prompt = f\"\"\"Query: {query}\n",
    "    TOOLS USED:\n",
    "    Tool Name: {response.tool_name}\n",
    "    Tool Input: {response.tool_input}\n",
    "    Tool Output: {tool_output}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This code is designed to help manage user queries and connect with different external tools in a smooth and organized way. It uses a system called Pydantic to check that the information users provide is accurate and follows the required format. This helps avoid errors and ensures that the program works properly.\n",
    "\n",
    "When a user asks a question, the code decides which external tool (like an API or service) is needed to answer the query and then runs that tool. The results are then returned in a clear, expected format.\n",
    "\n",
    "The way the code is set up makes it easy to add new tools or services, which means it can be used for various tasks. For example, it could be applied in customer service systems, where the program automatically interacts with users and uses external services to help answer their questions or solve problems.\n",
    "\n",
    "In short, this system allows the program to talk to different tools and services easily, making it efficient for automating tasks, answering queries, and processing information. It is designed to be flexible, reliable, and easy to expand for future needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! 🌐\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
