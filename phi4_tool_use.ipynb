{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set.\n",
      "ðŸš€ Setup complete. Continue to the next cell.\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"ðŸš€ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ollama: Generate Object\n",
    "\n",
    "from typing import Union\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import traceback\n",
    "import ollama\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You are an intelligent assistant. You are helping the user with their query.\"\n",
    ")\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 100\n",
    "DEFAULT_OLLAMA_MODEL = \"phi4\"\n",
    "DEFAULT_VERBOSE = True\n",
    "DEFAULT_DEBUG = True\n",
    "\n",
    "\n",
    "def build_dummy_pydantic_object(schema: BaseModel) -> BaseModel:\n",
    "    \"\"\"\n",
    "    Build a dummy Pydantic object using the given schema.\n",
    "\n",
    "    Args:\n",
    "      schema: The Pydantic schema to build the object from\n",
    "\n",
    "    Returns:\n",
    "      BaseModel: The dummy Pydantic object\n",
    "    \"\"\"\n",
    "    return schema()\n",
    "\n",
    "\n",
    "def generate_object(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=DEFAULT_SYSTEM_PROMPT,\n",
    "    model=DEFAULT_OLLAMA_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    "    max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    debug=DEFAULT_DEBUG,\n",
    "    verbose=DEFAULT_VERBOSE,\n",
    ") -> Union[BaseModel, None]:\n",
    "    \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "    try:\n",
    "        if verbose or debug:\n",
    "            print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "        prompt_with_structured_output = f\"\"\"\n",
    "            Prompt: {prompt} \n",
    "            SCHEMA: {build_dummy_pydantic_object(response_model).model_dump_json()}\n",
    "\n",
    "            INSTRUCTIONS: \n",
    "            - RESPOND IN JSON FORMAT. \n",
    "            - STRICTLY FOLLOW THE SCHEMA.\n",
    "            - MAKE SURE TO INCLUDE ALL THE REQUIRED FIELDS.\n",
    "            - DON'T INCLUDE ANY ADDITIONAL FIELDS.\n",
    "            - INCASE YOU DON'T HAVE AN ANSWER FOR A FIELD, LEAVE IT EMPTY.\n",
    "        \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            params = {\n",
    "                \"prompt\": prompt_with_structured_output,\n",
    "                \"system\": system,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"model\": model,\n",
    "            }\n",
    "            params = json.dumps(params, indent=2)\n",
    "            print(f\"Params: {params}\")\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": prompt_with_structured_output},\n",
    "            ],\n",
    "            format=\"json\",\n",
    "        )\n",
    "\n",
    "        response_json = response.message.content  # Get the response content\n",
    "\n",
    "        if verbose or debug:\n",
    "            print(f\"{model}: Response received successfully. ðŸŽ‰\")\n",
    "            print(f\"Response: {response_json}\")\n",
    "\n",
    "        response_obj = json.loads(response_json)\n",
    "        response_structured = response_model.model_validate(response_obj)\n",
    "\n",
    "        if verbose or debug:\n",
    "            print(\"Object generated successfully. ðŸŽ‰\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"EasyLLM Response: {response_json}\")\n",
    "        return response_structured\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "        if debug:\n",
    "            traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityapatange/thehackersplaybook/engineering/.venv/lib/python3.12/site-packages/pydantic/main.py:477: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `int` but got `str` with value `'move year'` - serialized value may not be as expected\n",
      "  Expected `float` but got `str` with value `'movie rating'` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_json(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\":\"Moonfall\",\"year\":2022,\"genre\":\"Science Fiction\",\"director\":\"Roland Emmerich\",\"rating\":6.5}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title: str = \"Movie name\"\n",
    "    year: int = \"move year\"\n",
    "    genre: str = \"movie genre\"\n",
    "    director: str = \"movie director\"\n",
    "    rating: float = \"movie rating\"\n",
    "\n",
    "\n",
    "response = generate_object(\n",
    "    prompt=\"Generate a random movie\",\n",
    "    response_model=Movie,\n",
    "    verbose=False,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "print(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating object for prompt: For the given query: Get the current price of AAPL.. \n",
      "    Tell me if you need any tools to be used and provide the input for the tool.\"\n",
      "    These are the available tools: [{'name': 'Get Stock Price', 'description': 'Get the current price of a stock.', 'function': <function get_stock_price at 0x1174a9440>, 'input': [{'name': 'symbol', 'type': 'str', 'description': 'The stock symbol.'}], 'output': {'return_type': 'str', 'description': 'The current price of the stock.'}}]\".\n",
      "    If no tool needs to be used respond with null values.\n",
      "    Respond strictly in the given schema format.\n",
      "    Don't make up field names or types.\n",
      "    Use the exact field names and types as given in the schema.\n",
      "\n",
      "Params: {\n",
      "  \"prompt\": \"\\n            Prompt: For the given query: Get the current price of AAPL.. \\n    Tell me if you need any tools to be used and provide the input for the tool.\\\"\\n    These are the available tools: [{'name': 'Get Stock Price', 'description': 'Get the current price of a stock.', 'function': <function get_stock_price at 0x1174a9440>, 'input': [{'name': 'symbol', 'type': 'str', 'description': 'The stock symbol.'}], 'output': {'return_type': 'str', 'description': 'The current price of the stock.'}}]\\\".\\n    If no tool needs to be used respond with null values.\\n    Respond strictly in the given schema format.\\n    Don't make up field names or types.\\n    Use the exact field names and types as given in the schema.\\n \\n            SCHEMA: {\\\"tool_name\\\":\\\"tool name\\\",\\\"tool_input\\\":{\\\"arg1\\\":\\\"val1\\\",\\\"arg2\\\":\\\"val2\\\",\\\"info\\\":\\\"This object should contain the args for the tool and the arg names should correspond to the provided tool args.\\\"}}\\n\\n            INSTRUCTIONS: \\n            - RESPOND IN JSON FORMAT. \\n            - STRICTLY FOLLOW THE SCHEMA.\\n            - MAKE SURE TO INCLUDE ALL THE REQUIRED FIELDS.\\n            - DON'T INCLUDE ANY ADDITIONAL FIELDS.\\n            - INCASE YOU DON'T HAVE AN ANSWER FOR A FIELD, LEAVE IT EMPTY.\\n        \",\n",
      "  \"system\": \"You are an intelligent assistant. You are helping the user with their query.\",\n",
      "  \"temperature\": 0.5,\n",
      "  \"max_tokens\": 100,\n",
      "  \"model\": \"phi4\"\n",
      "}\n",
      "phi4: Response received successfully. ðŸŽ‰\n",
      "Response: {\n",
      "  \"tool_name\": \"Get Stock Price\",\n",
      "  \"tool_input\": {\n",
      "    \"symbol\": \"AAPL\"\n",
      "  },\n",
      "  \"info\": \"\"\n",
      "}\n",
      "Object generated successfully. ðŸŽ‰\n",
      "EasyLLM Response: {\n",
      "  \"tool_name\": \"Get Stock Price\",\n",
      "  \"tool_input\": {\n",
      "    \"symbol\": \"AAPL\"\n",
      "  },\n",
      "  \"info\": \"\"\n",
      "}\n",
      "The current price of AAPL is 66.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "def get_stock_price(symbol: str) -> str:\n",
    "    price = random.randint(1, 200)\n",
    "    return f\"The current price of {symbol} is {str(price)}.\"\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"Get Stock Price\",\n",
    "        \"description\": \"Get the current price of a stock.\",\n",
    "        \"function\": get_stock_price,\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"name\": \"symbol\",\n",
    "                \"type\": \"str\",\n",
    "                \"description\": \"The stock symbol.\",\n",
    "            }\n",
    "        ],\n",
    "        \"output\": {\n",
    "            \"return_type\": \"str\",\n",
    "            \"description\": \"The current price of the stock.\",\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "query = \"Get the current price of AAPL.\"\n",
    "\n",
    "\n",
    "class ToolUseRequest(BaseModel):\n",
    "    tool_name: str = \"tool name\"\n",
    "    tool_input: dict = {\n",
    "        \"arg1\": \"val1\",\n",
    "        \"arg2\": \"val2\",\n",
    "        \"info\": \"This object should contain the args for the tool and the arg names should correspond to the provided tool args.\",\n",
    "    }\n",
    "\n",
    "\n",
    "prompt = f\"\"\"For the given query: {query}. \n",
    "    Tell me if you need any tools to be used and provide the input for the tool.\"\n",
    "    These are the available tools: {tools}\".\n",
    "    If no tool needs to be used respond with null values.\n",
    "    Respond strictly in the given schema format.\n",
    "    Don't make up field names or types.\n",
    "    Use the exact field names and types as given in the schema.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def call_tool(tool_name: str, tool_input: dict):\n",
    "    for tool in tools:\n",
    "        if tool[\"name\"] == tool_name:\n",
    "            return tool[\"function\"](**tool_input)\n",
    "    return \"Tool not found.\"\n",
    "\n",
    "\n",
    "response = generate_object(\n",
    "    prompt=prompt,\n",
    "    response_model=ToolUseRequest,\n",
    "    verbose=True,\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "tool_output = None\n",
    "\n",
    "if not (\n",
    "    not response.tool_name\n",
    "    or response.tool_name == \"null\"\n",
    "    or response.tool_name == \"None\"\n",
    "    or response.tool_name == \"\"\n",
    "):\n",
    "    result = call_tool(response.tool_name, response.tool_input)\n",
    "    print(\"Tool Output:\", result)\n",
    "    tool_output = result\n",
    "\n",
    "prompt = f\"\"\"Query: {query}\n",
    "    TOOLS USED:\n",
    "    Tool Name: {response.tool_name}\n",
    "    Tool Input: {response.tool_input}\n",
    "    Tool Output: {tool_output}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
