{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"ðŸš€ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ollama: Generate Object\n",
    "\n",
    "from typing import Union\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import traceback\n",
    "import ollama\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You are an intelligent assistant. You are helping the user with their query.\"\n",
    ")\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 100\n",
    "DEFAULT_OLLAMA_MODEL = \"phi4\"\n",
    "DEFAULT_VERBOSE = True\n",
    "DEFAULT_DEBUG = True\n",
    "\n",
    "\n",
    "def build_dummy_pydantic_object(schema: BaseModel) -> BaseModel:\n",
    "    \"\"\"\n",
    "    Build a dummy Pydantic object using the given schema.\n",
    "\n",
    "    Args:\n",
    "      schema: The Pydantic schema to build the object from\n",
    "\n",
    "    Returns:\n",
    "      BaseModel: The dummy Pydantic object\n",
    "    \"\"\"\n",
    "    return schema()\n",
    "\n",
    "\n",
    "def generate_object(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=DEFAULT_SYSTEM_PROMPT,\n",
    "    model=DEFAULT_OLLAMA_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    "    max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    debug=DEFAULT_DEBUG,\n",
    "    verbose=DEFAULT_VERBOSE,\n",
    ") -> Union[BaseModel, None]:\n",
    "    \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "    try:\n",
    "        if verbose or debug:\n",
    "            print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "        prompt_with_structured_output = f\"\"\"\n",
    "            Prompt: {prompt} \n",
    "            SCHEMA: {build_dummy_pydantic_object(response_model).model_dump_json()}\n",
    "\n",
    "            INSTRUCTIONS: \n",
    "            - RESPOND IN JSON FORMAT. \n",
    "            - STRICTLY FOLLOW THE SCHEMA.\n",
    "            - MAKE SURE TO INCLUDE ALL THE REQUIRED FIELDS.\n",
    "            - DON'T INCLUDE ANY ADDITIONAL FIELDS.\n",
    "            - INCASE YOU DON'T HAVE AN ANSWER FOR A FIELD, LEAVE IT EMPTY.\n",
    "        \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            params = {\n",
    "                \"prompt\": prompt_with_structured_output,\n",
    "                \"system\": system,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"model\": model,\n",
    "            }\n",
    "            params = json.dumps(params, indent=2)\n",
    "            print(f\"Params: {params}\")\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": prompt_with_structured_output},\n",
    "            ],\n",
    "            format=\"json\",\n",
    "        )\n",
    "\n",
    "        response_json = response.message.content  # Get the response content\n",
    "\n",
    "        if verbose or debug:\n",
    "            print(f\"{model}: Response received successfully. ðŸŽ‰\")\n",
    "            print(f\"Response: {response_json}\")\n",
    "\n",
    "        response_obj = json.loads(response_json)\n",
    "        response_structured = response_model.model_validate(response_obj)\n",
    "\n",
    "        if verbose or debug:\n",
    "            print(\"Object generated successfully. ðŸŽ‰\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"EasyLLM Response: {response_json}\")\n",
    "        return response_structured\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "        if debug:\n",
    "            traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title: str = \"Movie name\"\n",
    "    year: int = \"move year\"\n",
    "    genre: str = \"movie genre\"\n",
    "    director: str = \"movie director\"\n",
    "    rating: float = \"movie rating\"\n",
    "\n",
    "\n",
    "response = generate_object(\n",
    "    prompt=\"Generate a random movie\",\n",
    "    response_model=Movie,\n",
    "    verbose=False,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "print(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "def get_stock_price(symbol: str) -> str:\n",
    "    price = random.randint(1, 200)\n",
    "    return f\"The current price of {symbol} is {str(price)}.\"\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"Get Stock Price\",\n",
    "        \"description\": \"Get the current price of a stock.\",\n",
    "        \"function\": get_stock_price,\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"name\": \"symbol\",\n",
    "                \"type\": \"str\",\n",
    "                \"description\": \"The stock symbol.\",\n",
    "            }\n",
    "        ],\n",
    "        \"output\": {\n",
    "            \"return_type\": \"str\",\n",
    "            \"description\": \"The current price of the stock.\",\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "query = \"Get the current price of AAPL.\"\n",
    "\n",
    "\n",
    "class ToolUseRequest(BaseModel):\n",
    "    tool_name: str = \"tool name\"\n",
    "    tool_input: dict = {\n",
    "        \"arg1\": \"val1\",\n",
    "        \"arg2\": \"val2\",\n",
    "        \"info\": \"This object should contain the args for the tool and the arg names should correspond to the provided tool args.\",\n",
    "    }\n",
    "\n",
    "\n",
    "prompt = f\"\"\"For the given query: {query}. \n",
    "    Tell me if you need any tools to be used and provide the input for the tool.\"\n",
    "    These are the available tools: {tools}\".\n",
    "    If no tool needs to be used respond with null values.\n",
    "    Respond strictly in the given schema format.\n",
    "    Don't make up field names or types.\n",
    "    Use the exact field names and types as given in the schema.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def call_tool(tool_name: str, tool_input: dict):\n",
    "    for tool in tools:\n",
    "        if tool[\"name\"] == tool_name:\n",
    "            return tool[\"function\"](**tool_input)\n",
    "    return \"Tool not found.\"\n",
    "\n",
    "\n",
    "response = generate_object(\n",
    "    prompt=prompt,\n",
    "    response_model=ToolUseRequest,\n",
    "    verbose=True,\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "tool_output = None\n",
    "\n",
    "if not (\n",
    "    not response.tool_name\n",
    "    or response.tool_name == \"null\"\n",
    "    or response.tool_name == \"None\"\n",
    "    or response.tool_name == \"\"\n",
    "):\n",
    "    result = call_tool(response.tool_name, response.tool_input)\n",
    "    print(\"Tool Output:\", result)\n",
    "    tool_output = result\n",
    "\n",
    "prompt = f\"\"\"Query: {query}\n",
    "    TOOLS USED:\n",
    "    Tool Name: {response.tool_name}\n",
    "    Tool Input: {response.tool_input}\n",
    "    Tool Output: {tool_output}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
