{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://img.utdstc.com/icon/716/88f/71688f4905ece2a5ee744eaf351ec21bd51491e02025ea4a68501cc93d847e5c:200\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "## Description:\n",
    "This project implements a sentiment analysis model for Twitter data using the BERT transformer model. The pipeline includes data preprocessing, tokenization, model training, and evaluation. It reads a dataset of Twitter posts, performs necessary preprocessing (such as encoding sentiments), and then trains a sentiment classification model using BERT. The model is capable of classifying sentiments into three categories: Positive, Negative, and Neutral. The code also includes functions for installing dependencies, setting up environment variables, and saving the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "\n",
    "- The function install_requirements installs the necessary packages from the requirements.txt file.\n",
    "\n",
    "- It ensures packages are installed, retrying a set number of times if the installation fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "install_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Environment Variables\n",
    "\n",
    "- Loads environment variables from a .env file using dotenv and checks if specific variables are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = []\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Read Dataset\n",
    "\n",
    "- Reads a CSV file containing Twitter sentiment analysis data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_dataset():\n",
    "    \"\"\"Reads the dataset\"\"\"\n",
    "    dataset = pd.read_csv(\"data/twitter_sentiment_analysis/twitter.csv\")\n",
    "    return dataset\n",
    "dataset = read_dataset()\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training and Testing Data\n",
    "\n",
    "This code snippet is responsible for preparing the dataset by processing it into the right format for model training. Here’s a breakdown of what each step does:\n",
    "\n",
    "- Reading the Dataset:\n",
    "\n",
    "    - First, it loads the dataset (a CSV file containing Twitter data) using the read_dataset() function.\n",
    "\n",
    "- Data Splitting:\n",
    "\n",
    "    - The code separates the features (X) from the target variable (y). In this case, the target variable is the sentiment of the tweet, which is labeled as Positive, Negative, or Neutral.\n",
    "    - It then splits the dataset into training and testing sets, using train_test_split(). 80% of the data is used for training, and 20% is held out for testing.\n",
    "\n",
    "- Label Encoding:\n",
    "\n",
    "    - The target sentiment column (y) is transformed into numerical values using LabelEncoder. This is necessary for the model, as machine learning algorithms require numerical data.\n",
    "\n",
    "    - The possible labels (Positive, Negative, and Neutral) are converted into integers (e.g., Positive = 1, Negative = 0, Neutral = 2).\n",
    "\n",
    "- Preprocessing the Features:\n",
    "\n",
    "    - The dataset contains both numeric and categorical features. The numeric columns are scaled using StandardScaler to ensure all values are on a similar scale, which helps the model learn more effectively.\n",
    "\n",
    "    - The categorical columns are one-hot encoded using OneHotEncoder. This converts each unique category value into a new column with binary values.\n",
    "\n",
    "    - The ColumnTransformer is used to apply these transformations to the respective columns in a single step.\n",
    "\n",
    "- Converting to Tensors:\n",
    "\n",
    "    - After preprocessing, the features (X_train and X_test) are converted into PyTorch tensors. This allows the data to be used in a PyTorch-based model for training and testing.\n",
    "    - Similarly, the target variable (y_train and y_test) is also converted into tensors and reshaped.\n",
    "\n",
    "- Return Processed Data:\n",
    "\n",
    "    - Finally, the function returns the processed training and testing datasets: X_train, X_test, y_train, and y_test, which can now be used to train and evaluate the sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def get_train_test_data():\n",
    "    \"\"\"Prepare the training and testing data.\"\"\"\n",
    "    dataset = read_dataset()\n",
    "    target_column = \"sentiment\"\n",
    "    X = dataset.drop(columns=[target_column])\n",
    "    y = dataset[target_column]\n",
    "    \n",
    "    # Encode target labels into numerical values\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Separate columns into categorical and numeric\n",
    "    categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "    numeric_cols = X.select_dtypes(include=[\"number\"]).columns\n",
    "    \n",
    "    # Preprocessing the data (Scaling numeric columns and One-Hot Encoding categorical columns)\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[(\"num\", StandardScaler(), numeric_cols),\n",
    "                      (\"cat\", OneHotEncoder(sparse_output=False), categorical_cols)])\n",
    "    \n",
    "    # Split the dataset into training and testing data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Apply preprocessing and convert to PyTorch tensors\n",
    "    X_train = torch.tensor(preprocessor.fit_transform(X_train), dtype=torch.float32)\n",
    "    X_test = torch.tensor(preprocessor.transform(X_test), dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "dataset_path = \"data/twitter_sentiment_analysis/twitter.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Preprocessing and Dataset Class for Sentiment Analysis using BERT\n",
    "\n",
    "This code focuses on preparing the dataset for a BERT-based sentiment analysis model. The main tasks include:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "\n",
    "   - **Handling Missing Values**: The `content` column in the dataset (which contains the text of tweets) may have missing values. The missing values are filled with empty strings using the `fillna()` method, ensuring there are no NaN values in the text data.\n",
    "\n",
    "   - **Converting Sentiment Labels to Numerical Values**: The sentiment column in the dataset (`Positive`, `Negative`, `Neutral`) is converted to numerical labels using the function `sentiment_to_label`. This is necessary because machine learning models require numerical labels, and each sentiment is mapped to an integer (e.g., `Positive` → 1, `Negative` → 0, `Neutral` → 2).\n",
    "\n",
    "2. **Dataset Class**:\n",
    "   - The `TwitterDataset` class inherits from PyTorch's `Dataset` class, which is essential for working with data in PyTorch.\n",
    "\n",
    "   - **Constructor (`__init__`)**:\n",
    "\n",
    "     - Takes in the data, the tokenizer, and the maximum length of the tokenized sequences.\n",
    "\n",
    "     - The tokenizer (in this case, a BERT tokenizer) will convert tweets into tokenized sequences, which will be input into the BERT model.\n",
    "\n",
    "   - **`__len__` Method**:\n",
    "     - Returns the number of examples in the dataset. This method is required by PyTorch’s `DataLoader` to determine how many samples are in the dataset.\n",
    "\n",
    "   - **`__getitem__` Method**:\n",
    "     - For each data sample (in this case, a tweet), this method retrieves the tweet text and its associated sentiment label.\n",
    "\n",
    "     - The `tweet` is tokenized using the BERT tokenizer. The tokenization process includes padding the sequence to the specified `max_length` and truncating longer sequences. The result is a dictionary containing the `input_ids`, `attention_mask`, and the sentiment label.\n",
    "\n",
    "     - The `input_ids` represent the tokenized form of the tweet, while the `attention_mask` tells the model which tokens are actual words and which are padding.\n",
    "\n",
    "     - The `label` is the sentiment label (numerical value) associated with the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load your dataset\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Ensure the 'content' column has only strings and handle missing values\n",
    "dataset[\"content\"] = dataset[\"content\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Preprocessing sentiment to numerical labels\n",
    "def sentiment_to_label(sentiment):\n",
    "    return {\"Positive\": 1, \"Negative\": 0, \"Neutral\": 2}.get(\n",
    "        sentiment, 2\n",
    "    )  # Default to Neutral if not found\n",
    "\n",
    "dataset[\"sentiment_label\"] = dataset[\"sentiment\"].apply(sentiment_to_label)\n",
    "\n",
    "# Dataset Class\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tweet = self.data.iloc[index][\"content\"]\n",
    "        sentiment = self.data.iloc[index][\"sentiment_label\"]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            tweet,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(sentiment, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model and Training Setup\n",
    "\n",
    "In this section, we define the hyperparameters, prepare the dataset using PyTorch's `DataLoader`, build the model, and set up the loss function and optimizer for training. This step is crucial to fine-tune a pre-trained BERT model for sentiment classification.\n",
    "\n",
    "#### 1. **Hyperparameters**:\n",
    "\n",
    "   - **BATCH_SIZE**: Defines the number of samples processed in one batch. Here, it is set to 16.\n",
    "\n",
    "   - **MAX_LENGTH**: Specifies the maximum length of the input sequence. Any sequence longer than this is truncated, and shorter ones are padded. This is set to 128 tokens.\n",
    "\n",
    "   - **EPOCHS**: Number of times the entire dataset is passed through the model. We are training for 3 epochs.\n",
    "\n",
    "   - **LEARNING_RATE**: The learning rate for the optimizer. A smaller learning rate (2e-5) is chosen to fine-tune the pre-trained BERT model.\n",
    "\n",
    "#### 2. **Tokenizer**:\n",
    "\n",
    "   - We use the `BertTokenizer` from the Hugging Face `transformers` library, loading the pre-trained `bert-base-uncased` model to tokenize the text data into a format suitable for input to BERT.\n",
    "\n",
    "#### 3. **Dataset and DataLoaders**:\n",
    "\n",
    "   - The dataset is shuffled (`frac=1`) and split into training and validation sets. \n",
    "   - **Training Set**: 80% of the data is used for training.\n",
    "\n",
    "   - **Validation Set**: The remaining 20% is used for validation.\n",
    "\n",
    "   - Both the training and validation datasets are converted into `DataLoader` objects, which handle batching and shuffling for training.\n",
    "\n",
    "#### 4. **Model Architecture**:\n",
    "\n",
    "   - **SentimentClassifier**:\n",
    "     - A custom class that extends `nn.Module` and builds the model architecture.\n",
    "     - **BERT Layer**: The `BertModel` from Hugging Face is used for feature extraction. It outputs contextual embeddings for each token.\n",
    "\n",
    "     - **Dropout Layer**: A dropout layer with a rate of 0.3 is applied for regularization to avoid overfitting.\n",
    "\n",
    "     - **Output Layer**: A linear layer is used to produce predictions corresponding to the number of sentiment classes (3 classes: Positive, Negative, and Neutral).\n",
    "   \n",
    "   - The `forward` method takes `input_ids` and `attention_mask` as inputs and passes them through the BERT model to obtain the hidden state. The output is then passed through a dropout layer before being fed into the output layer for classification.\n",
    "\n",
    "#### 5. **Loss Function and Optimizer**:\n",
    "\n",
    "   - **Loss Function**: Cross-Entropy Loss is used for multi-class classification. This is appropriate since we have three sentiment classes.\n",
    "\n",
    "   - **Optimizer**: AdamW (Adam with Weight Decay) is used as the optimizer, which is commonly used for training transformer models like BERT. The learning rate is set to 2e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 128\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)  # Shuffle dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_data = dataset[:train_size]\n",
    "val_data = dataset[train_size:]\n",
    "\n",
    "train_dataset = TwitterDataset(train_data, tokenizer, MAX_LENGTH)\n",
    "val_dataset = TwitterDataset(val_data, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "model = SentimentClassifier(n_classes=3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train and Evaluate the Model\n",
    "\n",
    "This section contains functions for training and evaluating the sentiment analysis model. Additionally, it includes the training loop that iterates over multiple epochs, computes losses and accuracy for both the training and validation sets, and saves the trained model.\n",
    "\n",
    "#### 1. **Training Function**:\n",
    "\n",
    "   - **`train_epoch`**:\n",
    "\n",
    "     - This function is responsible for training the model for one epoch.\n",
    "\n",
    "     - **Parameters**:\n",
    "     \n",
    "       - `model`: The model to train.\n",
    "\n",
    "       - `data_loader`: The DataLoader for the training dataset.\n",
    "\n",
    "       - `criterion`: The loss function (Cross-Entropy Loss).\n",
    "\n",
    "       - `optimizer`: The optimizer (AdamW).\n",
    "\n",
    "       - `device`: The device (CUDA or CPU) where the model and data are located.\n",
    "\n",
    "     - **Process**:\n",
    "\n",
    "       - The model is set to training mode (`model.train()`).\n",
    "\n",
    "       - For each batch:\n",
    "\n",
    "         - Input data (`input_ids`, `attention_mask`, and `labels`) is moved to the device.\n",
    "\n",
    "         - The model's predictions (`outputs`) are calculated.\n",
    "\n",
    "         - The loss is computed using the criterion.\n",
    "\n",
    "         - The gradients are zeroed, backpropagation is performed, and the optimizer step is applied.\n",
    "\n",
    "       - The accuracy and loss are accumulated to return at the end of the epoch.\n",
    "\n",
    "#### 2. **Evaluation Function**:\n",
    "\n",
    "   - **`eval_model`**:\n",
    "\n",
    "     - This function evaluates the model on the validation dataset.\n",
    "\n",
    "     - **Parameters**:\n",
    "\n",
    "       - `model`: The model to evaluate.\n",
    "\n",
    "       - `data_loader`: The DataLoader for the validation dataset.\n",
    "\n",
    "       - `criterion`: The loss function (Cross-Entropy Loss).\n",
    "\n",
    "       - `device`: The device where the model and data are located.\n",
    "\n",
    "     - **Process**:\n",
    "       - The model is set to evaluation mode (`model.eval()`).\n",
    "\n",
    "       - For each batch:\n",
    "\n",
    "         - Input data is moved to the device.\n",
    "\n",
    "         - The model's predictions (`outputs`) are calculated.\n",
    "\n",
    "         - The loss is computed using the criterion.\n",
    "\n",
    "       - The accuracy and loss are accumulated to return at the end of the evaluation.\n",
    "\n",
    "\n",
    "#### 3. **Training Loop**:\n",
    "\n",
    "   - The loop iterates over the number of epochs defined by `EPOCHS`.\n",
    "\n",
    "   - For each epoch:\n",
    "\n",
    "     - The model is trained using `train_epoch`.\n",
    "\n",
    "     - The model is evaluated on the validation set using `eval_model`.\n",
    "\n",
    "   - The training and validation loss and accuracy for each epoch are printed.\n",
    "\n",
    "   - The model's state dictionary is saved after training is complete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    i = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(data_loader)\n",
    "\n",
    "# Evaluation Function\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(data_loader)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_acc, val_loss = eval_model(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), \"sentiment_model.pth\")\n",
    "\n",
    "print(\"Model training complete!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This sentiment analysis application uses a deep learning model based on BERT (Bidirectional Encoder Representations from Transformers) to classify tweets into three sentiment categories: Positive, Negative, and Neutral. The app follows a systematic approach to preprocess the data, tokenize the text using BERT's tokenizer, and train a model to predict sentiment.\n",
    "\n",
    "- Key highlights of the app include:\n",
    "\n",
    "    - `Data Preprocessing` : It handles missing values and encodes sentiment labels into numerical format for training.\n",
    "\n",
    "    - `Model Architecture`: The app uses a pre-trained BERT model, which has been fine-tuned to adapt to the sentiment classification task. A dropout layer is applied to prevent overfitting, and a fully connected layer is used for output classification.\n",
    "\n",
    "    - `Training and Evaluation`: The app includes functions for training and evaluating the model across multiple epochs, monitoring the loss and accuracy on both training and validation datasets.\n",
    "\n",
    "    - `Performance Monitoring`: The training loop prints the loss and accuracy after each epoch, allowing for easy tracking of model performance.\n",
    "\n",
    "    - `Model Saving`: After training, the model is saved to a file, allowing for future predictions or model loading without retraining.\n",
    "\n",
    "Overall, this application provides a robust framework for sentiment analysis of tweets, leveraging the power of BERT to achieve high accuracy and efficiency. It can be further extended to perform sentiment analysis on larger datasets or integrated into a web application for real-time tweet sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! 🌐\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
