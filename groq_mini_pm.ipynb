{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "    load_dotenv()\n",
    "\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "    if GROQ_API_KEY is None:\n",
    "        print(\"Please set the GROQ_API_KEY environment variable.\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        print(\"GROQ_API_KEY is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from groq import Groq\n",
    "import traceback\n",
    "from pydantic import BaseModel\n",
    "from typing import Union\n",
    "\n",
    "DEFAULT_MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "\n",
    "class LLMErrorResponse(BaseModel):\n",
    "    error: str\n",
    "\n",
    "\n",
    "def get_groq_client():\n",
    "    \"\"\"Returns an instance of the Groq class\"\"\"\n",
    "    groq = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    client = instructor.from_groq(groq, mode=instructor.Mode.JSON)\n",
    "    return client\n",
    "\n",
    "\n",
    "def llm(\n",
    "    prompt: str,\n",
    "    response_model: BaseModel,\n",
    "    system=\"You are a helpful AI assistant. The user will talk to you and its your job to provide detailed and clear responses.\",\n",
    "    model=DEFAULT_MODEL,\n",
    ") -> Union[BaseModel, LLMErrorResponse]:\n",
    "    \"\"\"Calls LLM API with the given prompt. Defaults to llama-3.3-70b-versatile\"\"\",\n",
    "    try:\n",
    "        client = get_groq_client()\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages, model=model, response_model=response_model\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return LLMErrorResponse(error=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Any\n",
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "class UserStory(BaseModel):\n",
    "    \"\"\"Represents a User Story\"\"\"\n",
    "\n",
    "    title: str\n",
    "    description: str\n",
    "    acceptance_criteria: List[str]\n",
    "    story_points: int\n",
    "\n",
    "\n",
    "class UserStories(BaseModel):\n",
    "    \"\"\"Represents a collection of User Stories\"\"\"\n",
    "\n",
    "    user_stories: List[UserStory]\n",
    "\n",
    "\n",
    "class PRD(BaseModel):\n",
    "    \"\"\"Represents a Product Requirements Document (PRD)\"\"\"\n",
    "\n",
    "    title: str\n",
    "    overview: str\n",
    "    problem_statement: str\n",
    "    business_domains: List[str]\n",
    "    proposed_solution: str\n",
    "    target_audience: str\n",
    "    features: List[str]\n",
    "    constraints: List[str]\n",
    "    assumptions: List[str]\n",
    "    dependencies: List[str]\n",
    "    user_personas: List[str]\n",
    "    conclusion: str\n",
    "\n",
    "\n",
    "class MiniPM:\n",
    "    \"\"\"An AI agent that acts as a Product Manager to help project managers create PRD and User Stories for their projects.\"\"\"\n",
    "\n",
    "    llm: Groq\n",
    "    project_description: str\n",
    "\n",
    "    def __init__(self, project_description: str):\n",
    "        \"\"\"Initializes the MiniPM AI agent with the project description.\"\"\"\n",
    "        self.project_description = project_description\n",
    "        self.llm = get_groq_client()\n",
    "\n",
    "    def get_prd(\n",
    "        self, response_format=\"markdown\", augment=True\n",
    "    ) -> Union[PRD, LLMErrorResponse, Any]:\n",
    "        \"\"\"Gets the Product Requirements Document (PRD) for the project.\"\"\"\n",
    "        system = \"\"\"\n",
    "                You are MiniPM; a Product Manager AI agent. \n",
    "                You help project managers to create a Product Requirements Document (PRD) for their projects.\n",
    "                \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                Provide a Product Requirements Document (PRD) for the project with the following details:\n",
    "                Details: {self.project_description}\n",
    "                \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=PRD, system=system)\n",
    "\n",
    "        if augment:\n",
    "            response = self._augment_prd(response)\n",
    "\n",
    "        if response_format == \"json\":\n",
    "            return response.model_dump_json()\n",
    "        elif response_format == \"markdown\":\n",
    "            response_dict = response.model_dump()\n",
    "            keys = response_dict.keys()\n",
    "            markdown = f\"# Product Requirements Document (PRD) for {response.title}\\n\"\n",
    "            for key in keys:\n",
    "                title = key.title().replace(\"_\", \" \").replace(\"Prd\", \"PRD\")\n",
    "                if key == \"title\":\n",
    "                    title = \"\"\n",
    "                    continue\n",
    "                markdown += f\"## {title}\\n\"\n",
    "                value = response_dict[key]\n",
    "                if type(value) == list:\n",
    "                    value_bullets = [f\"- {item}\" for item in value]\n",
    "                    value = \"\\n\".join(value_bullets)\n",
    "                    markdown += f\"{value}\\n\"\n",
    "                else:\n",
    "                    markdown += f\"{response_dict[key]}\\n\"\n",
    "            return markdown\n",
    "        elif response_format == \"dto\":\n",
    "            return response\n",
    "        raise ValueError(\"Invalid response_format. Use 'json', 'markdown' or 'dto'.\")\n",
    "\n",
    "    def _augment_prd(self, prd: PRD) -> PRD:\n",
    "        \"\"\"Augments the PRD with greater details.\"\"\"\n",
    "        prd_json = prd.model_dump()\n",
    "        print(\"Augmenting PRD: \", prd_json)\n",
    "        keys = prd_json.keys()\n",
    "        for key in keys:\n",
    "            if key == \"title\":\n",
    "                continue\n",
    "            value = prd_json[key]\n",
    "            prompt = f\"Provide more details about the {key} for the project.\"\n",
    "            if type(value) == str:\n",
    "                prompt += f\" Current value: {value}\"\n",
    "                expanded_value = llm(prompt, response_model=str)\n",
    "                prd_json[key] = expanded_value\n",
    "            elif type(value) == list:\n",
    "                expanded_values = []\n",
    "                for item in value:\n",
    "                    prompt += f\" Current value: {item}\"\n",
    "                    expanded_item = llm(prompt, response_model=str)\n",
    "                    expanded_values.append(expanded_item)\n",
    "                prd_json[key] = expanded_values\n",
    "        print(\"Augmented PRD: \", prd_json)\n",
    "        return PRD(**prd_json)\n",
    "\n",
    "    def get_user_stories(self) -> Union[UserStories, LLMErrorResponse]:\n",
    "        \"\"\"Gets the User Stories for the project\"\"\"\n",
    "        system = \"\"\"\n",
    "                You are MiniPM; a Product Manager AI agent. \n",
    "                You help project managers to create User Stories for their projects.\n",
    "                \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                Provide User Stories for the project with the following details:\n",
    "                Details: {self.project_description}\n",
    "                PRD: {self.get_prd()}\n",
    "                \"\"\"\n",
    "        response = llm(prompt=prompt, response_model=UserStories, system=system)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def render_output(markdown: str) -> None:\n",
    "    \"\"\"Renders the generated output file as markdown.\"\"\"\n",
    "    return Markdown(markdown)\n",
    "\n",
    "\n",
    "def save_markdown_file(markdown: str, file_path: str) -> None:\n",
    "    \"\"\"Saves the generated markdown file to the specified path.\"\"\"\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input Parameters: Change these values to get different PRD and User Stories\n",
    "project_description = (\n",
    "    \"A web application for tracking investigating behavioural change in aliens.\"\n",
    ")\n",
    "output_file = \"alien_behavioural_change_prd.md\"\n",
    "augment = True\n",
    "\n",
    "## Create an instance of MiniPM AI agent\n",
    "minipm = MiniPM(project_description)\n",
    "\n",
    "## Get the Product Requirements Document (PRD)\n",
    "prd = minipm.get_prd(response_format=\"markdown\", augment=augment)\n",
    "\n",
    "save_markdown_file(prd, output_file)\n",
    "\n",
    "## Render the PRD as markdown\n",
    "render_output(prd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input Parameters: Change these values to get different PRD and User Stories\n",
    "project_description = \"A web application for tracking fitness goals and workouts.\"\n",
    "\n",
    "## Create an instance of MiniPM AI agent\n",
    "minipm = MiniPM(project_description)\n",
    "\n",
    "## Get the Product Requirements Document (PRD)\n",
    "user_stories = minipm.get_user_stories()\n",
    "\n",
    "## Render the PRD as markdown\n",
    "print(user_stories.model_dump_json(indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
