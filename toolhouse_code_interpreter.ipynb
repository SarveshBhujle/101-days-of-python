{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesla Smart Routing Reporter\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "## Description:\n",
    "\n",
    "The Tesla Smart Routing Reporter is a Python-based intelligent system designed to analyze travel routes, traffic, and energy consumption for Tesla vehicles using a JSON dataset. It leverages OpenAI's GPT API integrated with the Toolhouse system design tools to process data, provide insights, and generate visually appealing reports. The app automates the evaluation of routes, calculates energy efficiency, and provides actionable information in markdown reports for electric vehicle optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Boilerplate And Requirements Setup:\n",
    "\n",
    "This initializes the environment and ensures all necessary dependencies are installed and environment variables are configured.\n",
    "\n",
    "- `install_requirements()`: Installs the required dependencies using the requirements.txt file.\n",
    "\n",
    "- `setup_env()`: Verifies the presence of critical environment variables (OPENAI_API_KEY and TOOLHOUSE_API_KEY).\n",
    "\n",
    "- Key Libraries:\n",
    "\n",
    "    - `dotenv`: Loads environment variables from a .env file.\n",
    "\n",
    "    - `os`: Manages environment variables and system commands.\n",
    "\n",
    "    - `IPython.display`: Clears output in a Jupyter notebook.\n",
    "\n",
    "The setup ensures smooth operation by preparing the environment with required tools and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\", \"TOOLHOUSE_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv(override=True)\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"🚀 Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: API Integration and Toolhouse Setup: \n",
    "\n",
    "### 1. **Imports**\n",
    "The script begins by importing essential libraries and tools:\n",
    "\n",
    "- **OS Library:**  \n",
    "  Used to access environment variables for securely storing and retrieving API keys.\n",
    "  \n",
    "- **OpenAI Client:**  \n",
    "  Enables communication with OpenAI's GPT models for natural language processing.\n",
    "\n",
    "- **Toolhouse Client:**  \n",
    "  Facilitates the integration of advanced tools, such as a system design assistant.\n",
    "\n",
    "- **Traceback Library:**  \n",
    "  Captures detailed error messages for debugging purposes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **API Key Retrieval**\n",
    "API keys for **OpenAI** and **Toolhouse** are fetched from environment variables to securely authenticate requests. This method avoids hardcoding sensitive information directly in the script.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Iteration Limit**\n",
    "A constant is defined to limit the maximum number of iterations for tool-based interactions. This prevents infinite loops when handling complex tasks that may require multiple tool calls.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Client Initialization**\n",
    "- The **Toolhouse client** is initialized with the API key and linked to OpenAI as the provider, enabling it to use **Toolhouse tools** alongside GPT.\n",
    "- The **OpenAI client** is also initialized to interact with GPT models and process user queries.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Function Definition**\n",
    "A wrapper function is defined to handle interactions with GPT models while incorporating **Toolhouse tools**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Parameters of the Function**\n",
    "- **Prompt:**  \n",
    "  The user’s input, describing the task or query to be processed.\n",
    "  \n",
    "- **Model:**  \n",
    "  Specifies which GPT model to use (e.g., an optimized version of GPT-4).\n",
    "  \n",
    "- **System Role:**  \n",
    "  Defines the assistant's behavior or persona (e.g., a helpful AI assistant).\n",
    "  \n",
    "- **Intermediate Output:**  \n",
    "  If enabled, logs intermediate responses for debugging or monitoring progress.\n",
    "  \n",
    "- **Verbose Mode:**  \n",
    "  Prints additional logs to aid in debugging.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Message Initialization**\n",
    "A message list is prepared to start the conversation, including:\n",
    "\n",
    "- A **system role** defining the assistant's behavior.\n",
    "- The **user’s prompt** describing their task.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Initial API Call**\n",
    "The script sends the initial request to OpenAI's GPT API with:\n",
    "\n",
    "- The chosen **model**.\n",
    "- The prepared **message list**.\n",
    "- A list of **tools** (e.g., the system design assistant) provided by Toolhouse.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Tool Execution**\n",
    "The script evaluates the response from GPT to identify if tools are needed. If required:\n",
    "\n",
    "- **Tools are executed**, and their outputs are incorporated into the conversation.\n",
    "- This process repeats iteratively until all tool calls are resolved or the iteration limit is reached.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Iterative Refinement**\n",
    "The script repeatedly:\n",
    "\n",
    "- Checks if **additional tool calls** are needed.\n",
    "- Updates the **message list** with new responses or tool outputs.\n",
    "- Refines the overall response with each iteration, ensuring accuracy and completeness.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Error Handling**\n",
    "If any issues arise during execution:\n",
    "\n",
    "- The script captures **detailed error information** using the traceback library.\n",
    "- This information is logged for debugging without crashing the application.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. **Final Output**\n",
    "Once all tools have been executed and the response is refined:\n",
    "\n",
    "- The script returns the **final output**, which is a polished and complete answer to the user’s query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: LLM with Toolhouse\n",
    "## Include the following tools in the Toolhouse bundle: thp-system-design-assistant\n",
    "## Sendgrid, Web Scraper, Code Interpreter, and Memory tools are included in this bundle.\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from toolhouse import Toolhouse\n",
    "import traceback\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TOOLHOUSE_API_KEY = os.getenv(\"TOOLHOUSE_API_KEY\")\n",
    "MAX_ITERS = 10\n",
    "th = Toolhouse(api_key=TOOLHOUSE_API_KEY, provider=\"openai\")\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "def llm(\n",
    "    prompt: str,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system=\"You are a helpful AI assistant\",\n",
    "    intermediate_output=True,\n",
    "    verbose=True,\n",
    ") -> str:\n",
    "    \"\"\"Wrapper over Open AI LLM calls with Toolhouse Tools.\"\"\"\n",
    "    try:\n",
    "        if verbose:\n",
    "            print(\"System: \", system)\n",
    "            print(\"Prompt: \", prompt)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=th.get_tools(\"thp-system-design-assistant\"),\n",
    "        )\n",
    "        messages += th.run_tools(response)\n",
    "\n",
    "        tool_calls = response.choices[0].message.tool_calls\n",
    "        content = response.choices[0].message.content\n",
    "        iters = 0\n",
    "\n",
    "        while tool_calls and len(tool_calls) > 0 and iters < MAX_ITERS:\n",
    "            print(\"Tools: \", tool_calls)\n",
    "            if verbose or intermediate_output:\n",
    "                if content:\n",
    "                    print(f\"Cycle {iters}: \", content)\n",
    "                else:\n",
    "                    print(f\"Cycle {iters}: No content.\")\n",
    "            response = openai.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                tools=th.get_tools(\"thp-system-design-assistant\"),\n",
    "            )\n",
    "            messages += th.run_tools(response)\n",
    "            if verbose or intermediate_output:\n",
    "                print(f\"Generated intermediate response...\")\n",
    "            content = response.choices[0].message.content\n",
    "            tool_calls = response.choices[0].message.tool_calls\n",
    "            iters += 1\n",
    "\n",
    "        if content and (verbose or intermediate_output):\n",
    "            print(f\"Final response: {content}\")\n",
    "\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return \"Sorry, failed to generate response.\\nError: \" + str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating Routing Data\n",
    "\n",
    "The **ROUTING_DATA Dictionary** provides a structured representation of various aspects needed for calculating routes, energy consumption, and comparing different travel options for the Tesla Model 3. Here's a breakdown of each key component:\n",
    "\n",
    "### 1. **Locations**:  \n",
    "\n",
    "This is a list of citie\n",
    "s, each represented by:\n",
    "- `id`: A unique identifier for each location (e.g., A, B, C).\n",
    "\n",
    "- `coordinates`: The geographic coordinates (latitude and long\n",
    "itude) of the city.\n",
    "\n",
    "- `name`: The name of the city (e.g., San Francisco, Los Angeles, Fresno).\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Traffic Data**:  \n",
    "\n",
    "This section defines the t\n",
    "raffic conditions between pairs of locations:\n",
    "\n",
    "- `from`: The starting location (using the `id`).\n",
    "\n",
    "- `to`: The destination location (using the `id`).\n",
    "\n",
    "- `distance_km`: The distance between the locations in kilometers.\n",
    "\n",
    "- `traffic_level`: The traffic conditions (light, moderate, heavy).\n",
    "\n",
    "- `speed_kmph`: The average speed of the route considering the traffic level.\n",
    "\n",
    "\n",
    "### 3. **Elevation Data**:  \n",
    "\n",
    "This defines the elevation (in meters) for each location, which is crucial for calculating energy consumption when traveling uphill or downhill.\n",
    "\n",
    "### 4. **Vehicle Specifications**:  \n",
    "\n",
    "This section provides details about the Tesla Model 3:\n",
    "\n",
    "- `model`: The model of the vehicle.\n",
    "\n",
    "- `efficiency_wh_per_km`: The energy efficiency of the vehicle, measured in watt-hours per kilometer (Wh/km).\n",
    "\n",
    "- `battery_capacity_wh`: The total battery capacity of the vehicle in watt-hours (Wh).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Prompts List**:  \n",
    "\n",
    "The **prompts list** consists of tasks the system needs to process using the **ROUTING_DATA**. Here's an overview of the prompts:\n",
    "\n",
    "1. **Load and Display Data**:  \n",
    "\n",
    "   - Load the routing data and display the locations, traffic, and elevation in an easy-to-read format.\n",
    "\n",
    "2. **Create a Map with Routes**:  \n",
    "\n",
    "   - Generate a map visualizing the routes between locations (A, B, and C), with distance and traffic labels.\n",
    "\n",
    "3. **Calculate Travel Time**:  \n",
    "\n",
    "   - Compute the travel time for each route by considering traffic and average speeds.\n",
    "\n",
    "4. **Energy Usage Formula**:  \n",
    "\n",
    "   - Write a formula to calculate additional energy used for elevation gains (for every 100 meters of elevation gain, the vehicle uses an additional 10 Wh/km).\n",
    "\n",
    "5. **Energy-Efficient Route**:  \n",
    "\n",
    "   - Compute the most energy-efficient route from San Francisco (A) to Los Angeles (B), considering distance, traffic, speed, and elevation.\n",
    "\n",
    "6. **Comparison Table**:  \n",
    "\n",
    "   - Generate a table comparing all routes between A and B, including metrics like travel time, elevation gain, energy consumption (Wh), and remaining battery percentage.\n",
    "\n",
    "7. **Simulate Traffic and Elevation Changes**:  \n",
    "\n",
    "   - Simulate how changes in traffic levels or elevation impact the energy consumption for the route from A to B.\n",
    "\n",
    "8. **Bar Chart for Energy Consumption**:  \n",
    "\n",
    "   - Create a bar chart to visualize energy consumption across the different routes between A and B.\n",
    "\n",
    "9. **Heatmap for Reachable Locations**:  \n",
    "\n",
    "   - Create a heatmap to show which locations can be reached from San Francisco (A) with 80% of the vehicle's battery, considering its energy specs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose of ROUTING_DATA**:  \n",
    "\n",
    "The **ROUTING_DATA** dictionary provides all the necessary data points to evaluate different routes, factoring in distance, traffic, elevation, and energy consumption. It enables the system to generate detailed insights and perform complex calculations related to route selection, energy efficiency, and travel time for the Tesla Model 3.\n",
    "\n",
    "### **Prompts Purpose**:  \n",
    "\n",
    "The prompts direct the system to analyze the data and generate specific outputs like travel times, energy consumption, visual maps, and charts. These insights are useful for tasks such as optimizing routes for energy efficiency, understanding how traffic affects travel, and estimating how far the Tesla Model 3 can travel with different levels of battery charge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tesla Smart Routing Reporter\n",
    "\n",
    "ROUTING_DATA = {\n",
    "    \"locations\": [\n",
    "        {\"id\": \"A\", \"coordinates\": [37.7749, -122.4194], \"name\": \"San Francisco\"},\n",
    "        {\"id\": \"B\", \"coordinates\": [34.0522, -118.2437], \"name\": \"Los Angeles\"},\n",
    "        {\"id\": \"C\", \"coordinates\": [36.7783, -119.4179], \"name\": \"Fresno\"},\n",
    "    ],\n",
    "    \"traffic_data\": [\n",
    "        {\n",
    "            \"from\": \"A\",\n",
    "            \"to\": \"C\",\n",
    "            \"distance_km\": 300,\n",
    "            \"traffic_level\": \"moderate\",\n",
    "            \"speed_kmph\": 80,\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"C\",\n",
    "            \"to\": \"B\",\n",
    "            \"distance_km\": 200,\n",
    "            \"traffic_level\": \"heavy\",\n",
    "            \"speed_kmph\": 50,\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"A\",\n",
    "            \"to\": \"B\",\n",
    "            \"distance_km\": 500,\n",
    "            \"traffic_level\": \"light\",\n",
    "            \"speed_kmph\": 100,\n",
    "        },\n",
    "    ],\n",
    "    \"elevation_data\": [\n",
    "        {\"id\": \"A\", \"elevation_m\": 16},\n",
    "        {\"id\": \"B\", \"elevation_m\": 71},\n",
    "        {\"id\": \"C\", \"elevation_m\": 94},\n",
    "    ],\n",
    "    \"vehicle_specs\": {\n",
    "        \"model\": \"Tesla Model 3\",\n",
    "        \"efficiency_wh_per_km\": 150,\n",
    "        \"battery_capacity_wh\": 75000,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Load this JSON dataset and display the locations, traffic, and elevation data in a readable format.\",\n",
    "    \"Create a map visualizing the routes between the locations A, B, and C with distances and traffic levels as labels.\",\n",
    "    \"Calculate the travel time for each route considering traffic and average speeds provided in the dataset. Provide the result in a readable format.\",\n",
    "    \"Write a formula to calculate additional energy usage for elevation gains, assuming that for every 100 meters of elevation gain, the vehicle uses an additional 10 Wh per km.\",\n",
    "    \"Compute the most energy-efficient route from San Francisco (A) to Los Angeles (B) using distance, traffic, speed, and elevation data.\",\n",
    "    \"Provide a table comparing all possible routes between A and B with metrics like travel time, elevation gain, energy consumption (Wh), and remaining battery percentage.\",\n",
    "    \"Simulate how changes in traffic levels or elevation impacts the energy consumption for the A to B route.\",\n",
    "    \"Plot a bar chart showing energy consumption for each route between A and B.\",\n",
    "    \"Create a heatmap showing locations reachable from A with 80% battery given the vehicle's specs.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: File Existence and Section Check:\n",
    "\n",
    "- **Imports**:\n",
    "  \n",
    "  - **`IPython.display` (Markdown, display):**  \n",
    "\n",
    "    Imports for rendering markdown content in an IPython (Jupyter) notebook, useful for displaying formatted output.\n",
    "\n",
    "  \n",
    "  - **`json`:**  \n",
    "\n",
    "    Imports the JSON module, though it is not used in this code snippet, it could be useful for handling JSON data in other parts of the script.\n",
    "  \n",
    "  - **`traceback`:**  \n",
    "\n",
    "    This module is used for capturing and printing detailed error information, aiding in debugging.\n",
    "\n",
    "- **Output File Path**:\n",
    "  \n",
    "  - **`output_file`:**  \n",
    "\n",
    "    A variable defining the output file path (`test_output/tesla_routing_output.md`). This path can be modified as required for saving output data.\n",
    "\n",
    "- **Function: `is_section_in_file(section_hint: str, file_path: str) -> bool`**:\n",
    "  \n",
    "  - **Purpose:**  \n",
    "\n",
    "    Checks if a specified section (identified by `section_hint`) exists in the file located at `file_path`.\n",
    "  \n",
    "  - **How It Works:**  \n",
    "\n",
    "    - The function attempts to open and read the content of the file (`file_path`).\n",
    "\n",
    "    - If successful, it checks whether the `section_hint` is present in the file content.\n",
    "\n",
    "    - If an error occurs (e.g., file not found or permission issues), it prints the error message and traceback details for debugging.\n",
    "\n",
    "    - Returns `True` if the section is found, otherwise `False`.\n",
    "\n",
    "- **Function: `create_file_if_not_exists_recursive(file_path: str)`**:\n",
    "  \n",
    "  - **Purpose:**  \n",
    "\n",
    "    Creates the file at `file_path` if it doesn't already exist. This also ensures that the necessary parent directories are created if missing.\n",
    "  \n",
    "  - **How It Works:**\n",
    "\n",
    "    - The function checks if the file at `file_path` exists.\n",
    "\n",
    "    - If it doesn't exist, the function creates any necessary parent directories using `os.makedirs()` and then creates an empty file at `file_path`.\n",
    "\n",
    "    - If an error occurs during this process (e.g., permission issues), the function captures and prints the error details, including the traceback.\n",
    "\n",
    "- **Error Handling:**\n",
    "  \n",
    "  - Both functions use a `try-except` block to handle potential exceptions, ensuring that the script doesn't crash and provides useful debugging information in case of failure.\n",
    "\n",
    "  - The `traceback.print_exc()` method prints the stack trace of the exception, providing detailed context about where and why an error occurred.\n",
    "\n",
    "- **Summary:**\n",
    "  \n",
    "  - The code defines two utility functions:\n",
    "\n",
    "    - `is_section_in_file`: Checks if a specific section exists in a file.\n",
    "\n",
    "    - `create_file_if_not_exists_recursive`: Creates the file and necessary directories if they don't exist.\n",
    "    \n",
    "  - Both functions include error handling to ensure smooth execution and detailed debugging in case of failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "output_file = \"test_output/tesla_routing_output.md\"  # change as required\n",
    "\n",
    "\n",
    "def is_section_in_file(section_hint: str, file_path: str) -> bool:\n",
    "    \"\"\"Check if a section hint is present in a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        return section_hint in content\n",
    "    except Exception as e:\n",
    "        print(\"Error reading file:\", str(e))\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_file_if_not_exists_recursive(file_path: str):\n",
    "    \"\"\"Create a file if it does not exist, creating parent directories if necessary.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(\"\")\n",
    "    except Exception as e:\n",
    "        print(\"Error creating file:\", str(e))\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prompt Handling and Response Logging:\n",
    "\n",
    "- ### create_file_if_not_exists_recursive(output_file):\n",
    "\n",
    "    - This line ensures that the file (output_file), specified earlier as \"test_output/tesla_routing_output.md\", exists.  \n",
    "\n",
    "    - If the file does not exist, it will be created.  \n",
    "\n",
    "    - If necessary, parent directories will also be created.  \n",
    "\n",
    "- ### for prompt in prompts::\n",
    "\n",
    "    - This begins a loop that iterates over each item (prompt) in the prompts list.  \n",
    "\n",
    "    - Each prompt in the list is a string that will be used to query the LLM (Language Learning Model).  \n",
    "\n",
    "- ### if is_section_in_file(prompt, output_file)::\n",
    "\n",
    "    - Checks if the section (identified by the current prompt) already exists in the output file (output_file).  \n",
    "\n",
    "    - The function is_section_in_file will return True if the section is already present in the file and False otherwise.  \n",
    "\n",
    "- ### print(f\"Skipping prompt: {prompt}\"):\n",
    "\n",
    "    - If the section already exists in the output file, it prints a message saying that the prompt is being skipped.  \n",
    "\n",
    "- ### continue:\n",
    "\n",
    "    - This skips the rest of the code inside the loop for the current prompt and moves to the next prompt in the prompts list.  \n",
    "\n",
    "- ### data_json = json.dumps(ROUTING_DATA):\n",
    "\n",
    "    - Converts the ROUTING_DATA dictionary into a JSON-formatted string using the json.dumps() method.  \n",
    "\n",
    "    - This string will be included in the prompt sent to the LLM.  \n",
    "\n",
    "- ### prompt = f\"{prompt}. Provide the response in a readable format.\\n\\n```json\\n{data_json}\\n```\":\n",
    "\n",
    "    - Modifies the current prompt by appending additional instructions and the JSON-formatted ROUTING_DATA string.  \n",
    "\n",
    "    - The LLM is then asked to provide the response in a readable format based on the provided data.  \n",
    "\n",
    "- ### print(f\"Prompt: {prompt}\"):\n",
    "\n",
    "    - Prints the modified prompt (with added instructions and data) to the console.  \n",
    "    - This allows you to see what the LLM is being asked to process.  \n",
    "\n",
    "- ### response = llm(prompt=prompt):\n",
    "\n",
    "    - Calls the llm() function (which interfaces with the OpenAI API and Toolhouse tools) to generate a response based on the modified prompt.  \n",
    "\n",
    "    - The response from the LLM is stored in the response variable.  \n",
    "\n",
    "- ### print(\"Response: \", response):\n",
    "\n",
    "    - Prints the response generated by the LLM to the console, allowing you to see what the model output is.  \n",
    "\n",
    "- ### if not is_section_in_file(prompt, output_file)::\n",
    "\n",
    "\n",
    "    - Checks again if the section (based on the current prompt) already exists in the output file.  \n",
    "\n",
    "    - If the prompt's section is not yet in the file, it proceeds to write the new response.  \n",
    "\n",
    "- ### with open(output_file, \"a\") as f::\n",
    "\n",
    "    - Opens the output file (output_file) in append mode (\"a\").  \n",
    "\n",
    "    - This allows new content to be added to the end of the file without overwriting existing content.  \n",
    "\n",
    "- ### f.write(f\"## {prompt}\\n\" + response + \"\\n\\n\"):\n",
    "\n",
    "    - Writes the prompt and its corresponding response to the output file.  \n",
    "\n",
    "    - It prepends the prompt with ## to format it as a section header in Markdown.  \n",
    "\n",
    "    - The response is written on the next line, followed by two newline characters for proper spacing.  \n",
    "\n",
    "- ### print(\"\\n\\n\"):\n",
    "\n",
    "    - Prints two newline characters to create some space in the console output for clarity.  \n",
    "\n",
    "- ### print(f\"🚀 All prompts completed. Check the output file ('{output_file}') for the responses.\"):\n",
    "\n",
    "    - After processing all the prompts, this message is printed to inform the user that all prompts have been processed,  \n",
    "    and they can now check the output file for the detailed responses.  \n",
    "\n",
    "### **Summary of Code Workflow**:\n",
    "\n",
    "The code first ensures that the output file exists.  \n",
    "Then, for each prompt in the list prompts, it checks if the section has already been processed (and saved in the output file).  \n",
    "If the section exists, it skips the prompt; otherwise, it prepares the prompt and sends it to the LLM for processing.  \n",
    "The generated response is then written to the output file, formatted in Markdown.  \n",
    "Once all prompts are processed, the code informs the user that the task is complete and to check the output file for the results.  \n",
    "This approach prevents the code from duplicating content in the output file and ensures that only new prompts are processed and saved.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_if_not_exists_recursive(output_file)\n",
    "\n",
    "for prompt in prompts:\n",
    "    if is_section_in_file(prompt, output_file):\n",
    "        print(f\"Skipping prompt: {prompt}\")\n",
    "        continue\n",
    "    data_json = json.dumps(ROUTING_DATA)\n",
    "    prompt = f\"{prompt}. Provide the response in a readable format.\\n\\n```json\\n{data_json}\\n```\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    response = llm(prompt=prompt)\n",
    "    print(\"Response: \", response)\n",
    "    if not is_section_in_file(prompt, output_file):\n",
    "        with open(output_file, \"a\") as f:\n",
    "            f.write(f\"## {prompt}\\n\" + response + \"\\n\\n\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    f\"🚀 All prompts completed. Check the output file ('{output_file}') for the responses.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The Tesla Smart Routing Reporter is a powerful tool for optimizing Tesla Model 3 routes by analyzing traffic, elevation, and energy consumption data. It efficiently processes JSON datasets, calculates travel times, energy efficiency, and provides actionable insights. By leveraging OpenAI's GPT models and Toolhouse, the app automates the generation of detailed reports, visualizations, and comparisons, ultimately helping drivers make smarter decisions for energy-efficient routes and better vehicle performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! 🌐\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
