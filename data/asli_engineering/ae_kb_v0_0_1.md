# https://arpitbhayani.me/

Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

# Hey, I am Arpit

### curious, tinkerer, and explorer

I am a software engineer and an engineering leader passionate about
System Architecture, Databases Internals, Language Internals, and
Advanced Algorithms. I am the creator and maintainer of [DiceDB](https://github.com/dicedb/dice)

- an open source, reactive, multi-tenant, cache optimized for modern
  hardware.

In 2024, I took the leap of faith and co-founded [Profile.fyi](https://profile.fyi), where I took care of all things tech and product. Before this, I
was a Staff Engineer at
[Google](https://cloud.google.com/) leading
the
[Dataproc](https://cloud.google.com/dataproc) India team in providing a managed big data ecosystem to GCP customers.
I was also part of [Amazon's](https://www.amazon.com/)
Fast Data Team and took care of cold tiering of hot data and providing
a seamless query interface across all tiers.

I held engineering leadership positions (both IC and management) at
[Unacademy](https://unacademy.com/),
where I built, grew, and led Search, Site Reliability Engineering
(SRE) teams, and Data Engineering teams. I hold a total of 10+ years
of experience in scaling backend services, taking products and teams
from 0 to 1, and beyond.

I keep diving deep into engineering details and share my learnings
by across my
[socials](https://twitter.com/arpit_bhayani) and videos on
[YouTube](https://youtube.com/c/ArpitBhayani).

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Recent blog posts
•
[Full archive ➔](/blogs)

---

Things I have written recently.

- Oct 19, 2024
  :

  [3P framework to know when it is the right time to make a switch](/blogs/3p)

- Oct 02, 2024
  :

  [Leverage the equilibrium](/blogs/leverage-the-equilibrium)

- Sep 10, 2024
  :

  [Not everything needs to be dumbed down](/blogs/not-everything-needs-to-be-dumbed-down)

- Aug 26, 2024
  :

  [The best resource is mythical](/blogs/best-resource-is-mythical)

- Aug 19, 2024
  :

  [It's not about what you know, but about how you think](/blogs/retention-vs-understanding)

- Aug 13, 2024
  :

  [Know a lot, a lot](/blogs/know-a-lot)

- Aug 12, 2024
  :

  [Going out of syllabus is okay](/blogs/out-of-syllabus)

- Aug 06, 2024
  :

  [Always negotiate the offer](/blogs/negotiate-the-offer)

- Jul 31, 2024
  :

  [Never bad-mouth your ex-employer](/blogs/never-bad-mouth-your-ex-exployer)

- Jul 29, 2024
  :

  [Prove you are a culture-fit](/blogs/culture-fit)

Recent papers
•
[Papershelf ➔](/papershelf)

---

Papers I have read recently.

- [The Chubby lock service for loosely-coupled distributed systems](https://drive.google.com/file/d/1o5Rto3ex5iDUONNPJf8Wvp43E7wWpdsR/view?usp=sharing)
- [Bigtable: A Distributed Storage System for Structured Data](https://drive.google.com/file/d/1o7HrswgtMRYTuDOFsEiAnhpRaYhWxpRt/view?usp=drive_link)
- [On-the-fly Sharing for Streamed Aggregation](https://drive.google.com/file/d/156BTit4ZbFdq526_2sdvLBy6WIHV5tmx/view?usp=sharing)
- [MapReduce: Simplified Data Processing on Large Clusters](https://drive.google.com/file/d/1fcJP_WE8j0L-QdxQhBSEzkyel8szFzbH/view?usp=sharing)
- [How to break software](https://drive.google.com/file/d/1zr4qYypLoaW521NX0WIlV_TXKpa8ZuDG/view?usp=sharing)
- [Web Search for a Planet: The Google Cluster Architecture](https://drive.google.com/file/d/1Cs1-ENNZFDcxLh9MB_p7j-IUCboIuwMQ/view?usp=sharing)
- [Amazon DynamoDB: A scalable, predictably performant, and fully managed NoSQL database service](https://drive.google.com/file/d/1ztxrZTh3Gn9WWoqprDCU80MjOoXnL6Es/view?usp=sharing)

From the First Principles
•
[All topics ➔](/first)

---

In-depth and structure explainers for various topics and concepts.

- [Designing Microservices](/microservices)
- [todo] BitTorrent Internals
- [todo] HashTable Internals
- [todo] Real-world System Design
- ...

SWE Math Weekly
•
[All problems ➔](/math)

---

Weekly questions that sits at the intersection of SWE, ML, and Math

- Jan 17, 2025
  :

  [Quest of Video Compression](/math/3)

- Jan 10, 2025
  :

  [Evolution of Tech Content Creators](/math/2)

- Jan 03, 2025
  :

  [The Ancient Library's Secret Code](/math/1)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs

Blogs | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

## Blogs

Every week, I documente and articulate my acquired knowledge and
personal perspectives on topics that captivate my interest. Here are
all blogs I wrote to date.

- Oct 19, 2024
  :

  [3P framework to know when it is the right time to make a switch](/blogs/3p)

- Oct 02, 2024
  :

  [Leverage the equilibrium](/blogs/leverage-the-equilibrium)

- Sep 10, 2024
  :

  [Not everything needs to be dumbed down](/blogs/not-everything-needs-to-be-dumbed-down)

- Aug 26, 2024
  :

  [The best resource is mythical](/blogs/best-resource-is-mythical)

- Aug 19, 2024
  :

  [It's not about what you know, but about how you think](/blogs/retention-vs-understanding)

- Aug 13, 2024
  :

  [Know a lot, a lot](/blogs/know-a-lot)

- Aug 12, 2024
  :

  [Going out of syllabus is okay](/blogs/out-of-syllabus)

- Aug 06, 2024
  :

  [Always negotiate the offer](/blogs/negotiate-the-offer)

- Jul 31, 2024
  :

  [Never bad-mouth your ex-employer](/blogs/never-bad-mouth-your-ex-exployer)

- Jul 29, 2024
  :

  [Prove you are a culture-fit](/blogs/culture-fit)

- Jul 23, 2024
  :

  [Quantify your resume, through and through](/blogs/quantification-in-resume)

- Jul 15, 2024
  :

  [Hiring is subjective and unfair](/blogs/hiring-is-unfair)

- Jul 08, 2024
  :

  [Questions you should ask to your interviewer](/blogs/questions-for-interviewers)

- Jul 03, 2024
  :

  [Be curious, not judgemental](/blogs/be-curious-not-judgemental)

- Jul 01, 2024
  :

  [Collaborate and communicate better](/blogs/collaboration-communication)

- Jun 20, 2024
  :

  [Once you are out of vicious interview cycle](/blogs/out-of-vicious-interview-cycle)

- Jun 17, 2024
  :

  [Pitch projects, not ideas](/blogs/pitch-projects-not-ideas)

- Jun 10, 2024
  :

  [Read design docs, even if they seem unrelated](/blogs/read-design-docs)

- May 29, 2024
  :

  [Be excited when production goes down](/blogs/read-rca-docs)

- May 27, 2024
  :

  [Being a generalist does not mean being mediocre](/blogs/generalist-not-mediocre)

- May 21, 2024
  :

  [Summaries create an illusion of mastery](/blogs/do-not-rely-on-summaries)

- May 13, 2024
  :

  [You are not hired to just ship features](/blogs/do-not-just-ship-features)

- May 06, 2024
  :

  [Structure your System Design interviews](/blogs/structure-your-design-interviews)

- Mar 27, 2024
  :

  [Never let your company inflate your title to retain you](/blogs/title-inflation)

- Mar 11, 2024
  :

  [Find your own project](/blogs/find-your-own-project)

- Jan 15, 2024
  :

  [Six pointers to crack coding and design interviews](/blogs/six-pointers-to-crack-coding-and-design-interviews)

- Nov 16, 2023
  :

  [Keep yourself unblocked](/blogs/keep-yourself-unblocked)

- Mar 07, 2022
  :

  [Genetic algorithm to solve the Knapsack Problem](/blogs/genetic-knapsack)

- Feb 21, 2022
  :

  [Pseudorandom number generator uusing LFSR](/blogs/pseudorandom-number-generation-lfsr)

- Feb 07, 2022
  :

  [How indexes work on partitioned and sharded data?](/blogs/how-indexes-work-on-partitioned-and-sharded-data)

- Jan 31, 2022
  :

  [Data partitioning strategies for distributed databases](/blogs/some-data-partitioning-strategies-for-distributed-data-stores)

- Jan 24, 2022
  :

  [What is Data Partitioning and why it matters at scale](/blogs/data-partitioning)

- Jan 16, 2022
  :

  [What is leaderless replication and how it works?](/blogs/leaderless-replication)

- Jan 03, 2022
  :

  [How to resolve conflicts in multi-master setup](/blogs/conflict-resolution)

- Nov 28, 2021
  :

  [How to detect conflicts in a multi-master setup](/blogs/conflict-detection)

- Nov 03, 2021
  :

  [What is multi-master replication and why do we need it?](/blogs/multi-master-replication)

- Oct 03, 2021
  :

  [What are monotonic reads, and why do we need them?](/blogs/monotonic-reads)

- Sep 22, 2021
  :

  [Read-your-write consistency](/blogs/read-your-write-consistency)

- Sep 07, 2021
  :

  [How to handle database outages?](/blogs/handling-outages-master-replica)

- Aug 23, 2021
  :

  [What happens when we add a new replica?](/blogs/new-replica)

- Aug 15, 2021
  :

  [Understanding replication formats in a master-replica setup](/blogs/replication-formats)

- Aug 10, 2021
  :

  [Two ways to replicate data across database cluster](/blogs/replication-strategies)

- Aug 07, 2021
  :

  [What is a master-replica setup and why it matters?](/blogs/master-replica-replication)

- Jul 19, 2021
  :

  [Understanding Durability in ACID](/blogs/durability)

- Jul 07, 2021
  :

  [Understanding Isolation in ACID](/blogs/isolation)

- Jul 02, 2021
  :

  [Understanding Atomicity in ACID](/blogs/atomicity)

- Jul 02, 2021
  :

  [Understanding Consistency in ACID](/blogs/consistency)

- Jun 22, 2021
  :

  [Common architectural patterns in distributed systems](/blogs/architectures-in-distributed-systems)

- Jun 17, 2021
  :

  [Assumptions people make while designing distributed systems](/blogs/mistaken-beliefs-of-distributed-systems)

- Jun 09, 2021
  :

  [Fork Bomb](/blogs/fork-bomb)

- Apr 28, 2021
  :

  [How Python evaluates chained comparison operators](/blogs/chained-operators-python)

- Apr 19, 2021
  :

  [How to design a taxonomy on a Relational DB](/blogs/taxonomy-on-sql)

- Apr 01, 2021
  :

  [Changing the Python's walrus operator - a deep dive into CPython](/blogs/the-weird-walrus)

- Feb 15, 2021
  :

  [Fully Persistent Arrays - a datastructure that let's us time travel](/blogs/fully-persistent-arrays)

- Feb 07, 2021
  :

  [Introduction to Persistent Data Structures](/blogs/persistent-data-structures-introduction)

- Jan 10, 2021
  :

  [How python optimises the runtime using constant folding](/blogs/constant-folding-python)

- Dec 20, 2020
  :

  [String interning in Python - a deep dive into CPython](/blogs/string-interning-python)

- Dec 13, 2020
  :

  [Building a simple recursion tree visualizer for Python](/blogs/recursion-visualizer-python)

- Dec 06, 2020
  :

  [Flajolet Martin algorithm for approximate counting](/blogs/flajolet-martin)

- Nov 29, 2020
  :

  [2Q-cache algorithm for disk-backed databases](/blogs/2q-cache)

- Nov 22, 2020
  :

  [Israeli Queues](/blogs/israeli-queues)

- Nov 16, 2020
  :

  [How video games programatically generate terrains?](/blogs/1d-terrain)

- Nov 08, 2020
  :

  [Quantifying set similarity using Jaccard similarity coefficient and MinHash](/blogs/jaccard-minhash)

- Nov 01, 2020
  :

  [Smooth out time-series data using Kurtosis](/blogs/ts-smoothing)

- Aug 23, 2020
  :

  [Constant time implementation of the LFU cache eviction algorithm](/blogs/lfu)

- Aug 02, 2020
  :

  [Morris's algorithm for approximate counting](/blogs/morris-counter)

- Jul 26, 2020
  :

  [Slowsort - the slowest sorting algorithm, ever](/blogs/slowsort)

- Jul 19, 2020
  :

  [Bitcask - A Log-Structured fast KV store](/blogs/bitcask)

- Jul 12, 2020
  :

  [Phi φ Accrual - a realistic failure detection algorithm](/blogs/phi-accrual)

- Jul 05, 2020
  :

  [Traits of a 10x engineer](/blogs/10x-engineer)

- Jul 05, 2020
  :

  [Deciphering Repeated-key XOR Ciphertext](/blogs/decipher-repeated-key-xor)

- Jun 21, 2020
  :

  [Deciphering Single-byte XOR Ciphertext](/blogs/decipher-single-xor)

- Jun 14, 2020
  :

  [Making Python integers iterable - a deep dive into CPython](/blogs/python-iterable-integers)

- Jun 07, 2020
  :

  [Stucture composition in C - implementing inheritence in C](/blogs/inheritance-c)

- May 31, 2020
  :

  [The RUM Conjecture](/blogs/rum)

- May 24, 2020
  :

  [Consistent Hashing - explanation and implementation](/blogs/consistent-hashing)

- May 17, 2020
  :

  [Python caches smaller integers - a deep dive into CPython](/blogs/python-caches-integers)

- May 10, 2020
  :

  [Fractional Cascading - a way to speed up binary searches](/blogs/fractional-cascading)

- May 03, 2020
  :

  [What is copy-on-write and why it matters?](/blogs/copy-on-write)

- Apr 26, 2020
  :

  [A midpoint insertion caching strategy to avoid cache bust](/blogs/midpoint-insertion-caching-strategy)

- Apr 19, 2020
  :

  [Building Finite State Machines with Python Coroutines](/blogs/fsm-python)

- Apr 12, 2020
  :

  [Bayesian average to compute average rating, properly](/blogs/bayesian-average)

- Apr 05, 2020
  :

  [Designing and implementing a Sliding Window based Rate Limiter](/blogs/sliding-window-ratelimiter)

- Mar 10, 2020
  :

  [The math behind Inverse Document Frequency - the IDF of TFIDF](/blogs/idf)

- Feb 28, 2020
  :

  [Eight Rituals to be a Better Programmer](/blogs/better-programmer)

- Feb 21, 2020
  :

  [Personalize your Python Prompt](/blogs/python-prompts)

- Feb 14, 2020
  :

  [Pseudorandom Number Generation using Cellular Automata - Rule 30](/blogs/rule-30-cellular-automata)

- Feb 07, 2020
  :

  [Implementing functional overloading in Python](/blogs/function-overloading)

- Jan 31, 2020
  :

  [Isolation Forest Algorithm for Anomaly Detection](/blogs/isolation-forest)

- Jan 17, 2020
  :

  [Image Steganography - the art of hiding data in images](/blogs/image-steganography)

- Jan 10, 2020
  :

  [How Python supports integers of infinite length - a deep dive into CPython](/blogs/long-integers-python)

- Jan 03, 2020
  :

  [I changed my python for fun!](/blogs/i-changed-my-python)

- Mar 24, 2017
  :

  [I benchmarked different pagination strategies in MongoDB](/blogs/benchmark-and-compare-pagination-approach-in-mongodb)

- Mar 23, 2017
  :

  [Why MongoDB's cursor.skip() is slow?](/blogs/mongodb-cursor-skip-is-slow)

- Mar 22, 2017
  :

  [How to paginate faster with consistent performance?](/blogs/fast-and-efficient-pagination-in-mongodb)

- Apr 12, 2016
  :

  [Understanding HTTP protocol using netcat](/blogs/making-http-requests-using-netcat)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/first

From the First Principles | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

## From the First Principles

"From the First Principles" is where I unpack topics ground up,
dissecting each concept with a clear and practical approach. This
helps in building the right intuition to understand similar
concepts, topics, and systems by drawing parallels. Everything is
broken down and covered using the [First Principles](https://en.wikipedia.org/wiki/First_principle).

- [Designing Microservices](/microservices)
- [todo] BitTorrent Internals
- [todo] HashTable Internals
- [todo] Real-world System Design
- [todo] Database Fundamentals
- [todo] JunoDB Internals
- [todo] DragonflyDB Internals
- ...

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/papershelf

Papershelf | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

## Papershelf

I read a few papers every week around various topics that interests
me and here are some of them that I found amusing categorized by
topics. Here's the process I follow to [read, understand, and remember a paper](https://youtu.be/V5-KxHdmbDo).

- Sep 15, 2024
  :

  [On-demand Container Loading in AWS Lambda](/blogs/on-demand-container-loading-in-aws-lambda)

- Sep 03, 2024
  :

  [SQL Has Problems. We Can Fix Them: Pipe Syntax In SQL](/blogs/sql-has-problems-we-can-fix-them-pipe-syntax-in-sql)

- Aug 28, 2024
  :

  [NanoLog: A Nanosecond Scale Logging System](/blogs/nanolog-a-nanosecond-scale-logging-system)

- Aug 21, 2024
  :

  [WTF: The Who to Follow Service at Twitter](/blogs/wtf-the-who-to-follow-service-at-twitter)

I have been reading papers for a very long time and the notes I took
were not public. I am in the process of making the notes public and this
might take some time. So, the papers you see below this notification are
links and the ones you see above are my notes and explanations for each
one of them. Slowly, I will be pushing notes for all the papers I've
read.

- [The Chubby lock service for loosely-coupled distributed systems](https://drive.google.com/file/d/1o5Rto3ex5iDUONNPJf8Wvp43E7wWpdsR/view?usp=sharing)
- [Bigtable: A Distributed Storage System for Structured Data](https://drive.google.com/file/d/1o7HrswgtMRYTuDOFsEiAnhpRaYhWxpRt/view?usp=drive_link)
- [On-the-fly Sharing for Streamed Aggregation](https://drive.google.com/file/d/156BTit4ZbFdq526_2sdvLBy6WIHV5tmx/view?usp=sharing)
- [MapReduce: Simplified Data Processing on Large Clusters](https://drive.google.com/file/d/1fcJP_WE8j0L-QdxQhBSEzkyel8szFzbH/view?usp=sharing)
- [How to break software](https://drive.google.com/file/d/1zr4qYypLoaW521NX0WIlV_TXKpa8ZuDG/view?usp=sharing)
- [Web Search for a Planet: The Google Cluster Architecture](https://drive.google.com/file/d/1Cs1-ENNZFDcxLh9MB_p7j-IUCboIuwMQ/view?usp=sharing)
- [Amazon DynamoDB: A scalable, predictably performant, and fully managed NoSQL database service](https://drive.google.com/file/d/1ztxrZTh3Gn9WWoqprDCU80MjOoXnL6Es/view?usp=sharing)
- [Amazon Redshift re-invented](https://drive.google.com/file/d/16Pb3BSWkmNJx0Dato3NVt36nDDKbkkRy/view?usp=sharing)
- [Scalable blocking for very large databases](https://drive.google.com/file/d/1RMELaWQ5sPbomHeq5bwwYH79xI6PBiTY/view?usp=sharing)
- [Firecracker: Lightweight virtualization for serverless applications](https://drive.google.com/file/d/1u-eQlDjrVn7lzOmX-Cv2si_Wj16H0Z-h/view?usp=drive_link)
- [Millions of tiny databases](https://drive.google.com/file/d/16K17SnbgAcFM7j7_qsJJwMX8dYBv9t-n/view?usp=drive_link)
- [Amazon Redshift and the Case for Simpler Data Warehouses](https://drive.google.com/file/d/1LwAAnoE2B17AkZ0hU5EBrjwUTcce7qQG/view?usp=drive_link)
- [Amazon Aurora: Design considerations for high throughput cloud-native relational databases](https://drive.google.com/file/d/1Aqp80fFRz6A2KiIxoVgB0gYZecoxgZUW/view?usp=drive_link)
- [Near-duplicate Question Detection](https://drive.google.com/file/d/1MuqS6WO9wFjOFwtQnp590Tz28o_y6Aco/view?usp=drive_link)
- [Striking the right chord: A comprehensive approach to Amazon Music search spell correction](https://drive.google.com/file/d/1SeBP_vLMohz7qlQsOX1pS9hB-8CAGstm/view?usp=drive_link)
- [Stage: Query Execution Time Prediction in Amazon Redshift](https://drive.google.com/file/d/1ZkernfuCbAMQOfbfOvK8Gjb9-JJnQOVg/view?usp=sharing)
- [A flexible large-scale similar product identification system in e-commerce](https://drive.google.com/file/d/16fqZFuri2WWKM6bfsy-HpWe0hWkdWZdV/view?usp=drive_link)
- [Intelligent Scaling in Amazon Redshift](https://drive.google.com/file/d/1E7cb5Ttj21JvI3svJC0QnkhncycX2PS-/view?usp=drive_link)
- [Serverless Runtime / Database Co-Design With Asynchronous I/O](https://drive.google.com/file/d/1nuURga5TdctAorXCRPOraIgiLwT0n6S-/view?usp=sharing)
- [Predicate Caching: Query-Driven Secondary Indexing for Cloud DataWarehouses](https://drive.google.com/file/d/1K-tWD8-SnQbenEajEHKfPOOL7va-3viA/view?usp=drive_link)
- [Query Attribute Recommendation at Amazon Search](https://drive.google.com/file/d/1ItSnpBjjhIyamZFt-82zriHt4Qvo_DBy/view?usp=sharing)
- [ROSE: Robust caches for Amazon product search](https://drive.google.com/file/d/13YwkeCc1XwnRVDVkcuE7MKt2r7aqld24/view?usp=drive_link)
- [The story of AWS Glue](https://drive.google.com/file/d/1CxK5bTV8ZQgNFe3TI582W9N8PyLa5nK3/view?usp=drive_link)
- [DecLog: Decentralized Logging in Non-Volatile Memory for Time Series Database Systems](https://drive.google.com/file/d/1CJwsj-dRcoc_wtkterUQ9l4xtYPOr3On/view?usp=drive_link)
- [SILK: Preventing Latency Spikes in LSM Key-Value Stores](https://drive.google.com/file/d/1RCBW70TNXqGowl4I7cPjRJjzpfZqI0rs/view?usp=drive_link)
- [Umbra: A Disk-Based System with In-Memory Performance](https://drive.google.com/file/d/1V4sahz-i4Z9wFBsZcj3mHxNAoSXaX53x/view?usp=drive_link)
- [Amazon MemoryDB: A fast and durable memory-first cloud database](https://drive.google.com/file/d/1W1nQ8wiT2upuOHXD9d1PZwALJSZx0MFB/view?usp=drive_link)
- [Take Out the TraChe: Maximizing (Tra)nsactional Ca(che) Hit Rate](https://drive.google.com/file/d/11BQhr3FtKBHrKFvmtNcmOsOnn9NGQ0li/view?usp=drive_link)
- [Distributed Transactions at Scale in Amazon DynamoDB](https://drive.google.com/file/d/1Yg2R-wN7KKugx-R4yc8c080XsXtBB0JT/view?usp=drive_link)
- [TiDB: A Raft-based HTAP Database](https://drive.google.com/file/d/114Xn8vqP3jrQKNS-9-XdPeLaHMKBO4UV/view?usp=drive_link)
- [Kora: A Cloud-Native Event Streaming Platform For Kafka](https://drive.google.com/file/d/1DwH_8_oqgkv8jNkoITqIYPx2-LZnspBx/view?usp=drive_link)
- [PolarDB-SCC: A Cloud-Native Database](https://drive.google.com/file/d/1HHEsBWa2MZnJ290ZCAEQK6lNc_X4Vp-2/view?usp=drive_link)
- [Epoxy: ACID Transactions Across Diverse Data Stores](https://drive.google.com/file/d/1-fp25FGGIBRqR1MTwG9hUbTm-CMIy9ad/view?usp=drive_link)
- [Scalable OLTP in the Cloud: What’s the BIG DEAL?](https://drive.google.com/file/d/1WOXxSBhoDyWe2ZpmOVBD4tcJiFuJTpHq/view?usp=drive_link)
- [BonsaiKV: Key-Value Store with Tiered and Heterogeneous Memory System](https://drive.google.com/file/d/15ow-jHUbzi9Hvue8VwYebpcO4wrhbZRW/view?usp=drive_link)
- [Automated Unit Test Improvement using Large Language Models at Meta](https://drive.google.com/file/d/1ckfwOOPfg3xTsw2CVoSC8nmlq6LfZoEm/view?usp=drive_link)
- [Designing Access Methods: The RUM Conjecture](https://drive.google.com/file/d/1EeZ4vCy8m0Mb06Z-uc-zbtZfQ30ERF6z/view?usp=drive_link)
- [Probabilistic Counting Algorithms for Database Applications - Flajolet-Martin](https://drive.google.com/file/d/147IAuDTmHuSS74xd27_HD3NKm14euOPT/view?usp=drive_link)
- [Cache-Efficient Top-k Aggregation over High Cardinality Large Datasets](https://drive.google.com/file/d/1fIasc3HCalvsu-CTJp0fyGihiz5xD_UV/view?usp=drive_link)
- [SIEVE - an Efficient Turn-Key Eviction Algorithm for Web Caches](https://drive.google.com/file/d/1CMJMCSXW9MrEe5s9i6EQYoTlmzh-fgzk/view?usp=drive_link)
- [Vector Database: Storage and Retrieval Technique, Challenge](https://drive.google.com/file/d/1HamJjegLEvLSebnEN-uKLWhQTiZ-UJdd/view?usp=drive_link)
- [Panda: Performance Debugging for Databases using LLM Agents](https://drive.google.com/file/d/16zfCBxo-xqhMhrq48dnQmCov-FAn3Lkk/view?usp=drive_link)
- [Magnet: A scalable and performant shuffle architecture for Apache Spark](https://drive.google.com/file/d/1xjKIl7SC8tqJeEdN7wGxg956QHRogo2W/view?usp=sharing)
- [ZIP: Lazy Imputation during Query Processing](https://drive.google.com/file/d/1yuMg3x4kgZ6eHVoWMgBBttH2C6wsJEa-/view?usp=sharing)
- [Anycast as a Load Balancing feature](https://drive.google.com/file/d/1209iTFzMJQDqkPNCneLSp56RyXZqfxoa/view?usp=share_link)
- [The Impact of Thread-Per-Core Architecture on Application Tail Latency](https://drive.google.com/file/d/1EJHkuxRJMxK_yFQpUftKW8LaFr2SQDSC/view?usp=sharing)
- [TreeLine - An Update-In-Place Key-Value Store for Modern Storage](https://drive.google.com/file/d/1MLkQIO9xqSMc6jv9lbqz32lWBRoTLjT_/view?usp=drive_link)
- [Manu: A Cloud Native Vector Database Management System](https://drive.google.com/file/d/1gLl_gSzt6cjnvdPpX40kdaOCiTyMNt4z/view?usp=drive_link)
- [Parallelism-Optimizing Data Placement for Faster Data-Parallel Computations](https://drive.google.com/file/d/1rO1FoyU2F0JrNmf5JOZfuPiYmpanwy6b/view?usp=drive_link)
- [Faster sorting algorithms discovered using deep reinforcement learning](https://drive.google.com/file/d/16n47YlDxbCXG5qZiu258eK-3y8_Aa9Ru/view?usp=sharing)
- [A Relational Model of Data for Large Shared Data Banks](https://drive.google.com/file/d/1_27sKT2kzGCuAL9hEldGO26p_qOhvhS6/view?usp=share_link)
- [Improving Language Understanding by Generative Pre-Training](https://drive.google.com/file/d/1yDyKWk4RhC40jbY2evevDnuuxKpIkAxi/view?usp=share_link)
- [Language Models are Few-Shot Learners](https://drive.google.com/file/d/1ICUPRGbARL1L_JgMrKfrYQ6xKzh46pjT/view?usp=share_link)
- [Attention Is All You Need](https://drive.google.com/file/d/1NI4fHNYauNvH3ynRuhi11Ey5s1-BOpmE/view?usp=share_link)
- [Amazon DynamoDB A Scalable, Predictably Performant, and Fully Managed NoSQL Database Service](https://drive.google.com/file/d/1nA7iL9b_WLlQKhuzAV9RlgsrSrWKDsG4/view?usp=share_link)
- [Dynamo Amazon’s Highly Available Key-value Store](https://drive.google.com/file/d/1dIX26Vyiva_qxO_5syfMa85ynFpVPIDO/view?usp=share_link)
- [The Google File System](https://drive.google.com/file/d/1S_hYRcjdo7aR0ShXuIuK5ePQm2U0FEs2/view?usp=share_link)
- [Neural Machine Translation of Rare Words with Subword Units](https://drive.google.com/file/d/1XTvz9HwZ1mlh7D7tfxlILcMgv7IkWkKp/view?usp=share_link)
- [The Bloom Paradox: When not to Use a Bloom Filter](https://drive.google.com/file/d/1luxcdZBCxo-ty9sgmEQ1rDs5AF4334-3/view)
- [The Deletable Bloom Filter](https://drive.google.com/file/d/1f-LFOroH5WihpfSENXCQJGyM4EWOAgc8/view?usp=share_link)
- [Zanzibar - Google's Consistent, Global Authorization System](https://drive.google.com/file/d/1Z3Uzhm-9dhG1DMhyAwDxMhy989N1BIXG/view?usp=share_link)
- [Space-Time Trade-offs in Hash Coding with Allowable Errors](https://drive.google.com/file/d/1tWyRo5ofJgMmZpXrSMO6FjfGI0_NFS54/view?usp=share_link)
- [Gorilla - A Fast, Scalable, In Memory Time Series Database](https://drive.google.com/file/d/13jFQkD2OmydymjPFLnsvsQUwMwhmynIh/view?usp=sharing)
- [Understanding BitTorrent - An Experimental Perspective](https://drive.google.com/file/d/17lvNCfgI2xwMA65VDBcKSPhq0iJqHXVU/view?usp=sharing)
- [Exploiting BitTorrent For Fun (But Not Profit)](https://drive.google.com/file/d/13qLIQFaytcTUD0pQEhP_7eeqN_Ryv3bh/view?usp=sharing)
- [Rarest First and Choke Algorithms Are Enough](https://drive.google.com/file/d/1GqnGoiQbrbxdn1oVPLVd3z8q-XLaoaIh/view?usp=sharing)
- [Implementation of a BitTorrent client - B. Sc. Thesis](https://drive.google.com/file/d/1mCgITghlVle3rFmJzd3Um7B5Wu_k3shb/view?usp=sharing)
- [Kademlia - a Peer-to-peer Information System based on XOR Metric](https://drive.google.com/file/d/1EREYP8U1jkxsbsLJvjKhSsz7Scc4xo2c/view?usp=sharing)
- [Peer-to-peer networking with BitTorrent](https://drive.google.com/file/d/1VS37P6J3v_trRCHzCOWtS-9lGGcaVq4a/view?usp=sharing)
- [Free Riding in BitTorrent is Cheap](https://drive.google.com/file/d/1JEu085WKpy0I-X_enknDs08TH6bnVc0T/view?usp=sharing)
- [Go To Statement Considered Harmful](https://drive.google.com/file/d/1qMOCSfTgyPKF6HFS1QFrek_A2pRMmwtK/view?usp=sharing)
- [Bitcoin - A Peer-to-Peer Electronic Cash System](https://drive.google.com/file/d/1R0B4ZD67-W4fPmd0EphOXxqWvujZAzHt/view?usp=sharing)
- [Understanding Inverse Document Frequency On theoretical arguments for IDF](https://drive.google.com/file/d/11cw9-riCQ5HJ0R2EOHxRijXfV1r76E7Y/view?usp=sharing)
- [MyRocks: LSM-Tree Database Storage Engine Serving Facebook's Social Graph](https://drive.google.com/file/d/1lOJcjJ-r8IaujMNx8btUy-VSKe4R5cnY/view?usp=sharing)
- [Isolation Forest](https://drive.google.com/file/d/1yTEFQaEizA-4oPuC4I1NI7XDbu1WXm1W/view?usp=sharing)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/courses

Courses | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

# Become a better engineer

Super practical courses, with a no-fluff no-nonsense approach, that
are designed to spark engineering curiosity and help you ace your
career.

## [System Design Masterclass](/masterclass)

A no-fluff masterclass that helps SDE-2, SDE-3, and above form the right intuition to design and implement highly scalable, fault-tolerant, extensible, and available systems.

[Details →](/masterclass)

## [System Design for Beginners](/system-design-for-beginners)

An in-depth and self-paced course for absolute beginners to become great at designing and implementing scalable, available, and extensible systems.

[Details →](/system-design-for-beginners)

## [Redis Internals](/redis-internals)

A self-paced and hands-on course covering Redis internals - data structures, algorithms, and some core features by re-implementing them in Go.

[Details →](/redis-internals)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

# Hey, I am Arpit

### curious, tinkerer, and explorer

I am a software engineer and an engineering leader passionate about
System Architecture, Databases Internals, Language Internals, and
Advanced Algorithms. I am the creator and maintainer of [DiceDB](https://github.com/dicedb/dice)

- an open source, reactive, multi-tenant, cache optimized for modern
  hardware.

In 2024, I took the leap of faith and co-founded [Profile.fyi](https://profile.fyi), where I took care of all things tech and product. Before this, I
was a Staff Engineer at
[Google](https://cloud.google.com/) leading
the
[Dataproc](https://cloud.google.com/dataproc) India team in providing a managed big data ecosystem to GCP customers.
I was also part of [Amazon's](https://www.amazon.com/)
Fast Data Team and took care of cold tiering of hot data and providing
a seamless query interface across all tiers.

I held engineering leadership positions (both IC and management) at
[Unacademy](https://unacademy.com/),
where I built, grew, and led Search, Site Reliability Engineering
(SRE) teams, and Data Engineering teams. I hold a total of 10+ years
of experience in scaling backend services, taking products and teams
from 0 to 1, and beyond.

I keep diving deep into engineering details and share my learnings
by across my
[socials](https://twitter.com/arpit_bhayani) and videos on
[YouTube](https://youtube.com/c/ArpitBhayani).

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/talks

Talks and Presentations | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

# Talks and Presentations

I enjoy speaking about engineering & career growth and have spoken at
multiple companies and conferences. If you want to organise a talk,
drop me an email at `speak.with.arpit@gmail.com`.

![Arpit Bhayani speaking at Rocketium](https://edge.arpitbhayani.me/img/talks/rocketium.jpg)

at Rocketium for their Blue Sky hour

## Appearances

These are some of the companies and events I have spoken at.

- When should you pick a serverless database

  •
  [MongoDB Local](https://events.mongodb.com/mongodb-local-bangalore)

- [The thought process behind starting Profile.fyi](https://youtu.be/JvMEgmKUR9I?si=lWpBU-sUjkCde_qN)

  •
  [FuelEd](https://youtu.be/JvMEgmKUR9I?si=lWpBU-sUjkCde_qN)

- [Being a curious software engineer](https://www.youtube.com/watch?v=vjfypJq5pK8)

  •
  [Scaler Pod](https://www.scaler.com/)

- Evolution Of Your Data Analytics Stack: From Startups To Unicorns

  •
  [The Maker's Summit by Inc42](https://themakerssummit.co/)

- Enigneering curiosity and navigating career growth

  •
  [Rocketium](https://rocketium.com/)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/collaborate

Collaborate with Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

# Collaborate with me

I am on a mission to bring out the best engineering stories from
around the world. If you are doing something interesting and want me
to dissect and talk about it, drop me an email at `speak.with.arpit@gmail.com`.

![](https://github.com/arpitbbhayani/articles/assets/4745789/243251e3-7b00-4c09-947f-5faf37f200e4)

![](https://github.com/arpitbbhayani/articles/assets/4745789/ffb610a7-8731-4331-9079-475f912bee6f)

![](https://github.com/arpitbbhayani/articles/assets/4745789/0bedba31-4e80-460d-9d71-94a7a02b73a7)

![](https://github.com/arpitbbhayani/articles/assets/4745789/b3557dc1-b3b5-4df7-b4a4-3fd509706615)

![](https://github.com/arpitbbhayani/articles/assets/4745789/41f656ca-30f9-4623-93dc-8bc553237542)

# Here's how I can help

I have been actively writing and sharing my learnings on social
platforms like [LinkedIn](https://www.linkedin.com/in/arpitbhayani/),
[Twitter](https://twitter.com/arpit_bhayani), and
[YouTube](https://www.youtube.com/c/ArpitBhayani) and have built a total following of **350,000+ engineers**.

I post no-fluff deep-tech content across my social handles along with
detailed videos dissecting blogs and research papers on my YouTube.
People follow me for the things I post to become better and curious
senior engineers. I can help you

- showcases your engineering prowess,
- get the eyeballs of quality engineers on your product or org,
- attract great talent.

Here are some of the ways I propose we do it

- [a five part video series](https://www.youtube.com/watch?v=Jddce8sPFsc&list=PLsdq-3Z1EPT3s3nghXpr0NDpgN3ZIoK4O) dissecting your engineering blogs, or
- [a one-off social media post](https://www.linkedin.com/posts/arpitbhayani_asliengineering-activity-7191055241314742273-w8By?utm_source=share&utm_medium=member_desktop) showcasing your engineering prowess, or
- [a deep-tech engineering conversation](https://youtu.be/yhqW82Ka-gw?si=0Dpq1W9VpVPh_BxF) (podcast) on my channel

By the way, I have already done similar engagements with multiple
companies (details below), so, if you think I can help, drop me an
email at `speak.with.arpit@gmail.com`. Always up for a
chat.

# Past collaborations

All the videos on my channel are no-fluff and my audience prefers me
doing technical deep dives like outage dissections, blog dissections,
paper readings, and deep-tech podcasts as I have a tendency to explain
the dense concepts pretty well.

To date, I have worked with [JioCinema](https://jiocinema.com/),
[Rockset](https://rockset.com/),
[CockroachDB](https://www.cockroachlabs.com/),
[Razorpay](https://razorpay.com/),
[Replit](https://replit.com/),
[CodeCraters](https://codecrafters.io/),
and [Dukaan](https://mydukaan.io/) helping them
get the quality eyeballs or showcase their engineering prowess to the community.
Here are some of the videos podcasts I did

- [How JioCinema live streams IPL to 20M concurrent devices w/
  Prachi Sharma](https://www.youtube.com/watch?v=36N1Bz7qW0A)
- [How Dukaan moved their infra to bare metal (Hindi)](https://youtu.be/vFxQyZX84Ro)
- [Everything you need to know about CockroachDB w/ Ben Darnell](https://youtu.be/yhqW82Ka-gw?si=n1yGPVGcz40aF9HQ)
- [Razorpay's journey to microservices](https://youtu.be/yqkyq8TPWbg)

I have also dissected and explained engineering blogs and papers from
various companies on my YouTube. Here are some of them

- [How Reddit designed their metadata store to serve 100k req/sec at
  p99 of 17ms](https://www.youtube.com/watch?v=b9zh0SFbkqM)
- [How Airbnb designed and scaled its central authorization system -
  Himeji](https://youtube.com/watch?v=5FIPtC3xJSQ)
- [How Booking com designed and scaled their highly available and
  performant User Review System](https://youtube.com/watch?v=BFyWl9MNDjY)
- [The architecture of Grab's data layer that serves millions of
  orders every day](https://youtube.com/watch?v=KeV4erIm47o)
- [... and many more.](https://www.youtube.com/playlist?list=PLsdq-3Z1EPT0RrDebPvBNlmuXDfT6Qs2T)

So, if this sounds exciting and you think I can bring value to you,
drop me an email at `speak.with.arpit@gmail.com`
or reach out on my socials - [LinkedIn](https://www.linkedin.com/in/arpitbhayani/) and
[Twitter](https://twitter.com/arpit_bhayani).

[shoot me an email →](mailto:speak.with.arpit@gmail.com)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/3p

3P framework to know when it is the right time to make a switch

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

# 3P framework to know when it is the right time to make a switch

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Knowing when to switch roles or companies significantly impacts your career growth and trajectory and I have a simple 3P formula that can help you find the right time to switch.

1. Paisa (money)
2. Power (core competency growth)
3. Position (ladder growth)

At any company you are working at or switching to, you should get at least two of the three Ps. If you are getting fewer than two, it is time to switch.

1. Paisa (Money)

Monetary compensation is often a primary motivator for job change. Consider a switch if your current role does not provide enough or if the increments do not keep pace with industry norms. If the other two P’s outweigh your average salary, it might be worth staying at the current company.

2. Power (Core Competency)

Power in this context refers to your growth in core competency and how close you are to becoming a subject matter expert in the domain you operate in. Aim to become a really good engineer, and a good job will always present you with opportunities to become one.

Assess whether your current role challenges you, introduces you to new technologies, methodologies, or projects, and ultimately contributes to your professional depth and breadth. Again, if the other two P’s outweigh the lack of core competency, it might be worth staying at the current company.

3. Position (Ladder Growth)

The third P, Position, involves your upward movement in the org ladder. Your official title matters and it dictates the roles and responsibilities you have handled. Hence, an important criterion to decide if it is the right time to switch or not.

Assess if your current job provides a clear and actionable path for promotion and increases in responsibility. Stagnation can often lead to your future employer doubting your abilities and will negatively impact your career growth.

Again, if the other two P’s outweigh the lack of ladder growth, it might be worth staying at the current company. Most people remain an L5 at Google is an example of this.

I always kept evaluating my situation every 6 months and kept over-optimized for two of the three Ps. For example,

1. at Practo, I optimized for Power and Paisa
2. at Amazon, I optimized for Position and Paisa
3. at Unacademy, I optimized for Position and Power
4. at Google, I optimized for Power and Paisa

My entrepreneurial stint has been about optimizing for Position and Power with a hope for a high gain in the third P in coming years.

To me, this has been a pretty structured framework to guide my thinking process, ensuring that my career decisions are both strategic and beneficial in the long run. Hope it helps you as well.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/leverage-the-equilibrium

Leverage the equilibrium

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Leadership](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Leadership](/knowledge-base/career-growth)

# Leverage the equilibrium

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Whether you are an IC or a leader, you will operate with one of two leadership styles - the one where comfort takes priority or where the key driving factor is competition.

Depending on your personality you will be naturally inclined towards one of the two, but doing it in excess is detrimental. Knowing how to strike the right balance is key to driving great outcomes.

The comfort-driven leadership benefits in research and exploration work where the output is uncertain. Given the ambiguity of the task, it is essential to create an environment where teams and peers feel secure, supported, and valued.

But, doing this in excess leads to complacency, and there have been many projects at big techs that keep getting delayed or are shipped with subpar quality because of this operating style.

That’s where the second style comes in which thrives on competition to drive excellence. It is all about pushing the teams to their limits by setting ambitious goals and fostering a sense of healthy rivalry (within or outside the team).

This approach can be incredibly effective in high-pressure situations, like taking the product from 0 to 1, during critical releases, or reacting to a competitor’s feature release. The adrenaline rush is what gets the team to put in twice the effort to ship in half the time.

This can yield impressive results in short bursts, but it is not sustainable in the long term. It gets things done quickly and can bring out the best in high-performing individuals. This almost always leads to burnout and attrition when the expected results and outcomes are not seen.

Introspect your leadership type and build an intuition to switch between these styles and maximize the outcome. Know which emotion to trigger, when, why, and how to achieve the desired outcome.

More importantly, know which team members respond to which motivators and tweak the style to bring out the best in each individual. But almost always, create a healthy environment where team members feel secure enough to take risks and experiment, but also motivated to push their limits and deliver excellence.

Leadership is not about extremes; it’s about leveraging the equilibrium.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/not-everything-needs-to-be-dumbed-down

Not everything needs to be dumbed down

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

# Not everything needs to be dumbed down

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

We are constantly bombarded with the idea that everything needs to be simplified. However, some concepts are inherently complex because they solve a complex problem.

I am not against simplifying explanations; they are an excellent starting point. But, as you progress in your career, you might not find a beginner-friendly explanation for some things you are working on. The only way out will be to go through the tough explanation, some not-so-well-written doc, or some obscure code.

So, it is crucial to build a habit of reading more detailed, in-depth content, even as a starting point.

Apart from this, one concern I have when something is dumbed down is how nuanced details get lost in the process. Going through such over-simplified stuff gives you an illusion of mastery, but in reality, it has given you an extremely superficial understanding. This is precisely what makes people say things like “I will use NoSQL because SQL doesn’t scale”.

> Some concepts cannot, do not, and should not have a “simple” explanation and should be covered with all their nuances.

So, when you find time do hard things - read blogs, read papers, watch conference videos, explore OSS code, etc. you know the drill. Going through these is daunting, time-consuming, and takes a lot of mental effort, but it is extremely important and rewarding.

By doing this consistently, you develop abstract thinking, which allows you to see patterns across different domains, draw analogies between seemingly unrelated concepts, and come up with out-of-the-box solutions. It also helps you understand a complex system without getting lost in the details. To be honest, abstract thinking is one skill that separates good engineers from great ones.

On a personal note, there is a certain satisfaction in truly grasping a complex concept. It’s like solving a challenging puzzle. The “aha” moment when everything clicks into place is incredibly motivating and gives you a nice dopamine kick.

So, whenever possible, do not seek out the simplest explanation and challenge yourself. Pick up that dense technical book. Watch that 3-hour conference talk. Read that research paper, even if you only understand 60% of it at first.

The goal is not to understand everything immediately, but to expose yourself to new ideas, to stretch your mental models, and to gradually build a deeper understanding; and in the process become a better engineer.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/best-resource-is-mythical

The best resource is mythical

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

# The best resource is mythical

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Best resource is mythical, it doesn’t exist. To be honest, it does not matter from whom you are learning from and from where, be it YouTube, books, papers, or blogs. The only thing that matters is that you are learning consistently.

You could be learning from a big-name tech influencer or some unknown engineer who just happens to explain things in a way that clicks for you. Some might explain certain concepts better than others, while others might have more up-to-date information.

Instead of waiting for the perfect, start with what’s available to you. Pick something that seems reasonably good. You can always supplement it with other sources. This allows you to have a high bias for action and start making progress immediately, rather than being stuck.

But remember, everybody makes mistakes. The resources you are referring to will have some mistakes. So, build a habit of understanding what you are learning and thinking critically. If you have even an iota of doubt, refer to other resources, get your doubts cleared, and resume.

In any case, you should always be ready to unlearn what you thought you knew and relearn the correct information. Stay flexible, adapt, and continue to become better every single day.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/retention-vs-understanding

It's not about what you know, but about how you think

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

# It's not about what you know, but about how you think

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

I speak with many engineers (both senior and junior) daily and one question I get asked a lot is “How do I retain the things I am learning”. My answer to that is simple … I do not.

I have seen people while reading a book, complete a few pages and then try to recollect what they read. If you are one of them then by doing this, you are testing your recall and not understanding. To me, this is not the right metric to chase.

Whenever you are reading a new concept, focus on understanding the underlying pattern and the core concept instead of remembering the bullet points. But what if someone asked you something, shouldn’t you thoroughly answer it?

Yes, you should. but thorough answers need not be bookish. If you have built an understanding, then there are three possible cases

1. best case: you would answer because you understood it well
2. avg case: with hints and nudges you will be able to answer
3. worst case: you would recall some keywords which will help you do the quick lookup in the right direction

Thus, even in the worst case, you are looking in the right direction and not being a headless chicken. Also, with the availability of note-taking apps, search engines, and even LLMs, we can offload the task of remembering to our devices; and we then are free to focus on understanding concepts, connecting the dots, and solving problems creatively.

In my experience, prioritizing understanding over memorization has the following benefits.

- it improves your learnability and grasp
- it helps you come up with creative solutions
- most importantly, it keeps you curious instead of a rote learner

By the way, this doesn’t mean that memory isn’t important at all. There is a massive value in having a sharp memory but the point is to shift your focus from rote memorization to deep understanding. So, the next time you’re learning ask yourself the following questions

1. what problem does this solve?
2. what are the underlying principles at work here?
3. how does this connect to concepts I already understand?

Remember, It’s not about what you know, but about how you think.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/know-a-lot

Know a lot, a lot

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

# Know a lot, a lot

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Most good engineers I worked with had a common trait - they just happen to know a lot of random stuff, facts, and practices at surface level. The conversations with them were always fun and insightful as they kept telling and sharing interesting nuggets. Although they were not an expert in those, they did have a primitive idea and understanding of what they were talking about.

They built this by reading articles and watching videos on seemingly interesting topics that they stumbled upon while surfing the internet. They consumed it even though the topics were unrelated to the domain they worked on.

Being open to learning new things is a sign of deep interest in the field and spending time exploring it builds a muscle to learn and grasp varied concepts. An interesting by-product of doing this is cross-pollination, where you can connect the dots draw parallels across fields, and come up with out-of-the-box solutions.

The stuff I am talking about may seem like standalone concepts and facts. Some of them are advanced data structures and algorithms, some fragments of database internals, communication protocols, interesting design choices made by some companies, common pitfalls of using a particular tech, etc.

Now, these facts and understandings are not learned and built overnight, and neither they are learned at your workplaces. These are built by consistently spending time self-studying.

To be honest, this is not difficult to achieve, just make sure you spend some time (say 30 minutes) every weekday to learn stuff that you find interesting and build a genuine interest in those topics.

Over time, you will build momentum and turn learning into a habit and find yourself dissecting complex concepts with ease, drawing connections between seemingly unrelated topics, and confidently navigating the landscape.

ps: there is no need to sacrifice your weekends, 30 minutes every weekday over 3 years is more than enough time to build a really good understanding and become a better engineer.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/out-of-syllabus

Going out of syllabus is okay

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

# Going out of syllabus is okay

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

If you always chase a roadmap then you are just continuing the behavior of sticking to a pre-defined syllabus from your school days, where it was discouraged to go beyond.

I understand that roadmaps provide a sense of comfort and a structured and clear way to achieve something, but, if you are focused on doing the bare minimum, then you

1. miss the opportunity to stumble upon something amazing
2. are at risk of developing a shallow understanding

More importantly, you are robbed of experiencing the joy of learning something super interesting. Some of the most interesting and helpful things I know happened because I went out of the way and spent days understanding the nuances.

To be honest, going out of the syllabus forced me to dig deeper because I started exploring it out of curiosity and genuine interest and not because of some peer pressure. I was driven by intrinsic motivation rather than an external impulse.

I personally believe that deeper understanding is what separates good engineers from the better. By not sticking to some predefined roadmap, you

1. become adaptable and comfortable with uncertainty and change
2. learn how to navigate through errors quickly
3. build an ability to learn any and every concept
4. become great at connecting the dots
5. and most importantly, you learn how to learn.

Again, I am not telling you to abandon roadmaps entirely. They are useful guides, especially when you’re starting out. The key is to use them as a starting point, not an endpoint. Let it not limit your learning and curiosity.

You excel when you dare to differ.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/negotiate-the-offer

Always negotiate the offer

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

# Always negotiate the offer

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Always negotiate the offer. Many people, feel uncomfortable but a good negotiation can significantly impact your career trajectory and financial well-being.

Remember, companies expect negotiation. It’s a normal part of the hiring process. But before you jump and say “you want more”, do your homework, talk to people, and be ready with the following two pointers

1. understand how the company typically structures its offers
2. be honest with yourself about your interview performance

When you are negotiating your offer, the two most important leverage you will have are - your current compensation and a competing offer. If possible, try to secure one of the two. This also emphasizes the importance of negotiating a higher compensation as it affects your future compensation.

Most people become arrogant if they have a good competing offer, but you should always remain respectful and diplomatic. When I put forth a competing offer during negotiation, I always say -

“I’m very excited about the opportunity with your company. I do have another offer that’s offering X. While compensation isn’t my only consideration, I’m wondering if there’s any flexibility in your offer to help make my decision easier.”

The way you negotiate is just as important as what you negotiate. Always maintain a respectful and professional throughout the process. Remember, this conversation is between two adults.

1. express your excitement about the role and the company
2. be specific and quantify your ask, instead of just asking for “more”
3. when you ask for more, explain why you deserve more
4. remember, it is not just about the money

So the next time you’re presented with a job offer, do your homework and negotiate with confidence and humility. Successful negotiations can have a compound effect on your financial well-being, so play well.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/never-bad-mouth-your-ex-exployer

Never bad-mouth your ex-employer

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

# Never bad-mouth your ex-employer

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Most people tend to vent about a frustrating work experience during a job interview. No matter what, you should never bad-mouth your ex-employers, here’s why …

Whenever I asked people about their work experience, most of them shared negative experiences claiming to be brutally honest. I realize people do this for 4 reasons

1. they want to justify their decision to leave
2. they are eager to express their frustration
3. they hope this would make the interviewer more sympathetic
4. they want to showcase how much they value a better environment

No matter how valid these feelings might be, expressing them in a professional context can have severe consequences, some of them that I have observed being brought up during debriefing

1. showed a lack of maturity
2. suggested inability to handle conflicts and difficult situations
3. suggested inability to challenge constructively
4. indicated that they might bring negativity to the organization
5. shows that you are almost always on a flight risk

One of the biggest consequences of this is how this behavior can damage your reputation. Remember, our industry is very small and after a decade of experience, almost everyone is your 1st or 2nd degree connection. So, be mindful of how you conduct and operate. Whenever you are talking about your ex-employers, make sure

1. you focus on positives, learnings, and growth
2. mention challenges as learning experiences
3. if you are discussing a conflict, just mention your role
4. if someone is asking about negatives, just redirect the question

Although, your technical skills are crucial, but your ability to work well with others and handle challenging situations professionally is equally important. Like I always say, remember, nobody wants to work with a genius jerk.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/culture-fit

Prove you are a culture-fit

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

# Prove you are a culture-fit

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Interviews are designed to evaluate you on two things - Core competency and Culture Fit and to be honest, both are equally important. Core competency is what most of us prepare for, but what about the other?

A couple of coding tests and system design rounds are enough to test the skills. However, evaluating cultural fit is ambiguous and subjective. When I interview someone, I look for the following three things

1. enthusiasm for the role,
2. how pleasant it will be to work with with him/her, and,
3. attitude and behavior under a pressure situation

The above pointers hold for more companies and interviewers, but the list is not exhaustive by any means. To demonstrate that you are a culture fit for any company, I would highly recommend you to

1. research the company, role, and team, well
2. listen before you speak
3. never interrupt the interviewer
4. disagree, but with respect and humility
5. emphasize collaboration and demonstrate genuine curiosity
6. acknowledge that your past achievements were the team’s success
7. show enthusiasm for the company’s mission and goals.

Remember, for any pointer you put forth, back it with some situation or incident from your experience; keep it short and crisp. The most important gesture, end the interview by thanking the interviewer for his/her time. It might just tip the scale in your favor.

No matter if you are an exceptional engineer, if you cannot be a team player, the company would be happy to part ways with you. Nobody wants to work with a genius jerk.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/quantification-in-resume

Quantify your resume, through and through

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

# Quantify your resume, through and through

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

One of the most common lines I find in resumes is “Implemented features and bugfixes”, but this line adds zero value because…

it is vague and it tells the hiring manager absolutely nothing about the impact of your work; literally any engineer could have added this line to their resume and it still would have made sense.

It is important to showcase real-world outputs and outcomes in your resume and this is best done by quantifying the outcome of your work. Two simple templates that you can leverage -

1. improved X by Y%
2. improved X by doing Z

Instead of writing “worked on recent searches”, I write “improved CTR by 42% by implementing recent search feature”. This is where I have shown the needle-moving impact of my work.

Even in the case of fixing a bug, you should quantify against the error it was causing, maybe the number of errors, the number of users impacted, or anything usecase specific - eg: reduced transaction failures by 80% by resolving issue X.

By the way, you should know your numbers, and if you don’t ask your manager, or product manager, or refer to the monitoring tool.

Remember, the goal of quantification is not to inflate the achievements, but to accurately represent the value you bring to a table. By quantifying your impact, you’re not only showing what you can do but why it matters.

Thinking in numbers shifts your mindset and makes you think and focus on outcomes instead of outputs.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/hiring-is-unfair

Hiring is subjective and unfair

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

# Hiring is subjective and unfair

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Hiring is subjective and in some cases unfair, but here are a few things that can help tip the scale in your favor

Ideally, the decision should been based on objective, quantifiable criteria, but that’s rarely the case. Each member of the hiring committee uses their intuition to rate you against some rubrics. This rating, although a number, is still subjective and the final decision often comes down to a gut feeling.

Given this subjectivity, here are a few things that will help to tip the scale in your favor.

It always pays to be humble and pleasant during interviews. Make sure you articulate your thoughts well, speak clearly, and put forth your views and understanding in a structured manner. Leaving a good impression will earn you some extra pointers and turn an 8 into a 9.

I do remember interviewing a candidate who was a decently strong engineer (definitely not the strongest) but he was curious, had a great attitude toward work and learning, and was communicating his thoughts very well. The committee felt that this person would be a valuable team player and decided to extend an offer.

By the way, this does not mean you should only focus on your soft skills and ignore the rest. Your tech competencies matter, but in case of a tie or even when you lag a bit, a company will always prefer someone who has showcased the right mindset and a genuine enthusiasm for the role and the company.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/questions-for-interviewers

Questions you should ask to your interviewer

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

# Questions you should ask to your interviewer

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Whenever the interviewer asks “Do you have any questions for me?”, never skip this question. Here are 7 questions that I have actually asked the interviewer to understand the company and role better

1. What is the team’s biggest challenge right now?
2. What new tech is the team experimenting with?
3. What is that one company project that you are excited about?
4. What was an interesting work the team shipped recently?
5. How do you measure the success of engineering initiatives?
6. How does the team/company celebrate wins?
7. What are some company traditions that you enjoy?

Each of the above questions covers one interesting aspect of the role and team, pick one that is most important to you. I ask one of these questions because I am interested in innovation, culture, growth, dealing with failures, and decision DNA.

Remember, it is important to ask questions in the interview for two reasons - first, it demonstrates your genuine interest in the role, position, and company; second, it allows you to get some super important insights about the role and company.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/be-curious-not-judgemental

Be curious, not judgemental

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

# Be curious, not judgemental

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

One of the unsettling trends I observed at big tech and FAANGs is that people are more interested in critiquing ideas than in understanding them.

BigTechs have a massive promotion-driven culture. Every action one takes is kind of a tick on one of the traits for promotion. Hence the behavior stems from a desire to be ‘visible’ and to stand out, but it comes at the cost of belittling someone and giving up on genuine curiosity and learning.

I am not saying not to ask questions in meetings and take things at face value, but sometimes, people need to be more curious without trying to show off their superiority over others.

One simple way to do this is by having a ‘no-criticism’ rule until the person is ready to take questions or has covered a significant portion of the topic i.e. an ‘explore first, critique later’ mindset.

Again, I will reiterate that it is important to strike a balance between curiosity and critical thinking. While it’s crucial to be open to new ideas, it’s equally important to evaluate them rigorously before implementation. The key is to ensure that the initial phase is free from judgment.

Be curious, not judgemental.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/collaboration-communication

Collaborate and communicate better

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

# Collaborate and communicate better

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

We engineers are great at coding, but things get shaky when it comes to collaboration. Here are three simple rules I followed that helped me build trust with other engineers and teams. When I was leading horizontal teams - SRE and Data Engineering - at Unacademy, me and my team were known for Ownership, Execution, and Collaboration. This happened because of the three rules we followed

1. Always close the loop

The core of this is never to leave any discussion or thread open-ended. For example, if there was some bug to fix from our side, we made sure to follow up with the individual and confirm the resolution after the fix was pushed.

Even when some task was done on other platforms (say Github) and that auto notifies the user, we still ensured to close the Slack thread by putting the final message on it. This way, even if some leader was going through the Slack messages or threads, he/she needs to wonder about the current state.

2. Be humble, no matter what.

In a collaborative environment, it’s essential to listen to others to understand their perspective. It is possible that we do not have all the answers or could extrapolate our understanding. So instead of being aggressive in defending our approach, we listened to what others had to say, discussed with them to understand their pain points, and then came up with an approach that worked for both of us.

We always patiently listened to them, and this led to us having a higher chance that they would take the approach we suggested, or at least be a relatively easier and minimal change on our end.

3. Do not make others follow up, ever

If someone is waiting for your input or action, it can make them anxious. So we ensured to be proactive in communicating the progress, the completion, the delays, and the blockers.

This kept everybody in the loop, made us operate with high ownership, and made the entire collaboration transparent and pleasant. The best thing, after seeing us operate this way, other teams reciprocated whenever they were working with us :)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/out-of-vicious-interview-cycle

Once you are out of vicious interview cycle

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

# Once you are out of vicious interview cycle

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Once you land at a good company with decent pay, do not just keep preparing for interviews and be stuck in this endless cycle.

Engineering is a beautiful domain, when you pursue it without the pressure of deadlines or deliverables. So, once you are out of the cycle, spend some time exploring things you were always curious about or pick up projects that you always wanted to build.

Again. I am not saying not be prepared for interviews, it is always good to be prepared, but there are some things you do for your living and some things you do for your soul. This is extremely rewarding, fulfilling, and crucial for your growth as an engineer.

Exploring new domains might open up new opportunities and make you a much stronger engineer. It might help you make a solid impression at workplace when you give an interesting tech talk on some obscure topic, or better help you come up with an out-of-the-box solution to a complex problem.

Some domains that I love to explore out of sheer curiosity are - database internals, language internals, Approximate Algorithms, and Information Retrieval. To find an interesting domain, just visit CS curriculum of an ivy league college and see what strikes the right note.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/pitch-projects-not-ideas

Pitch projects, not ideas

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

# Pitch projects, not ideas

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Most of us struggle to get our ideas to be prioritized and worked upon, they are awesome, but still, leadership ignores them, why? The answer is simple - we pitched the idea and not the project. Let’s understand the difference.

Ideas are abundant and easy to come by, but execution is what separates successful projects from mere concepts. Leadership needs to see a clear execution path, and that’s where many engineers fall short.

When you pitch a project, you show that you’ve thought through the details and are prepared to bring the idea to life. Some of the things you can do are

- estimating the number of engineers required
- list down skill set requirements
- estimate the timeline of the project with a buffer
- list down the risks involved
- predicting potential outcomes and benefits for the org.

This not only increases your chances of approval but also demonstrates your capability to see the project through to completion and thus increases the success rate of getting your project approved.

It is all about doing your homework well. Pitch a project, not just an idea.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/read-design-docs

Read design docs, even if they seem unrelated

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

# Read design docs, even if they seem unrelated

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

One habit I built during my early days was to read design docs, even if they did not belong to my team. The first thing I did after joining Amazon, back in 2016, was to go through their internal Wiki portal. The portal hosted all the public design docs and documentation written by various teams. The portal was a goldmine of information.

One thing that I absolutely love about design docs is how practical they are. The designs are not just some random set of boxes drawn on a piece of paper, but rather they contain highly practical approach to solving a problem and the solution will be shipped to production.

With multiple engineers writing them and several tech leads reviewing them, these docs hold all the required context, trade-offs made, alternate designs, implementation nuances, and potential pitfalls. Reading them gives a deeper understanding of the domain, the problem, and the system.

To be honest, I was initially quite overwhelmed reading them. But over time I got used to it and started connecting the dots. So, if you try to do this, do not be discouraged by the initial complexity, because things will get easier over time.

So, if your company also practices writing design docs, do spend time reading them, even if they are from different teams. If not, then be the one who initiates and drives this process.

Forming a habit of reading design docs consistently, rewired my thought process and made me a better engineer; hence I would highly recommend you pick this habit up.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/read-rca-docs

Be excited when production goes down

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

# Be excited when production goes down

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

I get excited whenever there is a production outage, because I know I will be learning something new and interesting very soon. I always hop on the call, even when I am not on call :)

Being part of stressful situations and seeing how seniors fixed the issues and operated with a calm head made me a better engineer. Reading old postmortem documents (RCAs) of outages at all my workplaces became a habit.

If you are an engineer early in your career, do not shy away from being on-call and spend some time reading RCAs. They are filled with interesting, practical, real-world insights about the systems, coding blunders, tuning parameters, etc.

To be very honest, you will learn more from being on-call than literally anything out there.

By the way, I have a massive playlist on my YouTube. I dissected 18 production outages of GitHub, Atlassian, Spotify, etc., and went deeper into the details not even mentioned in their blogs. The link to the playlist is in the comments, in case you want to check them out.

One interesting outage that happened at GitHub happened because their primary key went beyond the max value, and it had a ton of details on how they fixed it. It had a ton of details about running data migrations without downtimes.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/generalist-not-mediocre

Being a generalist does not mean being mediocre

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

# Being a generalist does not mean being mediocre

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

All good engineers I know are generalists, but if required, it would not have taken much time to become a specialist. They all could grasp any concept quickly and start contributing to any system from day one.

Don’t get me wrong, I am not saying you should not specialize. Specialization is important and you need to eventually become one because early in your career you are paid to get things done, while you are paid to minimize the blunders in your later stages.

During the first 6-8 years focus on becoming a generalist and explore many verticals. While doing this, identify one thing you love, and build raw expertise. To be honest, it takes time to figure out a domain you genuinely are passionate about.

By the way, being a generalist does not mean being mediocre in everything; it means building a strong foundation across core concepts, allowing you to be flexible, adaptable, and ready to tackle any problem statement. The strong foundation and varied experiences will help you

1. spot gaps in existing systems, which specialists might overlook
2. cross-pollinate and come up with innovative solutions
3. become empathetic and hence a better team player

So, don’t say no early in your career; optimize for learning and exploring as much as you can, while building an ability to learn, unlearn, and relearn quickly.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/do-not-rely-on-summaries

Summaries create an illusion of mastery

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Learning](/knowledge-base/career-growth)

# Summaries create an illusion of mastery

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

LLMs are great at summarization, but summaries are not great for learning. Many engineers, across all levels, have started relying solely on summaries generated by LLMs to understand any concept. But it does more harm than good.

Learning from summaries is quick and easy. It makes you think that you understood the concept, but one probing question and you will realize how surface-level your understanding was.

Summaries are lossy, which means there is a loss of information (crucial details) when your favorite LLM is summarizing stuff for you. Uncovering that information requires you to ask deep questions to yourself and subsequently to LLMs.

Asking tough questions is tougher, and it requires you to have a deeper understanding of the subject. Deep understanding cannot be built just by reading summaries, which is the new habit in town. Do you see the cyclic dependency here?

Don’t get me wrong, summaries are important as they act as a great starting point, but solely on them is catastrophic. So, whenever you find time, grind it out - read books, blogs, papers, docs, and good codebases. Once in a while build projects and prototype the concept.

The depth of understanding separates the best from the better, the better from the good, and the good from the average.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/do-not-just-ship-features

You are not hired to just ship features

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

# You are not hired to just ship features

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

You are not hired to just ship features but rather ship features that matter. All great engineers I know understand the worth of their work and the business impact it brings; this way they prioritize the tasks that matter.

Do not be a transactional engineer - given a work, get it done without thinking too much about it. Operate like you own it, almost like the CEO, ensuring things are put to completion on time and with maximum impact. A few things I highly recommend you do are

1. understand the worth of your work
2. prioritize work that matters to the business
3. be responsible about your work and deliver on time
4. if something goes wrong, own it
5. acknowledge the help you got from your peers to get it done

When you get some work, step back and see the bigger picture. Elevate from being just a coding machine.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/structure-your-design-interviews

Structure your System Design interviews

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

# Structure your System Design interviews

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

System design interviews can be tricky because the discussions are always open-ended. Instead of getting overwhelmed, here is a structured approach that you can use to put your best foot forward and crack it with ease.

1. understand the problem statement: 3 to 4 minutes

Instead of jumping into designing the system from the word go, spend the first few minutes understanding the problem statement, and asking clarifying questions to understand the context, constraints, and expectations.

2. design a day-0 solution: 5 to 7 minutes

Now take 5-7 minutes to design a simple, naive, unoptimized day-0 solution. It is okay if this design is not scalable, available, or fault-tolerant With this the interviewer

- will have a fair bit of idea about your approach,
- can tell you if you are on the right track or not
- or if you should be focussing on a specific component more

3. improve your design: 15 minutes

Now for each component in your system, assume it in different scenarios - large number of requests, system crash, network failures, etc. and see how your system behaves. Rearchitect it to handle the failure cases gracefully, and add new components if required.

4. ask the interviewer for any follow-up questions: 8 minutes

Allow the interviewer to ask you follow-up questions about your system and you provide him/her with justifications for your design and trade-offs you took. This segment is important because it just turned your interview into a discussion.

5. go into finer details: 8 minutes

Depending on the conversation and expectations, touch upon the finer implementation details like handling concurrency, streaming responses, transient failures, etc. This is where you showcase that you know what you are talking about.

This is how I always spend 40-45 minutes during the System Design interviews I appear in. Hope this helps you crack your next one.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/title-inflation

Never let your company inflate your title to retain you

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

# Never let your company inflate your title to retain you

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Never let your company inflate your title to retain you.

A lot of companies hand out senior titles to improve retention. Don’t take it as a sign of true advancement. Let your title be something that is aligned with the quality and impact of your work.

Your inflated title may get you a fancy interview call, but when the interviewer asks for the impact, the quantifications, the challenges, and the complexities, you might not have an answer. Nothing beats the practical experience.

Building a strong foundation of technical skills and a track record of solving problems, improving business, and increasing revenue. Leave a lasting impact at work to ensure your value remains constant, regardless of title trends.

Don’t misunderstand – titles have their place. They offer a framework for understanding roles and experience. Be true to yourself, evaluate your contributions, and ask for the title you deserve.

But for those who seek a fulfilling and impactful career in software engineering, the true measure lies in the outcome and impact you bring by the work you do. Build a career that’s not just decorated with titles, but rich with accomplishments.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/find-your-own-project

Find your own project

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

# Find your own project

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Beyond a certain level, you are required to come up with new ideas and do something innovative to get promoted, but how exactly?

At some point in your career, you are required to propose some potential projects that your team or your organization can take up. Coming up with project ideas is a sign of a good senior engineer.

It indicates that you are actively looking at potential gaps in your product and thus solving them can lead to an increase in revenue, cost reduction, or an improvement in user experience. To come up with new projects, here are a few things you should actively do

1. talk to your customers and understand potential gaps
2. keep exploring new things, every single day
3. prototype to build a practical understanding
4. watch conference talks, explore new GitHub projects
5. understand the nuances by going deeper through source code and papers

It takes one paper, one blog, one talk, or one conversation, to come up with one insight that will change your trajectory. So, instead of waiting for your manager to give you that magical project, pitch yours.

Remember, a senior engineer finds their own work.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/six-pointers-to-crack-coding-and-design-interviews

Six pointers to crack coding and design interviews

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Interviews](/knowledge-base/career-growth)

# Six pointers to crack coding and design interviews

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

These are the 6 pointers that I used throughout my career, which made it relatively easy for me, and might help you as well, to crack your next big interview.

1. explain your thought process throughout the interview - don’t be quiet
2. start with a _relatively_ simpler solution - not naive
3. ask clarifying questions to understand constraints and assumptions better
4. the interviewer wants you to win, so when in doubt ask for help and hints
5. when confused or about to panic, take a deep breath, and start over
6. write code in a language you are extremely comfortable with

I have personally preferred Python during interviews because it makes it really easy to express your solution quicker through a more concise code v/s going for high verbosity Java or C++ code.

note: some interviews are meant to judge you on a specific language, so clarify this with your interviewer before you jump to the solution.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/keep-yourself-unblocked

Keep yourself unblocked

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [At your job](/knowledge-base/career-growth)

# Keep yourself unblocked

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

We as software engineers encounter roadblocks and moments of frustration every now and then, but a good engineer always **remains unblocked**. Although counterintuitive, getting yourself unblocked is an art and it indicates a strong problem-solving ability. In this article, let’s talk about how to remain unblocked.

# Why should you remain unblocked?

Let me make this clear, it is very common and natural to get stuck and it is a sign that you are exploring new domains or problems, and in general, pushing your limits. However, remaining stuck for prolonged periods can lead to stagnation, frustration, and a loss of motivation.

It becomes quite awkward and guilty to keep saying “Hey, I was stuck …” or “Hey, I was blocked …” too often during your standups. Once in a while it is okay, and acceptable, but this cannot be your daily update.

# How to keep yourself unblocked

It is important to _somehow_ find a way to get unblocked, and getting unblocked is an art and like all the art there is a structure to it. Here are a few things I do to remain unblocked.

## Broadcast - Ask

![Broadcast](https://user-images.githubusercontent.com/4745789/283439518-0bf57764-7321-4261-b7e4-79fe3bd98a96.png)
Ask your teammates to help you out. Sharing your challenges with others can provide fresh perspectives and potential solutions. It is very likely, that while you are explaining the problem to your team, you realize a way to solve it. To improve your chances of getting a quicker response, it is also important to **build camaraderie** with the team.

If your organization has an internal portal where people ask questions, maybe a Slack channel, google group, or even StackOverflow-like portal, ask…just ask.

## Diversion - Take some time off

![Diversion and Time off](https://user-images.githubusercontent.com/4745789/283439515-f28122c7-36c1-4803-991e-36665989171b.png)
When you’re stuck, it’s often helpful to take a step back and approach the problem from a different angle. If you are at work, consider replying to a few emails and some Slack threads, or completing any mundane activity.

This diversion, will either help your brain process the problem subconsciously and find an answer or it will give you a fresh perspective when you start again. Sometimes, the solution presents itself when you least expect it.

## Breakage - Divide and Conquer

![Divide and Conquer](https://user-images.githubusercontent.com/4745789/283439512-0ca431af-db1d-4482-a051-6618c622b2a5.png)
Complex problems can be overwhelming, leading to mental blocks. Instead, try breaking the problem down into smaller, more manageable components. This approach makes the task less daunting and allows you to tackle each segment systematically.

## Grind - Tackle harder

![Grind and Tackle Harder](https://user-images.githubusercontent.com/4745789/283439509-4f4e6875-ba7b-4eda-b1be-fe4356604424.png)
Go through the grind, and explore source code, online resources, documentation, and tutorials to gain a deeper understanding and potential solutions. Experiment with different approaches and techniques, keeping track of your insights, attempts, and learnings.

With the advancements in AI, leverage LLMs to get hints to your solutions. Although, LLMs might not be 100% accurate, but use the potential solution as a guide that nudges you in the right direction.

## Persuasion and Escalation

![Persuasion and Escalation](https://user-images.githubusercontent.com/4745789/283439500-812468a2-5e45-4ade-872a-c21426fcc579.png)
If you are blocked on some other team then you cannot just sit and crib about it; find a way to get unblocked. Build a camaraderie with them and get your work prioritized, if you are good with communication then persuade, if nothing works then escalate through the right channel.

# Summary

I would like to conclude it on a note that, getting blocked at work is quite common and natural and it is okay to be blocked once in a while, but find a way to unblock yourself, whatever it takes. Once you unblock yourself, make sure that you do not get blocked for the same reason; learn from it and become a better engineer.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/genetic-knapsack

Genetic algorithm to solve the Knapsack Problem

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Genetic algorithm to solve the Knapsack Problem

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

The Knapsack problem is one of the most famous problems in computer science. The problem has been studied since 1897, and it refers to optimally packing the bag with valuable items constrained on the max weight the bag can carry. The [0/1 Knapsack Problem](https://en.wikipedia.org/wiki/Knapsack_problem) has a pseudo-polynomial run-time complexity. In this essay, we look at an approximation algorithm inspired by genetics that finds a high-quality solution to it in polynomial time.

# The Knapsack Problem

The Knapsack problem is an optimization problem that deals with filling up a knapsack with a bunch of items such that the value of the Knapsack is maximized. Formally, the problem statement states that, given a set of items, each with a weight and a value, determine the items we pack in the knapsack with a constrained maximum weight that the knapsack can hold, such the the total value of the knapsack is maximum.

## Optimal solution

The solution to the Knapsack problem uses Recursion with memoization to find the optimal solution. The algorithm covers all possible cases by considering every item picked and not picked. We remember the optimal solution of the subproblem in a hash table, and we reuse the solution instead of recomputing.

```
memo = {}

def knapsack(W, w, v, n):
    if n == 0 or W == 0:
        return 0

    # if weight of the nth item is more than the weight
    # available in the knapsack the skip it
    if (w[n - 1] > W):
        return knapsack(W, w, v, n - 1)

    # Check if we already have an answer to the sunproblem
    if (W, n) in memo:
        return memo[(W, n)]

    # find value of the knapsack when the nth item is picked
    value_picked = v[n - 1] + knapsack(W - w[n - 1], w, v, n - 1)

    # find value of the knapsack when the nth item is not picked
    value_notpicked = knapsack(W, w, v, n - 1)

    # return the maxmimum of both the cases
    # when nth item is picked and not picked
    value_max = max(value_picked, value_notpicked)

    # store the optimal answer of the subproblem
    memo[(W, n)] = value_max

    return value_max
```

## Run-time Complexity

The above solution runs with a complexity of `O(n.W)` where `n` is the total number of items and `W` is the maximum weight the knapsack can carry. Although it looks like a polynomial-time solution, it is indeed a _pseudo-polynomial_ time solution.

Given that the computation needs to happen by a factor of `W`, the maximum times the function will execute will be proportional to the max value of `W` which will be `2^m` where `m` is the number of bits required to represent the weight `W`.

This makes the complexity of above solution `O(n.2^m)`. The numeric value of the input `W` is exponential in the input length, which is why a pseudo-polynomial time algorithm does not necessarily run in polynomial time with respect to the input length.

This raises a need for a polynomial-time solution to the Knapsack problem that need not generate an optimal solution; instead, a good quick, high-quality approximation is also okay. Genetic Algorithm inspired by the Theory of Evolution does exactly this for us.

# Genetic Algorithm

[Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm) is a class of algorithms inspired by the process of natural selection. As per Darwin’s Theory of Evolution, the fittest individuals in an environment survive and pass their traits on to the future generations while the weak characteristics die long the journey.

While solving a problem with a Genetic Algorithm, we need to model it to undergo evolution through natural operations like Mutation, Crossover, Reproduction, and Selection. Genetic Algorithms help generate high-quality solutions to optimization problems, like [Knapsack](https://en.wikipedia.org/wiki/Knapsack_problem), but do not guarantee an optimal solution. Genetic algorithms find their applications across aircraft design, financial forecasting, and cryptography domains.

## The Genetic Process

The basic idea behind the Genetic Algorithm is to start with some candidate _Individuals_ (solutions chosen at random) called _Population_. The initial population is the zeroth population, which is responsible for the spinning of the _First Generation_. The First Generation is also a set of candidate solutions that evolved from the zeroth generation and is expected to be better.

To generate the next generation, the current generation undergoes natural selection through mini-tournaments, and the ones who are fittest reproduce to create offspring. The offspring are either copies of the parent or undergo crossover where they get a fragment from each parent, or they undergo an abrupt mutation. These steps mimic what happens in nature.

![The Genetic Flow - Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156874170-608cd9a4-6241-4882-b123-658d14a64c89.png)

Creating one generation after another continues until we hit a termination condition. Post which the fittest solution is our high-quality solution to the problem. We take the example of the Knapsack problem and try to solve it using a Genetic Algorithm.

# Knapsack using Genetic Algorithm

Say, we have a knapsack that can hold 15kg of weight at max. We have 4 items `A`, `B`, `C`, and `D`; having weights of 7kg, 2kg, 1kg, and 9kg; and value `$5`, `$4`, `$7`, and `$2` respectively. Let’s see how we can find a high-quality solution to this Knapsack problem using a Genetic Algorithm and, in the process, understand each step of it.

![Knapsack Problem](https://user-images.githubusercontent.com/4745789/156769708-68ae14b5-4ccd-484b-b5be-7445ef3526cb.png)

> The above example is taken from the Computerphile’s [video](https://www.youtube.com/watch?v=MacVqujSXWE) on this same topic.

## Individual Representation

An _Individual_ in the Genetic Algorithm is a potential solution. We first find a way to represent it such that it allows us to evolve. For our knapsack problem, this representation is pretty simple and straightforward; every individual is an `n`-bit string where each bit correspond to an item from the `n` items we have.

![Individual Representation Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156770627-e6cc63e9-72b7-4afa-a968-60e994963a26.png)

Given that we have 4 items, every individual will be represented by a 4-bit string and the `i`th position in this string will denote if we picked that item in our knapsack or not, depending on if the bit is `1` or `0` respectively.

## Picking Individuals

Now that we have our _Individual_ representation done, we pick a random set of Individuals that would form our initial _population_. Every single individual is a potential solution to the problem. Hence for the Knapsack problem, from a search space of 2^n, we pick a few individuals randomly.

The idea here is to evolve the population and make them fitter over time. Given that the search space is exponential, we use evolution to quickly converge to a high-quality (need not be optimal) solution. For our knapsack problem at hand, let us start with the following 6 individuals (potential solutions) as our initial population.

![Initial Population Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156789144-c0d14ee5-2ec9-4c51-aaab-a715728a38af.png)

```
def generate_initial_population(count=6) -> List[Individual]:
    population = set()

    # generate initial population having `count` individuals
    while len(population) != count:
        # pick random bits one for each item and
        # create an individual
        bits = [
            random.choice([0, 1])
            for _ in items
        ]
        population.add(Individual(bits))

    return list(population)
```

## Fitness Coefficient

Now that we have our initial population randomly chosen, we define a _fitness coefficient_ that would tell us how fit an individual from the population is. The fitness coefficient of an individual totally depends on the problem at hand, and if implemented poorly or incorrectly, it can result in misleading data or inefficiencies. The value of the fitness coefficient typically lies in the range of 0 to 1. but not a mandate.

![Fitness Coefficient Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156890925-13e0f1bf-ec4a-40fe-8d48-60d867cdacae.png)

For our knapsack problem, we can define the fitness coefficient of an individual (solution) as the summation of the values of the items picked in the knapsack as per the bit string if the total weight of the picked items is less than the weight knapsack can hold. The fitness coefficient of an individual is 0 if the total weight of the item picked is greater than the weight that the knapsack can hold.

For the bit string `1001` the fitness coefficient will be

```
total_value  = (1 * v(A)) + (0 * v(B)) + (0 * v(C)) + (1 * v(D))
             = ((1 * 5) + (0 * 4) + (0 * 7) + (1 * 2))
	         = 5 + 0 + 0 + 2
	         = 7

total_weight = (1 * w(A)) + (0 * w(B)) + (0 * w(C)) + (1 * w(D))
             = ((1 * 7) + (0 * 2) + (0 * 1) + (1 * 9))
	         = 7 + 0 + 0 + 9
	         = 16

Since, MAX_KNAPSACK_WEIGHT is 15
the fitness coefficient of 1001 will be 0
```

The higher the individual’s fitness, the more are the chances of that individual to move forward as part of the evolution. This is based on a very common evolutionary concept called _Survival of the Fittest_.

```
def fitness(self) -> float:
    total_value = sum([
        bit * item.value
        for item, bit in zip(items, self.bits)
    ])

    total_weight = sum([
        bit * item.weight
        for item, bit in zip(items, self.bits)
    ])

    if total_weight <= MAX_KNAPSACK_WEIGHT:
        return total_value

    return 0
```

## Selection

Now that we have defined the Fitness Coefficient, it is time to _select_ a few individuals to create the next generation. The selection happens using selection criteria inspired by evolutionary behavior. This is a tunable parameter we pick and experiment with while solving a particular problem.

One good selection criteria is [Tournament Selection](https://en.wikipedia.org/wiki/Tournament_selection), which randomly picks two individuals and runs a virtual tournament. The one having the higher fitness coefficient wins.

![Selection Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156891433-13a356c7-d219-4a33-b7b3-423cdf10b910.png)

For our knapsack example, we randomly pick two individuals from the initial population and run a tournament between them. The one who wins becomes the first parent. We repeat the same procedure and get the second parent for the next step. The two parents are then passed onto the next steps of evolution.

```
def selection(population: List[Individual]) -> List[Individual]:
    parents = []

    # randomly shuffle the population
    random.shuffle(population)

    # we use the first 4 individuals
    # run a tournament between them and
    # get two fit parents for the next steps of evolution

    # tournament between first and second
    if population[0].fitness() > population[1].fitness():
        parents.append(population[0])
    else:
        parents.append(population[1])

    # tournament between third and fourth
    if population[2].fitness() > population[3].fitness():
        parents.append(population[2])
    else:
        parents.append(population[3])

    return parents
```

## Crossover

Crossover is an evolutionary operation between two individuals, and it generates children having some parts from each parent. There are different crossover techniques that we can use: single-point crossover, two-point crossover, multi-point crossover, uniform crossover, and arithmetic crossover. Again, this is just a guideline, and we are allowed to choose the crossover function and the number of children to create we desire.

The crossover does not always happen in nature; hence we define a parameter called the _Crossover Rate,_ which is relatively in the range of 0.4 to 0.6 given that in nature, we see a similar rate of crossover while creating offspring.

![Crossover Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156891548-bfafdc41-0158-4146-b6c6-b9d14d2c536a.png)

For our knapsack problem, we keep it simple and create two children from two fit parents such that both get one half from each parent and the other half from the other parent. This essentially means that we combine half elements from both the individual and form the two children.

```
def crossover(parents: List[Individual]) -> List[Individual]:
    N = len(items)

    child1 = parents[0].bits[:N//2] + parents[1].bits[N//2:]
    child2 = parents[0].bits[N//2:] + parents[1].bits[:N//2]

    return [Individual(child1), Individual(child2)]
```

## Mutation

The mutation is an evolutionary operation that randomly mutates an individual. This is inspired by the mutation that happens in nature, and the core idea is that sometimes you get some random unexpected changes in an individual.

Just like the crossover operation, the mutation does not always happen. Hence, we define a parameter called the _Mutation Rate,_ which is very low given that mutation rate in nature. The rate typically is in the range of 0.01 to 0.02. The mutation changes an individual, and with this change, it can have a higher or lower fitness coefficient, just how it happens in nature.

![Mutation Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156822218-716ea60d-4d6b-434e-9112-26cba6c93b2c.png)

For our knapsack problem, we define a mutation rate and choose to flip bits of the children. Our `mutate` function iterates through the bits and sees if it needs to flip as per the mutation rate. If it needs to, then we flip the bit.

```
def mutate(individuals: List[Individual]) -> List[Individual]:
    for individual in individuals:
        for i in range(len(individual.bits)):
            if random.random() < MUTATION_RATE:
                # Flip the bit
                individual.bits[i] = ~individual.bits[i]
```

## Reproduction

Reproduction is a process of passing an individual as-is from one generation to another without any mutation or crossover. This is inspired by nature, where evolutionary there is a very high chance that the genes of the fittest individuals are passed as is to the next generation. By passing the fittest individuals to the next generation, we get closer to reaching the overall fittest individual and thus the optimal solution to the problem.

Like what we did with the Crossover and the Mutation, we define a _Reproduction Rate. Upon_ hitting that, the two fittest individuals will not undergo any crossover or mutation; instead, they would be directly passed down to the next generation.

![Reproduction Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156825824-8cac7164-f526-47fd-958b-d24eba11c08a.png)

For our knapsack problem, we define a _Reproduction Rate_ and, depending on what we decide to pass the fittest individuals to the next generation directly. We keep the reproduction rate to `0.30` implying that 30% of the time, the fittest parents are passed down as is to the next generation.

## Creating Generations

We repeat the entire process of Selection, Reproduction, Crossover, and Mutation till we get the same number of children as the initial population, and we call it the first generation. We repeat the same process again and create subsequent generations.

![Creating Newer Generations Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156872749-9e93f05d-6ed1-4283-876d-4e7b62a01de9.png)

When we continue to create such generations, we will observe that the _Average Fitness Coefficient_ of the population increases and it converges to a steady range. The image above shows how our value converges to 12 over 500 generations for the given problem. The optimal solution to the problem at hand is 16, and we converged to 12.

```
def next_generation(population: List[Individual]) -> List[Individual]:
    next_gen = []
    while len(next_gen) < len(population):
        children = []

        # we run selection and get parents
        parents = selection(population)

        # reproduction
        if random.random() < REPRODUCTION_RATE:
            children = parents
        else:
            # crossover
            if random.random() < CROSSOVER_RATE:
                children = crossover(parents)

            # mutation
            if random.random() < MUTATION_RATE:
                mutate(children)

        next_gen.extend(children)

    return next_gen[:len(population)]
```

## Termination Condition

We can continue to generate generations upon generations in search of the optimal solution, but we cannot go indefinitely, which is where we need a termination condition. A good termination condition is deterministic and capped. For example, we will at max go up to 500 or 1000 generations or until we get the same Average Fitness Coefficient for the last 50 values.

We can have a similar terminating condition for our knapsack problem where we put a cap at 500 generations. The individual with the max fitness coefficient is our high-quality solution. The overall flow looks something like this.

```
def solve_knapsack() -> Individual:
    population = generate_initial_population()

    avg_fitnesses = []

    for _ in range(500):
        avg_fitnesses.append(average_fitness(population))
        population = next_generation(population)

    population = sorted(population, key=lambda i: i.fitness(), reverse=True)
    return population[0]
```

# Run-time Complexity

The run-time complexity of the Genetic Algorithm to generate a high-quality solution for the Knapsack problem is not exponential, but it is polynomial. If we operate with the population size of `P`And iterate till `G` generations, and `F` is the run-time complexity of the fitness function, the overall complexity of the algorithm will be `O(P.G.F)`.

Given that the parameters are known before starting the execution, you can predict the time taken to reach the solution. Thus, we are finding a non-optimal but high-quality solution to the infamous Knapsack problem in [Polynomial Time](https://en.wikipedia.org/wiki/Time_complexity).

# Multi-dimensional optimization problems

The problem we discussed was trivial and was enough for us to understand the core idea of the Genetic Algorithm. The true power of the Genetic Algorithm comes into action when we have multiple parameters, dimensions, and constraints. For example, instead of just weight what if we have size and fragility as two other parameters to consider and we have to find a high-quality solution for the same Knapsack problem.

# The efficiency of Genetic Algorithm

While discussing the process of Genetic Algorithm, we saw that there are multiple parameters that can increase or decrease the efficiency of the algorithm; a few factors include

## Population

The size of the initial population is critical in achieving the high efficiency of a Genetic Algorithm. For our Knapsack problem, we started with a simpler problem with 4 items i.e. search space of a mere 16, and an initial population of 6. We took such a small problem set just to wrap our heads around the idea.

In the real-world, genetic algorithms operate on a much larger search space and typically start with a population size of 500 to 50000. It is observed that if the initial population size does not affect the execution time of the algorithm, it converges faster with a large population than a smaller one.

## Crossover

As we discussed above there are multiple Crossover functions that we can use, like Single Point Crossover, Two-point Crossover, and Multi-point Crossover. Which crossover function would work better for a problem depends totally on the problem at hand. It is generally observed that the two-point crossover results in the fastest convergence.

> You can find the complete source code to the genetic algorithm discussed above in the repository [github/genetic-knapsack](https://github.com/arpitbbhayani/genetic-knapsack)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/pseudorandom-number-generation-lfsr

Pseudorandom number generator uusing LFSR

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Pseudorandom number generator uusing LFSR

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

A few essays back, we saw how pseudorandom numbers are generated using [Cellular Automaton - Rule 30](https://arpitbhayani.me/blogs/rule-30). This essay takes a detailed look into random number generation using [LFSR - Linear Feedback Shift Registers](https://en.wikipedia.org/wiki/Linear-feedback_shift_register). LFSR is widely adopted to generate random numbers on microcontrollers because they are very simple, efficient, and easy to adopt both hardware and software.

# Pseudorandom Number Generators

A [pseudorandom number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) produces numbers that seem aperiodic (random) but are deterministic. It uses a seed value and generates a sequence of random numbers as a function of the current state and some previous states.

These are pseudorandom (not truly random) because the following numbers can be determined algorithmically if the seed value is known. True random numbers are either generated using hardware or from natural phenomena like blood volume pulse, atmospheric pressure, thermal noise, quantum phenomenon, etc.

# What is LFSR

A linear feedback shift register is a collection of bits that shifts when triggered, and the next state is a linear function of its previous state. We use [right-shift](https://en.wikipedia.org/wiki/Logical_shift) (`>>`) as the shift operation and [XOR](https://en.wikipedia.org/wiki/Exclusive_or) (`^`) as the linear function to generate the next state of the register.

![8-bit LFSR](https://user-images.githubusercontent.com/4745789/154759305-23a775cd-f4fe-4aa5-9b4a-e68a7365695f.png)

The LFSR is initiated with a random value called a seed. The next state of the register can be computed deterministically using its previous state and the mentioned operations.

# LFSR in action

A series of latches (bits) connected to the next in line forming a chain is called a register. The diagram below shows an 8-bit shift register that shifts to the right upon an impulse. The rightmost bit that is thrown out during the shift is the output bit.

![8-bit LFSR with a feedback loop](https://user-images.githubusercontent.com/4745789/154759436-7f7d4937-40bb-4a2a-964f-02df14c44d2b.png)

When the shift happens, the leftmost latch gets vacant, and it is either

- filled with the output bit forming a circular shift register
- filled with zero, like a pure right shift operation of a programming language
- filled with a result of some boolean logic on the latches

LFSR that we used to generate pseudorandom numbers goes with the third approach and applies a boolean XOR on a set of chosen latches, called _taps_, and puts the resultant bit in the leftmost latch, creating a Linear Feedback.

## A simple 4-bit LFSR

An LFSR has 3 configurable parameters

- number of bits in the register - `n`
- initial n-bit seed value - `seed`
- position of taps for XOR - `taps`

We build a simple `4`-bit LFSR with a seed value of `0b1001` and tap position of `1`. The output bit will be the rightmost bit of the register, and the next state of the LFSR will be computed as the

- XOR the output bit with the bit in the `1`st position (indexed at 0) from the right
- shift the bits of the register by one to the right
- set the vacant leftmost bit with the output of the XOR operation

![4-bit LFSR](https://user-images.githubusercontent.com/4745789/154893624-b7a8c040-9e26-4f05-b24d-b0b60bcf88a7.png)

After all the above operations are completed, the set of bits in the LFSR becomes the current state and is then used to output the next random bit, thus continuing the cycle.

> The above example is taken from the Computerphile’s [video](https://www.youtube.com/watch?v=Ks1pw1X22y4) on this same topic.

Golang-based implementation of the above LFSR is as shown below. We define a struct holding LFSR with the mentioned three parameters - the number of bits in the register, the seed, and the position of the taps. We define the function named `NextBit` on it that returns a pseudorandom bit generated with the logic mentioned above.

```
type LFSR struct {
	n    uint32
	seed uint32
	taps []uint32
}

func (l *LFSR) NextBit() uint32 {
	seed := l.seed

	// output bit is the rightmost bit
	outputBit := l.seed & 1

	// XOR all the bits present in the tap positions
	for _, tap := range l.taps {
		seed = seed ^ (l.seed >> tap)
	}

	// the new msb is the output of this XOR
	msb := seed & 1

	// rightsift the entire seed
	// and place newly computed msb at
	// the leftmost end
	l.seed = (l.seed >> 1) | (msb << (l.n - 1))

	// return the output bit as the next random bit
	return outputBit
}
```

When we execute the above code with seed `0b1001`, tap position `1`, on a `4`-bit LFSR, we get the following random bits as the output, and with a little bit of pretty-printing, we see

```
lfsr: 1001       output: 1
lfsr: 1100       output: 0
lfsr: 0110       output: 0
lfsr: 1011       output: 1
lfsr: 0101       output: 1
lfsr: 1010       output: 0
lfsr: 1101       output: 1
lfsr: 1110       output: 0
lfsr: 1111       output: 1
lfsr: 0111       output: 1
lfsr: 0011       output: 1
lfsr: 0001       output: 1
lfsr: 1000       output: 0
lfsr: 0100       output: 0
lfsr: 0010       output: 0
lfsr: 1001       output: 1
lfsr: 1100       output: 0
lfsr: 0110       output: 0
lfsr: 1011       output: 1
lfsr: 0101       output: 1
```

The output bits seem random enough, but upon a close inspection, we see that the last 5 output bits are the same as the first 5, and it is because, for the given configuration of seed and tap, the value in the LFSR becomes the same as the seed value `0b1001` after 15 iterations; thus, from the 16th position, we can see the same set of output bits generated.

By carefully selecting the seed and the taps, we can ensure that the cycle is long enough to never repeat in our process’s lifetime. You can find the detailed source code for this LFSR at [github.com/arpitbbhayani/lfsr](https://github.com/arpitbbhayani/lfsr).

## LFSR Bits to Number

Although LFSR generates pseudorandom bits, generating random numbers is fairly simple using it. To generate a `k`-bit random number we need to generate `k` random bits using our routine LFSR and accumulate them in a `k`-bit integer, which becomes our random number.

Golang-based implementation of the above logic can be seen in the function named `NextNumber` defined below.

```
func (l *LFSR) NextNumber(k int) uint32 {
	var n uint32 = 0

	// generate a random bit using LFSR
	// set the random bit in the lsb of a number
	// left-shift the number by 1
	// repeat the flow `k` times for a k-bit number
	for i := 0; i < k; i++ {
		n = n << 1
		n |= l.NextBit()
	}
	return n
}
```

The first ten 8-bit random numbers generated with a seed `0b1001` and tap position `1` using the above logic are

```
154
241
53
226
107
196
215
137
175
19
```

We can see that the numbers are fairly random and are within the limits of an 8-bit integer. You can find the detailed source code for this LFSR at [github.com/arpitbbhayani/lfsr](https://github.com/arpitbbhayani/lfsr).

# Applications of LFSR

LFSRs find their application across a wide spectrum of use cases, given how efficient they generate randomness. Their applications include digital counters, generating pseudorandom numbers, pseudo-noise, scramble radio frequencies, and in general, a stream of bytes. We take a detailed look into LFSR for scrambling.

## Scrambling using LFSR

LFSRs are computationally efficient and deterministic for a seed value, i.e., they generate the same set of numbers in the same order for a seed value, and here’s how they find their application in scrambling and unscrambling a stream of bytes.

The idea of scrambling using LFSR goes like this. We read the raw bytes from the file and pass them to the scrambler function. The function initializes the LFSR with the necessary register length, seed, and tap positions and is kept ready to generate the 8-bit random numbers.

The stream of bytes, from the input file, is then XORed with the random numbers generated from the LFSR such the `i`th byte of the file is XORed with the `i`th 8-bit number generated from the LFSR. Golang-based implementation of `scramble` function is as shown below.

```
func scramble(in []byte, n uint32, seed uint32,
			  taps []uint32) []byte {

  	// initiatializing LFSR with the provided config
	l := lfsr.NewLFSR(n, seed, taps)

  	// creating an output byte slice
	out := make([]byte, len(in))

  	// XOR byte by byte
	for i := range in {
		out[i] = in[i] ^ byte(l.NextNumber(8))
	}

  	// return the output slice
	return out
}
```

The unscrambling process exploits the following property of XOR and the fact that the LFSR will generate the same set of random numbers in the same order for the given configuration.

```
a XOR b XOR a = b
```

So, if we XOR a byte twice with the same number, we get the same byte in return. We apply the same `scramble` function even to unscramble the scrambled data.

```
func unscramble(in []byte, n uint32, seed uint32,
				taps []uint32) []byte {

	// invoking the same scramble function
	return scramble(in, n, seed, taps)
}
```

# Concerns with LFSR

Although LFSRs are very efficient both on the software and hardware sides, there are some concerns about using them.

The number of bits in the LFSR is limited (as configured); the register will repeat the same set after generating a certain set of bits. The length of the cycle depends solely on the seed value and the tap configuration. So, while employing LFSR for random number generation, it is essential to pick a good set of tap positions and seeds to ensure a very long cycle.

Suppose we get hold of a few consecutive random numbers generated from the LFSR. In that case, we can put them in a few linear equations and reach the initial configuration, enabling us to predict the future set of random numbers. Given how vulnerable LFSRs can be, they are not used at places that need cryptographic strength.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/how-indexes-work-on-partitioned-and-sharded-data

How indexes work on partitioned and sharded data?

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# How indexes work on partitioned and sharded data?

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

The previous essay looked at two popular ways to [horizontally partition](https://arpitbhayani.me/blogs/data-partitioning-strategies) the data - Range-based Partitioning and Hash-based Partitioning. In this essay, we will take a detailed look into how we could [index](https://en.wikipedia.org/wiki/Database_index) the partitioned data, allowing us to query the data on secondary attributes quickly.

# Partitioning and Querying

In a [partitioned database](https://arpitbhayani.me/blogs/data-partitioning), the data is split horizontally on the partitioned key. Given that each partition is required to handle a fragment of data, the query that is bound to a single partition is answered very quickly vs the query that requires cross partition execution. But what happens when we want to query the data on any attribute other than the partitioned key; that is where things become very interesting.

Say we have a `movies` collection partitioned on `id` (the movie ID), and each record has the following structure.

```
{
	"id": tt0111161,
	"name": "The Shawshank Redemption",
	"genre": ["Drama"],
	"year": 1994
}
```

Given that the collection is partitioned on `id`, querying a movie by its `id` will be lightning-quick as it would need to hit just one partition to grab the record as determined by the Hash function.

![Pointed Query in Partitioned Database](https://user-images.githubusercontent.com/4745789/152688735-16e15acf-fcee-491c-9b74-965e3590df9c.png)

What if we need to get the list of all the movies belonging to a particular genre? Answering this query is very expensive as we would have to go through every record across all the partitions and see which ones match our criteria, accumulate them, and return them as the response. Given that this process is tedious we leverage indexing to compute the answer quickly.

# Indexing

Indexing is a popular technique to make reads super-efficient, and it does so by creating a query-able mapping between indexed attributes and the identity of the document. An index that maps non-primary key attributes to the record id is called a Secondary Index.

Say, we have the following 6 movie documents, partitioned on `id` (the movie ID) and split across 2 partitions as shown below

```
{ "id": tt0111161, "name": "The Shawshank Redemption", "genre": ["Drama"], "year": 1994 }
{ "id": tt0068646, "name": "The Godfather", "genre": ["Crime", "Drama"], "year": 1972 }
{ "id": tt0071562, "name": "The Godfather: Part II", "genre": ["Crime", "Drama"], "year": 1974 }


{ "id": tt0468569, "name": "The Dark Knight", "genre": ["Action", "Crime", "Drama"], "year": 2008 }
{ "id": tt0050083, "name": "12 Angry Men", "genre": ["Crime", "Drama"], "year": 1957 }
{ "id": tt0108052, "name": "Schindler's List", "genre": ["Biography", "Drama", "History"], "year": 1993 }
```

![movies Partitioned across 2 partitions](https://user-images.githubusercontent.com/4745789/152688989-d70c541f-92c9-4f3f-ad54-740f660f7bd0.png)

To query movies by `genre = Crime`, we will have to index the data on `genre` allowing us to find the relevant documents quickly. Indexes are a little tricky in a partitioned database, and there are two ways to implement them: Local Indexing and Global Indexing. [AWS’s DynamoDB](https://aws.amazon.com/dynamodb/) is a partitioned KV store that supports secondary indexes on non-partitioned attributes, and it supports both of these indexing techniques.

## Local Secondary Index

Local Secondary Indexing creates indexes on a non-partitioned attribute on the data belonging to the partition. Thus, each partition has a secondary index that is built on that data owned by that partition and it knows nothing about the data present in other partitions. Hence, on the example that we have at hand, the Local Secondary Index on attribute `genre` would look like this

![Local Secondary Index - Movies](https://user-images.githubusercontent.com/4745789/152689634-c3235f38-5cf2-4446-af87-ecd58807296d.png)

The key advantage of having a Local Secondary Index is that whenever a write happens on a partition, the index update happens locally without needing any cross partition communication (mostly a network IO). When the data is fetched from a Local Secondary Index, it is fetched from the partition that holds the index data and the entire record; so execution takes a minimal time.

Local Secondary Indexes come in handy when we want to query the data in conjunction with the partitioned key. For example, if the movies were partitioned by `genre` (instead of `id`) and we create an index on `year` it will help us efficiently answer the queries like movies of a particular `genre` released in a specific `year`.

### When Local Secondary Indexes suffer?

Although Local Secondary Indexes are great, they cannot efficiently answer the queries that require cross partition fetch. For example, if we fire the query to get all `Crime` movies through a Local Secondary Index, we will be getting the records that are local to the partition on which the query executes.

But, answering the query to fetch all the movies from the `Crime` genre requires us to go through all the partitions and individually execute the query, then gather (accumulate) the results and return. This is an extremely expensive process that is also prone to network delays, partitioning, and unreliability.

![Scatter Gather Local Secondary Index](https://user-images.githubusercontent.com/4745789/152691045-f9958236-b532-4fed-a5d3-902962edd1b0.png)

We face this limitation because the movies with the `crime` genre are distributed across partitions because there is no way to ensure all movies with the `Crime` genre belong to the same partition when the data partitioning is done on `id`.

Hence, it is very important to structure data partitioning and indexing depending on the queries we want to support ensuring that the queries can be answered through just one partition. To address this problem of being able to query the data on an indexed attribute, we create Global Secondary Indexes.

## Global Secondary Index

Global Secondary Indexes choose not to be local to a partition’s data instead, this indexing technique covers the entire dataset. Global Secondary Index is a kind of re-partitioning of data on a different partition key allowing us to have faster reads and a global view on the indexed attribute.

On the example that we have at hand, Global Secondary Index on `genre` would look like this.

![Global Secondary Index - Movies example](https://user-images.githubusercontent.com/4745789/152696486-33d94f29-6918-48f6-8644-b6ee809a2a81.png)

The key advantage of having a Global Secondary Index is that it allows us to query the data on the indexed attribute globally and not limit ourselves to a fragment of the data. Since it literally re-partitions the data on a different attribute, firing query on the indexed attribute requires it to hit just one partition for execution and thus saving fanning out to multiple partitions.

### When Global Secondary Indexes suffer?

The database takes a performance hit when a Global Secondary Index needs to be synchronously updated as soon as the update happened on the main record, and if the updation happens asynchronously then the readers need to be aware of a possible stale data fetch.

Synchronous updation of a Global Secondary Index is an extremely expensive operation given that every write on primary data will be translated to a number of synchronous updation across partitions for index updation wrapped in a long [Distributed Transaction](https://en.wikipedia.org/wiki/Distributed_transaction) to ensure [Data Consistency](https://en.wikipedia.org/wiki/Data_consistency).

![Global Secondary Index updation](https://user-images.githubusercontent.com/4745789/152697444-4b90d88b-fa18-4456-b8ba-bdde15ddbac4.png)

Hence, in practice, most Global Secondary Indexes are updated asynchronously involving a rick of Replication Lag and stale data reads. The readers from these indexes should be okay with reading stale data and the system being eventually consistent. The delay in propagation could vary from a second to a few minutes, depending on the underlying hardware’s CPU consumption and network capacity.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/some-data-partitioning-strategies-for-distributed-data-stores

Data partitioning strategies for distributed databases

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# Data partitioning strategies for distributed databases

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

[Partitioning](https://arpitbhayani.me/blogs/partitioning) plays a vital role in scaling a database beyond a certain scale of reads and writes. This essay takes a detailed look into the two common approaches to horizontally partition the data.

# Partitioning

A database is partitioned when we split it, logically or physically, into mutually exclusive segments. Each partition of the database is a subset that can operate as a smaller independent database on its own.

## Our goal with partitioning

Our primary goal with partitioning is to spread the data across multiple nodes, each responsible for only a fraction of the data allowing us to dodge the limitations with vertical scaling. A database is uniformly partitioned across 5 data nodes; each node will be roughly responsible for a fifth of the reads and writes hitting the cluster, allowing us to handle a greater load seamlessly.

## What if partitioning is skewed?

Partitioning does help in handling the scale only when the load spreads uniformly. Partitions are skewed when few (hot) partitions are responsible for bulk data or query load. This happens when the partitioning logic does not respect the data and access pattern of the use-case at hand.

![Skewed Partitioning](https://user-images.githubusercontent.com/4745789/150775353-358b6183-30a2-4fb4-8291-e3642c668747.png)

If the partitioning is skewed, the entire architecture will be less effective on performance and cost. Hence, the access and storage pattern of the use-case is heavily considered while deciding on the partitioning attribute, algorithm, and logic.

# Ways of Partitioning Data

## Range-based Partitioning

One of the most popular ways of partitioning data is by assigning a continuous range of data to each partition, making each partition responsible for the assigned fragment. Every partition, thus, knows its boundaries, making it deterministic to find the partition given the partition key.

![Range-based Partitioning](https://user-images.githubusercontent.com/4745789/150777106-4ee22e27-de48-4dda-999e-f1b286a7d5f5.png)

An example of range-based partitioning is splitting a Key-Value store over 5 partitions with each partition responsible for a fragment, defined as,

```
partition 1: [a - e]
partition 2: [f - k]
partition 3: [l - q]
partition 4: [r - v]
partition 5: [w - z]
```

Each partition is thus responsible for the set of keys starting with a specific character. This allows us to define how our entire key-space will be distributed across all partition nodes.

Given that we partition the data to evenly distribute the load across partition nodes, we create the range of the keys that uniformly distributes the load and not the keyspace. Hence in range-based partition, it is not uncommon to see an uneven distribution of key-space. The goal is to optimize the load distribution and not the keyspace.

### When Range-based partitioning fails?

A classic use-case where range-based partitioning fails is when we range-partition the time-series data on timestamp. For example, we create per-day partitions of data coming in from thousands of IoT sensors.

Since IoT sensors will continue to send the latest data, there will always be just one partition that will have to bear the entire ingestion while others will just be sitting idle. When the write-volume for time-series data is very high, it may not be wise to partition the data on time.

## Hash-based Partitioning

Another popular approach for horizontal partitioning is by hashing the partitioned attribute and determining the partition that will own the record. The hashing function used in partitioning is not cryptographically strong but does a good job evenly distributing values across the given range.

Each partition owns a set of hashes. We hash the partitioned attribute when a record needs to be inserted or looked up. A partition that owns the hash will own and store the record. While fetching the record, we first hash the partition key find the owning partition, and then fire the query to get our record from it.

![Hash-based Partitioning](https://user-images.githubusercontent.com/4745789/150777895-b524d8b2-56f3-4a53-bf8b-27f06b824bc6.png)

Hash-based partitioning defers the problem of hot partition to statistics and relies on the randomness of hash-based distribution. But, there is still a slim chance of some partition being hot when many records get hashed to the same partition; this issue is addressed to some extent with the famous [Consistent Hashing](https://arpitbhayani.me/blogs/consistent-hashing).

### When Hash-based partitioning fails?

Hash-based partitioning is a very common technique of data partitioning and is quite prevalent across databases. Although the method is good, it suffers from a few major problems.

Since the record is partitioned on an attribute through a hash function, it is difficult to perform a range query on the data. Since the data is unordered and scattered across all partitions, we will have to visit all the partitions, making the entire process inefficient to perform a range query on key.

Range queries are doable when the required range lies on one partition. This is something leveraged by [Amazon’s DynamoDB](https://aws.amazon.com/dynamodb/) that asks us to specify Partition Key (Hash Key) and Range Key. The data is stored across multiple partitioned and is partitioned by the Hash Key. The records are ordered by Range Key within each partition, allowing us to fire range queries local to one partition.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/data-partitioning

What is Data Partitioning and why it matters at scale

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# What is Data Partitioning and why it matters at scale

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Partitioning plays a vital role in scaling a database beyond a certain scale of reads and writes. In this essay, we take a detailed look into Partitioning basics and understand how it can help us scale our Reads and Writes beyond a single machine.

# What is Partitioning?

A database is partitioned when we split it, logically or physically, into mutually exclusive segments. Each partition of the database is a subset that can operate as a smaller independent database on its own. A database is always deterministically partitioned on a particular attribute like User ID, Time, Location, etc., allowing all records having the same attribute value to reside in the same partition. This will enable us to fire localized queries on a partitioned attribute.

![Partitioning](https://user-images.githubusercontent.com/4745789/149617510-73d710c4-4ff1-4f6c-8ba7-6f8345847248.png)

Say a database has grown to be 100GB big, and we choose to _partition_ it on User ID into 4 partitions. To decide which record goes in which partition, we can use a [Hash Function](https://en.wikipedia.org/wiki/Hash_function) applied on the User ID to map one record to exactly one partition. Hence to trace where a record of a particular user resides, we pass it through the same Hash Function and find the owning partition.

> We will talk about partitioning strategies in detail in future [essays](https://arpitbhayani.me/blogs), so keep an eye.

# Why do we partition?

We need to partition a database for several reasons, but load distribution and availability are the primary reasons. Let’s dive deeper into each and see how partitioning benefits us.

## Load Distribution

A database is partitioned when it needs to handle more reads or writes than one over-scaled database. Our go-to strategy to handle more reads or more writes is to scale the database vertically. Given that vertical scaling has a limit due to hardware constraints, we have to go horizontal and distribute the load across multiple nodes.

### Scaling Reads

By partitioning a database into multiple segments, we get a significant boost in the performance of localized queries. Say we have a database with 100M rows split into 4 partitions with roughly 25M rows each. Now, instead of one database supporting querying over 100M rows, we split the read load across 4 databases allowing us to quickly execute the query and serve the results to the users.

If the read query is localized by partitioned attribute, we need only one (of the four) partitions to execute the query and get the results, thus distributing the read load. For example, in a blogging platform, if our database is partitioned by User ID and we want to find the total number of posts made by a user, this query only needs to be executed on one small partition of data.

![Scaling Reads with Partitioning](https://user-images.githubusercontent.com/4745789/149617513-2dd6bd59-7fea-413a-a73d-313fad080661.png)

Suppose the read queries require us to fetch records from multiple partitions, given that each partition is independent. In that case, we can parallelize the execution and then merge the results before sending them out to the users. In either case, we get a massive performance boost in query execution.

![Scaling Reads with Partitioning - Parallel Reads](https://user-images.githubusercontent.com/4745789/149617508-e62d16d1-bc3e-4aec-9b5c-49785699cff8.png)

### Scaling Writes

In a traditional [Master-Replica setup](https://arpitbhayani.me/blogs/master-replica-replication), there is one Master node that takes in all the write requests, and to scale reads, this Master has a few configured Replicas. To handle more Write operations in such a setup, one approach is to scale the Master node vertically by adding more CPU and RAM. The second approach is to scale it horizontally by adding multiple nodes acting as independent Multiple Master nodes.

Given that vertical scaling has a limit, scaling writes that adding multiple independent Master nodes becomes a go-to strategy beyond a certain scale, where Partitioning plays a key role. In a partitioned setup, since one record can is present on one partition, the total write operations are evenly distributed across all the Master nodes, allowing us to scale beyond a single machine.

![Scaling writes with Partitioning](https://user-images.githubusercontent.com/4745789/149632842-1497874e-13a3-4af2-86fd-096c1eb2e1d7.png)

## Improving Availability

By partitioning a database, we also get a massive improvement in data availability. Since our data is divided across multiple data nodes, even if one of the nodes abruptly crashes and becomes unrecoverable, we only lose a fraction of our data and not the whole of it.

We can further improve the availability of our data by replicating it across multiple secondary data nodes. Thus each partition resides on multiple data nodes, and in case of them crashes, we can fully recover the lost data from the secondary node, giving our fault tolerance a massive boost.

![Partitioning for High Availability](https://user-images.githubusercontent.com/4745789/149632846-d9be03ca-104b-4628-9d5a-b03f9c6ea690.png)

Each record thus belongs to exactly one partition, but the replicated copy of the record can be stored on other data nodes for fault tolerance. These replicated copies are similar to [Read Replicas](https://arpitbhayani.me/blogs/master-replica-replication) that either synchronously or asynchronously follow the primary copy and keep itself updated.

# Types of partitioning

Data can be partitioned in two ways - Horizontal and Vertical. In terms of relational databases, Horizontal Partitioning involves putting different rows into different partitions, and Vertical Partitioning involves putting different columns into separate partitions.

Horizontal partitioning is a very common practice in scaling relational and non-relational databases. It allows us to visit just one partition and get our query answered. It also enables us to split our query load across partitions by making one partition responsible for a particular row/record.

Vertical partitioning is seen in action in [Data Warehouses](https://en.wikipedia.org/wiki/Data_warehouse), where we have to crunch a lot of numbers and fire complex aggregation queries. Vertical partitioning is particularly useful when we are not querying all the columns of a particular record and refer to querying a fewer set of columns in each query.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/leaderless-replication

What is leaderless replication and how it works?

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# What is leaderless replication and how it works?

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Traditional leader-centric replication strategies revolve around the fact that there will be one Master (leader) node that will acknowledge and accept all the writes operations and then replicate the updates across the replicas (read or master). In this essay, we take a look into a different way of replication, called Leaderless Replication, that comes in handy in a multi-master setup that demands strong consistency.

# Leader-centric Replication

In a leader-centric replication, there is a Master node that accepts the writes. Upon applying the writes on its copy of data, the database engine sends out the updates across read-replicas or master nodes. Given that all the writes flow through the Master node, the order of the writes is deterministic, slimming down the chances of having a write conflict.

![Leader-centric Replication](https://user-images.githubusercontent.com/4745789/148632480-29a90496-0d65-41f0-a31a-5fb83cf208d0.png)

Leader-centric replication is not fault-tolerant by design because we lose the write operation when the Master node is down. Leaderless replication addresses this concern and ensures our system can handle Write operations even when a subset of nodes are having an outage.

# Leaderless Replication

Leaderless Replication eradicates the need of having a leader accepting the writes; instead, it leverages quorum to ensure strong consistency across multiple nodes and good tolerance to failures. Here’s how `WRITE` and `READ` operations happen in such a system.

## Write Operation

Say we have a database cluster of `5` nodes (all Masters). In Leaderless Replication, when a client wants to issue a write operation, it fires this operation on all `5` nodes and waits for `ACK` from at least `3` nodes. Once it receives `ACK` from a majority of the nodes, it marks the write as `OK` and returns; otherwise, it marks the operation as `FAIL`.

![Write Operation in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634243-36259bc6-ee6e-4fd2-b399-c475ad7a405d.png)

Every record in the database has a monotonically increasing version number. Every successful write updates this version number allowing us and the system to identify the latest value of the record upon conflict.

Say, when the write operation was triggered, it reached just `4` nodes because the fifth node was down; so, when this node comes back up, it gossips with the other `4` nodes and identifies the writes that it missed and then pulls the necessary updates.

Once the write is `ACK` and confirmed, the nodes gossip internally to propagate the writes to other nodes. There could be a significant delay for the writes to propagate and sync across all nodes; hence we need to ensure that our reading strategy is robust enough to handle this delay while guaranteeing strong consistency.

## Read Operation

Given that there could be a significant delay in the updates to propagate across all `N` nodes, the Read strategy in Leaderless Replication needs to be robust enough.

Like how the client fanned out the write operation to all the nodes, it also fans out the Read operation to all `N` nodes. The client waits to get responses from at least `N/2 + 1` nodes. Upon receiving the responses from a majority of the nodes, it returns the value having the largest version number.

![Read Operation in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634242-adf09717-a063-455f-b5d2-57497e60ca27.png)

Given that we mark a write as `OK` only when at least `N/2 + 1` of them `ACK` it and we return our read-only when we get responses from at least `N/2 + 1` nodes, we ensure that there is at least one node that is sending the latest value of the record.

If we send our read operation to just one node chosen at random, there is a high chance that the value it returns in the response is a stale one, defeating our guarantee of having a strong consistency.

# Generic Leaderless Replication

The Leaderless Replication system we just discussed is specific because it restricts clients to send write to all `N` nodes and wait for `ACK` from at least `N/2 + 1` nodes. This constraint is generalized in real-world with

- `w`: number of nodes that confirm the writes with an `ACK`
- `r`: number of nodes we query for the read
- `n`: total number of nodes

to have a strong consistency i.e. we can expect to get the latest value of any record so long as `w + r > n`, because with this there will be at least one node that has the latest value that will return it. Such reads and writes are called Quorum Reads and Writes.

## Approaches for Leaderless Replication

Now that we understand Leaderless Replication, we look at approaches to implementing it. In the flow discussed above, the approach we saw was the client was sending reads and writes to all the replicated data nodes and, depending on the quorum configuration, decides the correctness. This approach is called _client-driven fan-out_ and is very popular.

![Client Driven Fan-out in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634241-245bb1bd-c766-44d0-8f0a-ebd0618c6628.png)

Another popular approach to implement Leaderless Replication is to have a _Node Coordinator_. The client will make the request to any one node, and it then starts to act as the coordinator for that transaction. This node coordinator will then take care of the fan-out to other nodes and complete the transaction. Upon completion, it returns the response to the client.

![Node Coordinator Driven Fan-out in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634240-0c05e68f-ec35-4ce8-8053-e7334f616dd6.png)

Node coordinator-based replication makes life simpler for clients by offloading all the complications and coordination to the node coordinator. [Apache Cassandra](https://cassandra.apache.org/_/index.html) uses this approach for implementing Leaderless Replication.

# Why Leaderless Replication?

Now that we understand the micro-nuances of Leaderless Replication, let’s address the elephant in the room - Why should we even use Leaderless Replication when it is so complex to set up? There are a couple of strong reasons.

## Strong Consistency

Leaderless Replication is strongly consistent by default. In an over-simplified database cluster of `N` nodes, we are fan-out writes to all `N` nodes and wait for the `ACK` from at least `N/2 + 1` nodes; and while reading, we fan-out reads to all `N` nodes and wait for a response from at least `N/2 + 1` nodes, and then return the value that is most recent among all.

Given that we are playing with the majority here, the set of nodes that handle reads and that handles writes cannot be mutually exclusive and will always have at least `1` overlapping node having the latest value. The assurance of fetching the latest value when Read, no matter how immediate, is how Leaderless Replication ensures a Strong Consistency by design.

> Note: As discussed in previous sections, it is not mandatory to fan-out reads and writes to majority nodes, instead we need a subset of nodes for reads `r`, and another subset for writes `w`, and ensure `r + w > N` for strong consistency.

## Fault tolerance by design

Leaderless Replication is fault-tolerant by design with no [Single Point of Failure](https://en.wikipedia.org/wiki/Single_point_of_failure) in the setup. Given there is no single node acting as a leader, the system allows us to fan-out writes to multiple nodes and wait for an `ACK`; and once we get it from a majority of nodes we are assured that our Write is registered and will never be lost.

Similarly, the reads do not go to just one node; instead, the reads are also fanned-out to all the nodes, and upon receiving the response from a majority of the nodes, we are bound to get the most recent value in one of those responses given there will be an overlap of at least `1` node where the latest write went and the value was read from.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/conflict-resolution

How to resolve conflicts in multi-master setup

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# How to resolve conflicts in multi-master setup

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Every multi-master replication setup comes with a side-effect - Conflicts. Conflict happens when two or more database accepts conflicting updates on the same record. We say that the updates are conflicting when we cannot resolve them to one value deterministically. In the previous essay, we took a detailed look into [Conflict Detection](https://arpitbhayani.me/blogs/conflict-detection), and in this one, we go in-depth to understand ways to handle conflicts (resolve and avoid).

# Conflict Resolution

In the case of a single-master setup, conflicts are avoided by funneling all the writes sequentially. When there are multiple updates on the same field, the last write operation on the record will determine the final value. We can leverage this insight into devising solutions that apply to multi-master setup.

Given that the writes can hit any Master in a multi-master setup, the challenge is to deterministically find the _order_ of the operations to identify which operation came last. So, the approaches for conflict resolution will all revolve around determining or assigning the order to the operations, somehow.

## Globally unique ID to transaction

One possible way to determine the order of the write operations spanning multiple masters is to assign globally unique monotonically increasing IDs to each write operation. When conflict is detected, the write operation having the largest ID overwrites everything else.

![Globally Unique ID](https://user-images.githubusercontent.com/4745789/148541564-aafe6b1d-66e8-434e-8879-85180d09be8f.png)

A globally unique, monotonically increasing ID generator has challenges and is an exhausting problem to solve for scale across distributed nodes. Still, it is essential to consider the idea behind the solution and understand the pattern.

This approach is similar to ordering write on a single master node but without affecting write throughput and concurrent updates. The monotonically increasing globally unique ID gives an implicit ordering to the writes to determine which one came last and hence mimics _Last Write Wins_.

## Precedence of a database

Given that managing an ID generator at scale could be taxing, another possible solution is to assign the order to the master nodes. Upon conflict, the write from the Master having the highest number wins.

![Database Precedence - Conflict Resolution](https://user-images.githubusercontent.com/4745789/148541568-7f1da590-62ad-4764-9995-a3569fc23e0a.png)

This approach is very lightweight, given that assigning orders to master nodes is simple and an infrequent activity. This approach will not guarantee the actual ordering of writes, so it is possible that the actual Last Write got overwritten by some write that happened on Master with the higher ID (precedence).

## Track and Resolve

If, for a use case, it is not possible to resolve the conflicts at the database level, then the best approach in such a scenario is to record the conflict in a data structure designed to preserve all the information. Build a separate job that reads this data structure and resolves the conflict through a custom business logic.

# When to resolve conflict?

There are two possible places where we can inject our conflict resolution logic (handler); the first one is upon writing, and the second one is upon reading.

## On Write

In this approach, as soon as a conflict is detected, the custom conflict resolution logic has triggered that resolve the conflict and make the data consistent. This is a more proactive approach to conflict resolution.

## On Reading

The other approach is to be lazy and resolve conflict when someone tries to read the conflicting data. The custom conflict resolution handler is triggered when the read is triggered on the conflicting data, the database engine realizing it and then invoking the solution handler.

This lazy approach can be seen in action in scenarios where we have to ensure that the [writes are never rejected](https://arpitbhayani.me/blogs/conflict-detection), no matter what.

# Conflict Avoidance

Now that we have gone through these seemingly complex ways of conflict resolution, it seems better to try to avoid conflicts in the first place. This is indeed the simplest and widely adopted strategy for dealing with conflicts.

A possible way to avoid conflict is by adding stickiness in the system, allowing all writes of a particular record to go to a specific Master node, ensuring sequential operations, and simplifying the core requirement of _Last Write Wins_.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/conflict-detection

How to detect conflicts in a multi-master setup

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# How to detect conflicts in a multi-master setup

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Every [multi-master replication](https://arpitbhayani.me/blogs/multi-master-replication) setup comes with a side-effect - Conflicts. Conflict happens when two or more database accepts conflicting updates on the same record. We say that the updates are conflicting when we are unable to resolve them to one value deterministically. In this essay, we talk about conflicts and understand what they are, how to detect them.

# Conflicts

Say we are building an online book store allowing users to purchase books at the click of a button. Like any e-commerce application, even ours has a _Shopping Cart_, which acts as a staging area for everything the user shops for. The user likes a book, adds it to the cart, and proceeds to pay once the shopping is done, proceed to payment. When a user adds a book to the cart, this operation can never be forgotten or rejected - as it will result in loss of revenue and a very poor user experience.

Say the user had books `B1, B2, B3` in the cart already, and this state of the cart is consistent in both the master nodes `Master 1` and `Master 2`. Say the user just added book `B4` to the cart, and this request went to `Master 1` which makes the local state of `Master 1` to be `B1, B2, B3, B4`, while `Master 2` continues to remain at `B1, B2, B3`.

Now, say the user added another book `B5` to the cart, and this request went to `Master 2`. The request reached `Master 2` before the changes from `Master 1` got a chance to be propagated. Since the master node will accept the write the local state of `Master 2` becomes `B1, B2, B3, B5` while state of `Master 1` is `B1, B2, B3, B4`.

![Conflict in real world - shopping cart](https://user-images.githubusercontent.com/4745789/143672208-5be61867-13ba-41dd-bae5-d3f856512d54.png)

Thus, we have two versions of the same shopping cart. When the two master nodes syn, they will detect a conflict that needs to be resolved. The resolution to this conflict is not as simple as replacing one with the other because replacement will lead to the loss of information.

The correct way to address this situation is that we will have to merge the two versions of the cart such similar to the set union. This is a classical case of Conflict Detection and Resolution, and the possible resolution strategy depends on the application and context.

In the above example, we saw a custom conflict resolution strategy in a real use case, but resolving conflicts is not always possible. For example, when two users book the same seat for the same movie, the requests go to two different Masters, and both successfully acknowledge the user confirming the seat. Thus, in this case, the same seat for the same movie show was allotted to two different users. But, since we already sent the confirmation to the user, there is no way to resolve this conflict without giving one of the users an extremely poor experience.

# Conflict Detection

Detecting conflict is simple when we have a single Master node, given that we can serialize all the writes going through it - the second write waits while the first one executes.

But when we have a multi-master setup, all the Master nodes can accept the writes and successfully apply them to their copy of data. When the changes [asynchronously](https://arpitbhayani.me/blogs/replication-strategies) propagate to other Master nodes, the conflict is detected. Given that both the writes requests were successfully accepted and applied, there is no way to communicate the conflict to the client.

![Conflict Detection - Async Replication](https://user-images.githubusercontent.com/4745789/143669401-7dbe6429-a802-496a-83ec-aafc58ca2989.png)

Given that it becomes tough to do something after we detect a conflict when the master-master replication is asynchronous, a possibly easier way out would be to make replication [synchronous](https://arpitbhayani.me/blogs/replication-strategies). In this setup, when one of the Master nodes accepts write, let is successfully apply to its own copy of data and synchronously propagate the write to other Master nodes before responding to the client.

![Conflict Detection - Sync Replication](https://user-images.githubusercontent.com/4745789/143669672-51fcf264-97df-434e-940b-f77e3bfd3f2a.png)

Although this approach solves the problem of detecting conflicts and getting a chance to resolve them, it makes the setup lose its main advantage of allowing multiple masters to accept writes in parallel.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/multi-master-replication

What is multi-master replication and why do we need it?

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# What is multi-master replication and why do we need it?

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

A not-so-common yet super-useful replication strategy is Multi-Master replication - in which multiple nodes in the cluster accept writes, contrary to what is observed in a typical [Master-Replica replication](https://arpitbhayani.me/blogs/master-replica-replication). In this essay, we look at what Multi-Master Replication is, the core challenge it addresses, use-cases we can find this replication in action, and the possible concerns of using it.

# Multi-Master Replication

A Multi-Master setup has multiple nodes of a database cluster accepting the write requests. It typically depends on the client to pick a Master node, or an abstracted proxy may choose one. As soon as a Master node gets the write request, it accepts it and applies the changes. Once the update is made on one Master, it is [propagated](https://arpitbhayani.me/blogs/replication-strategies) to all other Master nodes of the cluster asynchronously, making the data eventually consistent.

Each Master, thus, also acts as a replica (not read-replica) for the other Masters, reading and applying the updates on its copy of data. Each Master node can optionally form a sub-cluster by adding [read-replicas](https://arpitbhayani.me/blogs/master-replica-replication) and scaling the overall incoming reads.

![Multi-Master Replication](https://user-images.githubusercontent.com/4745789/139211714-fc9266bd-ca22-48c4-9095-c6bff0ae99e6.png)

# Why do we need a Multi-Master setup?

An exciting exploration is to find why we would ever need a Multi-Master setup and what kind of problem it would solve for us? Here are three key reasons to have a Multi-Master setup.

## Sharing Load

The most common reason to have a Multi-Master setup is to allow our database cluster to handle more write traffic than a single node. Vertical scaling has theoretical and practical limitations, and the machine can’t go beyond a particular scale.

To truly scale the database cluster, we have to scale the reads by adding read-replicas and having multiple machines that handle writes. Hence, when the writes on a database cluster become a bottleneck, have multiple Master nodes instead of a single one that can take in incoming writes allowing our cluster to share the load and handle multi-fold of write requests.

Typically, clients choose one of the many Master nodes to send their Write requests. These updates are then propagated [asynchronously](https://arpitbhayani.me/blogs/replication-strategies) to other Masters keeping them in sync with the changes and making the system eventually consistent.

## Maintaining a second copy

The second common scenario where Multi-Master comes in handy is when we want to keep a second consistent copy of our Master database, which is also required to accept the write requests. This sounds convoluted, but in the real world, such a requirement is widespread. Let’s go through a few scenarios.

### No SPoF Master

Just like any other node in the database cluster, the Master node can also crash. If the only Master node of the cluster takes all the write requests, crashes, it makes the entire ineffective resulting in a massive downtime. This is a classical case of our Master node becoming the [Single Point of Failure](https://en.wikipedia.org/wiki/Single_point_of_failure).

Given that the failures and crashes are inevitable, it makes sense to have multiple masters running in a cluster and all of them entertaining the write requests. This way, if one of the Master nodes crashes, the other Master can continue to handle the write requests seamlessly, and the cluster will continue to function.

### Lower latencies across geographies

When the clients of your database are spread across geographies, the write latencies shoot up since all the writes across all geographies have to go to this one region where the Master resides.

To keep the write latencies to a minimum across geographies, we set up Multi-Master such that one Master node resides in one region closer to the user. When a client makes the write request, the request can be served from the closest Master giving a great user experience.

### Upgrading the database to a newer version

Every database needs at least a yearly upgrade, and it is never easy to do it on the fly. Before the version is upgraded, every dependent service typically tests its business logic on a newer version. We need to have two databases running during this exercise - one with the older version handling production and the other with the newer version. Both of these databases require to be kept in sync, and both should accept writes. The writes on the newer database will not be as dense as on the production, considering that the service teams will test their workflows on it.

A typical way to facilitate this parallel setup is to have a Multi-Master replication set up between the two databases - one with an older version serving production traffic, the other with a newer version given to application teams to test their workflows. Apart from testing their workflows, the parallel setup also helps incrementally move traffic from old to new versions keeping the blast radius at a bare minimum in case of failure.

The other two similar scenarios where Multi-Master replication comes in handy and are very similar to database upgrade are

- Encrypting the database without taking in a significant downtime
- Downscaling an over-provisioned database without a massive downtime

## Need of a split-brain

The third but particular reason for a Multi-Master setup is where having a split-brain is the necessity and the core of the system. In a general sense, split-brain is considered an erroneous situation that causes mayhem, but it is not a bug but a feature in these scenarios.

A great example of such a system is Collaborative Editing tools like Google Docs, where multiple users on the same document are editing it simultaneously. Each user has its copy of data and edits as if it owns the document wholly. Another example of a split-brain use case has multiple clients using an offline database to work on the same set of values offline and then sync them with a central value store once the internet is back.

# Concerns with Multi-Master

Although Multi-Master replication is excellent and solves a wide range of problems in the real world, it comes with its own set of concerns. Before deciding if you want to have a Multi-Master setup, do consider the following concerns.

## Eventual Consistency

With the replication between Multi-Master being asynchronous, the updates made on one Master will take some time to reflect on the other Masters, making the system eventually consistent. Because of this eventual consistency, a relational database running in Multi-Master mode will lose its ACID guarantees.

## Sluggish Performance

Every update happening on one Master needs to be sent to every other Master node in the cluster. This data movement adds a considerable load on the network bandwidth and could lead to a sluggish network performance at scale.

## Conflict Resolution

The main concern while running a database in Multi-Master mode is Conflict. Since all Master nodes accept writes, there may arise situations where the same entity is updated on multiple Master simultaneously, leading to conflicts while syncing. The way these conflicts are handled depends on the application at hand. Some use cases would suggest discarding the entire sequence of writes, while others would mean the last write wins. It becomes the responsibility of the business logic and the use case to define steps to be taken upon a conflict.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/monotonic-reads

What are monotonic reads, and why do we need them?

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# What are monotonic reads, and why do we need them?

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Asynchronous replication leads to a fascinating situation where it feels like we are going through a wormhole traveling back and forth in time. In this essay, we understand why this happens and the consequences and devise a quick solution to address it.

# Through the wormhole

As per Wikipedia, a [wormhole](https://en.wikipedia.org/wiki/Wormhole) can be visualized as a tunnel with two ends at different points in spacetime (i.e., different locations, different points in time, or both), allowing us to traverse back and forth in time again and again. So, where exactly is a wormhole in the context of a distributed datastore?

Say, we have a distributed KV store having one Master and 2 Replica nodes, and we make three updates on a key `X`, the first update `U1` sets `X` as `1`, the second update `U2` sets it to `2`, while the third update `U3` sets `X` to `3`. Like in a typical Master Replica setup, the writes go to the Master, and they are propagated to Replicas through an Asynchronous replication. The reads are typically sent to any one of the Replicas at random.

The writes are propagated to the Replicas [asynchronously](https://arpitbhayani.me/blogs/replication-strategies), which means both the Replicas will have slight replication lags and say this lag on Replica 1 is of `2 seconds`, and on Replica 2 is `1 second`. As of current time instant, all the three updates `U1`, `U2`, and `U3` have happened on the Master, while only update `U1` has reached Replica 1, and it is lagging behind Replica 2 that saw updates `U1` and `U2`.

![time traveling database - monotonic reads](https://user-images.githubusercontent.com/4745789/135746302-4ff940ba-9ca4-4925-9362-d5fc03f166f6.png)

Say, after making the update `U3` at instant `t`, the User initiates a read that hits Replica 2. Since the update `U3` is yet to reach the Replica 2, it returned `2`, an old value of `X`. This breaks [Read your write consistency](https://arpitbhayani.me/blogs/read-your-write-consistency) and make the user feel that the recent write is lost. Say the user makes another read after this one, which now reaches Replica 1, and since the Replica 1 has just seen the update `U1`, it returns the value `1`, which is even older than the last returned value.

Here we see that after the latest write `U3`, the two successive reads yielded historical values depending on which Replica it hit, giving a feel of traveling back in time. The situation becomes even more interesting when the Replica starts to catch up. Depending on which Replica the read request went to, the User would be oscilating between the old and new values of `X`, giving it a feel of going through the wormhole.

# Monotonic Reads

Monotonic read guarantees users to see value always moving forward in time, no matter how many or how quickly it tries to read the data. It is a weaker guarantee than strong consistency but a stronger one than eventual consistency.

## Achieving Monotonic Reads

The root cause of this seemingly random fetch lies in allowing the read request to hit Replicas with different Replication Lags. For a particular Replica, the writes are always applied in order, moving forward in time. So, a niche solution for this problem is to make the read request of a user sticky to a replica.

![monotonic reads](https://user-images.githubusercontent.com/4745789/135746307-2c3fc584-7154-4d13-96d9-b1a2b29c7d49.png)

Once it is ensured that a particular user’s request only goes to a specific replica, that User will see updates always moving forward in time as the Replica continues to catch up with the Master.

To implement stickiness, the server can pick the Replica using the [hash](https://en.wikipedia.org/wiki/Hash_function) of the User ID instead of picking it randomly. This way, the stickiness between a user and a Replica helping us achieve Monotonic Reads.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/read-your-write-consistency

Read-your-write consistency

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# Read-your-write consistency

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

The most common way to scale the reads hitting a distributed data store is by adding [Read Replicas](https://arpitbhayani.me/blogs/master-replica-replication). These replicas handle all the reads of the systems freeing up the Master to deal with the writes. Although Replicas do help us scale, it brings a new set of problems; and in this essay, we discuss one such issue, called “Read-your-write” consistency, and look at possible solutions.

# The Problem

In a [Master-Replica](https://arpitbhayani.me/blogs/master-replica-replication) setup, the Writes happening on the Master take some time to reach the Replica. This delay in propagation is called Replication Lag. If a client has made a Write and is immediately trying to read the written item, this read may go to the Read Replica that is yet to sync with the Master.

When the client issues the Read on a Replica that has yet to receive the write, it leads to an undesirable behavior wherein the client will see the old value (or null) and think that the write it made was lost.

**Read-Your-Writes consistency** states that the system guarantees that, once an item has been updated, any attempt to read the record by the same client will return the updated value. This consistency makes no promises about other clients getting the updated value immediately after the Write and is meant to reassure the user that their Write is successful.

## Problem in action

To ensure that the issue we are trying to address is not something made up, let’s see what happens across industries when we do not ensure Read-your-write consistency.

Imagine you made a post on a Social Platform, and when you refreshed the page, it threw a 404 error saying Post does not exist, or when you fixed a spelling mistake on the post and refreshed the page, you still see the old text with the same spelling mistake. This inconsistency leads to a terrible user experience.

![Read after Write Fails - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198510-78129b65-5c4c-4d88-a10a-39523d1886d7.png)

In some cases, caching is the root cause; but it is also possible that the Read request for the post was routed to the Read Replica, which was yet to apply the write that happened on the Master, typically due to Replication Lag.

A few more examples of why we need Read-Your-Write consistency:

- Imagine getting a match on Tinder and disappearing upon refresh
- Imagine buying an AAPL Stock and seeing no trace of it on the orders page
- Imagine adding items in your Amazon cart and realizing it empty when placing the order

# Implementing Read-your-write consistency

The primary root cause of not having Read-your-write consistency is Replication Lag. The longer it takes for the write to propagate to the Replica, the longer our end user will see an inconsistent behavior depending on which Read Replica serves the read. So, every single solution revolves all-around reading from a place where Replication Lag is zero. We start dissecting and devising approaches to address this problem.

## Synchronous Replication

Replication Lag exists because the writes are propagated to Replica [asynchronously](https://arpitbhayani.me/blogs/replication-strategies). If the replication is done synchronously, every Write operation on Master is not termed completed unless it is done replicating it on all the Replicas. This way, the Master and the Replica will always remain in sync with ZERO Replication Lag, and no matter to which Replica the read is forwarded, it will always have the latest copy of the data.

![Synchronous Replication - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/128765459-67347320-5b77-4722-884b-015fc1b0c5fb.png)

Synchronous Replication sounds tempting and foolproof approach, but it comes at a massive cost. Synchronous Replication severely affects the write throughput of the database. More than that, write failing on any one of the Replica will choke the entire system. Gaining such Strong Consistency at the expense of write throughput and availability is not a great choice.

## Pinning User to Master

Instead of serving Read requests from the Replica, what if we also serve them from the Master. Forwarding all the read requests to Master defeats the purpose of creating Read Replica - scaling reads. But since we know that the Master will always have the latest copy of the data, can we devise something around it?

Instead of routing all the reads from all the users to the Master, what if we routed reads of the User who recently performed the Write to the Master? This sounds promising and addresses our concern, and this exactly is Pinning the User to the Master.

When a user performs a Write operation, for a specific time window, we pin the User to the Master node, which means every single Read and Write coming from the user will go to the Master, which means the Reads will happen from the data node that always has the latest copy of the data and hence we would achieve Read-your-write consistency.

The time window for pinning should be big enough to ensure that the Writes happened on the Master would have propagated to all the Replicas; this ensures that once the pinning window is over and reads of the user start hitting the Replica, it would continue access the latest copy of the data.

![User PInning to Master - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198508-4c8bd1e4-2336-4063-8ceb-06e675c24554.png)

Although this solves the problem well, it is not optimal when the system is very write-heavy. If in a system most users Write, this would mean the requests most users will go to the Master, most of the time, defeating the purpose of Replica and becoming the bottleneck.

## Fragmented Pinning

Pinning a user to the Master would mean queries, both Read and Write, made by the user will hit the Master for a configured time window. But instead of pinning everything, what if we pick only a few critical reads to hit the Master; this is Fragmented Pinning.

For example, in social media, once the user made or updated a post, pin the user to the Master for 10 minutes such that the request for getting the post goes to the Master; all other reads would continue to hit the Replica.

![Fragmented Pinning - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198504-5b886713-9a16-45ba-9a63-332d19b5894c.png)

By doing fragmented pinning, we ensure that most critical and most likely Read operations, during the pinned window, go to the Master, ensuring that our Master is not overwhelmed even when the system is write-heavy.

## Master Fallback

There is one more way of ensuring Read-your-write consistency, but it works well for a system with a lower Replication Lag, and most queries made on the data store are for keys that exist, i.e., fewer 404s, and the approach is using Master as a fallback.

![Master Fallback - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198497-099bce25-bef1-468e-84b2-69b31e1ae3e0.png)

There is no User Pinning in this approach, and all the Read operations go to the Replica while the Master node only handles Write. The Master and Replica are kept in sync using [asynchronous replication](https://arpitbhayani.me/blogs/replication-strategies). If the Read request that went to the Replica resulted in the 404, i.e., Key Not Found, the application forwards the same query on the Master node and then returns the response.

Since the reads go to the Replica and the Master every time the data is not present in Replica, for this system to be efficient, we need fewer cases where this particular path would be taken, and also the Replication Lag to not inflate much.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/handling-outages-master-replica

How to handle database outages?

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Productionization](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Productionization](/knowledge-base/database-engineering)

# How to handle database outages?

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Master-Replica architecture is one the most common high-level architectural pattern prevalent in distributed systems. We can find it in use across databases, brokers, and custom-built storage engines. In the previous essay, we saw how a [new replica](https://arpitbhayani.me/blogs/new-replica) is set up in a distributed data store and the challenges that come with that. This essay talks about the worse - nodes going down - impact, recovery, and real-world practices.

# Nodes go down, and it is okay

> Anything that can go wrong will go wrong. - [Murphy’s Law](https://en.wikipedia.org/wiki/Murphy%27s_law)

Outages are inevitable; we cannot completely eradicate them, but we can be prepared enough to minimize the impact. One way to make any distributed data store robust is by assuming that the node goes down after executing every operation. Building systems around these aggressive failure scenarios is the easiest way to make any distributed system robust and fault-tolerant.

A few reasons why nodes crash are: system overload, hardware issues, overheating, physical damage, or worse, a natural disaster. Nodes going down is a very common phenomenon, and it happens all the time in massive infrastructures. So, instead of panicking, deal with it by minimizing cascaded failures and speeding up the recovery.

In a Master-Replica setup, there are two kinds of nodes - Master and Replica. Let’s see what happens when the nodes start crashing and recover the system after the crash.

# Replica outage

When Replica is down, the reads going to it fail. If the nodes are within an abstracted cluster, the front-facing proxy can redirect the request to another Replica upon discovering the outage.

Talking about writes during the Replica outage, we know that the Replica does not handle any writes directly from the client. Still, it does pull the updates through the [Replication log](https://arpitbhayani.me/blogs/replication-formats) and re-applies the changes on its own copy of data. So, are these writes affected during the outage?

To keep track of the updates that Replica already applied to its data, it keeps track of the [Sequence Number](https://arpitbhayani.me/blogs/new-replica) of the operation. This **sequence number** is stored on disk and is atomically updated after pulling and applying update operation from the Master.

When the Replica crashes and recovers, it reconnects to the Master, and the hindered replication resumes from the last processed sequence number from what was persisted on the disk. Once the replication resumes, the Replica will fetch all the updates that happened on the Master, apply them on its own copy of data, and eventually catch up.

# Master outage

Master is the most critical component in any distributed data store. When the Master crashes and becomes unavailable, it does not accept any request (read or write) coming to the system. This directly impacts the business, resulting in the loss of new writes coming to the system.

The 3 typical steps in fixing the Master outage are:

- Discover the crash
- Set up the new Master
- Announce the new Master

## Discovering the crashed Master

The first step of fixing the crashed master is to identify that the Master crashed. This discovery step is not just limited to Master but is also applicable to Replica and other relevant nodes in the datastore. So, although we might use the term “Master” while discovering the crash, the process and the flow would be the same for other nodes.

A typical process of detecting a crash is as simple as checking the **heartbeat** of the Master. This means either Master ping the orchestrator that it is alive or the orchestrator checking with the Master if it is healthy. In both cases, the orchestrator would expect that the Master responds within a certain threshold of time, say 30 seconds; and if the Master does not respond, the orchestrator can infer the crash.

![discovering an outage](https://user-images.githubusercontent.com/4745789/132089584-a5177e7a-3104-4f86-8d9d-cbb106b7ce35.png)

In a typical architecture, the orchestrator is a separate node that keeps an eye on all the nodes and repeatedly checks how they are doing. There are [Failure Detection](https://en.wikipedia.org/wiki/Failure_detector) systems that specialize in detecting failures, and one efficient and interesting algorithm for detecting failure is called [Phi φ Accrual Failure Detection](https://arpitbhayani.me/blogs/phi-accrual) that instead of expecting a heartbeat at regular intervals estimates the probability and sets a dynamic threshold.

## Setting up the new Master

When the Master crashes, there are two common ways of setting up the new Master - manual and automated. Picking one over the other depends on the sophistication and maturity of the organization and the infrastructure. Still, it is typically observed that smaller organizations tend to do it manually, while the larges ones have automation in place.

### The manual way

Once the orchestrator detects the Master crash, it raises the alarm to the infrastructure team. Depending on the severity of the incident and the nature of the issue, the infrastructure engineer will either.

- reboot the Master node, or
- restart the Master database process, or
- promote one of the Replica as the new Master

Setting up the Master after the crash manually is common when the team is lean, and the organization is not operating at a massive infrastructure scale. Master crashing is also once in a blue moon event, and setting up complex automation just for one rare occurrence might not be the best use of the engineer’s time.

The entire process is automated once the infrastructure grows massive, and even an outage of a few minutes becomes unacceptable.

### The automated way

In a Master-Replica setup, the automation is mainly around promoting an existing replica as the new Master. So, when the orchestrator detects that the Master crashed, it triggers the [Leader Election](https://en.wikipedia.org/wiki/Leader_election) among the Replicas to elect the new Master. The elected Replica is then promoted, the other Replicas are re-configured to follow this new Master.

![electing the new master](https://user-images.githubusercontent.com/4745789/132089586-d52e558f-10e2-4f10-8807-5920f9117ea8.png)

The new Master is thus ready to accept new incoming writes and reads to the system. This automated way of setting up the new Master is definitely faster, but it requires a lot of sophistication from the infrastructure, the algorithms, and practices before implementation.

## Announcing the new Master

Once the new Master is set up, either manually or elected among the Replicas, this information must be conveyed to the end clients connecting to the Master. So, as the final step of the process, the Clients are re-configured such that they now start sending writes to this new Master.

![announcing the new master](https://user-images.githubusercontent.com/4745789/132089587-dc7e25c6-f43d-4be5-a2c1-e09253ec2205.png)

# The Big Challenge

What if the Master crashed before propagating the changes to the Replica? In such scenarios, if the Replica is promoted as the new Master, this would result in data loss, or conflicts, or split-brain problems if the old Master continues to act as the Master.

## The Passive Master

Data loss or inconsistencies is unacceptable in the real world, so as the solution to this problem, the Master always has a passive standby node. Every time the write happens on the Master, it is [synchronously replicated](https://arpitbhayani.me/blogs/replication-strategies) to this stand by passive node, and asynchronously to configured Replicas.

The write made on the Master is marked as complete only after the updates are synchronously made on this passive Master. This way, the passive Master node is always in sync with the Master. So when the main Master node crashes, we can safely promote this passive Master instead of an existing Replica.

![The Passive Master](https://user-images.githubusercontent.com/4745789/132282367-50feb0be-f952-4bf3-ab62-85ab4f6c86d6.png)

This approach is far better and accurate than running an election across asynchronously replicated Replicas. Still, it incurs a little extra cost of running a parallel Master that will never serve production traffic. But given that it helps in avoiding data loss, this approach is taken by all managed database services.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/new-replica

What happens when we add a new replica?

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Productionization](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Productionization](/knowledge-base/database-engineering)

# What happens when we add a new replica?

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

A distributed data store adds Replica to scale their reads and improve availability. This is the most common and logical way to scale the throughput of a system without massively changing the architecture. In the previous essays, we talked about [Master-Replica Replication](https://arpitbhayani.me/blogs/master-replica-replication), different [Replication Strategies](https://arpitbhayani.me/blogs/replication-strategies), and [Replication Formats](https://arpitbhayani.me/blogs/replication-formats). In this one, we take a look into how these Replicas are set up and understand some quirky nuances.

# Setting up a Replica

In the [Master-Replica setup](https://arpitbhayani.me/blogs/master-replica-replication), Replica is a node that follows the Master. The updates happening on the Master are communicated to the Replica through the process called Replication. The Master publishes the updates on the Replication Log, which are then pulled by the Replica and applied on its own copy of data.

The Replica nodes are read-only in the Master-Replica setup, making this architecture pattern suitable for scale reads and improving availability. The most typical steps taken when a new Replica is set up are

1. Take a point-in-time **snapshot** of the Master data.
2. **Spin up** a Replica node with this snapshot.
3. **Start** the process on the Replica and configure it to follow the Master.
4. The process of **Replication** begins, and the Replica eventually **catches up**.

![The new Replica](https://user-images.githubusercontent.com/4745789/130204028-db759df1-2ea9-4aa5-98f4-6cb3e2b16813.png)

Now that we have talked about the general process of setting up a new Replica, let’s dissect the steps and answer really quirky questions about it.

## Replica keeping track of Replication

Once the Replication is set up between the Replica and the Master, one of the key things to understand is how a Replica keeps track of operations and updates that it has pulled and applied on its own copy of data.

The idea to achieve this is simple. Every update on the Master is associated with a monotonically increasing **sequence number**. Both the Master and the Replica keep track of this sequence number, and it denotes the sequence number of the last operation executed on their respective copy of the data.

Since the Master generates the sequence number, it holds the latest one. The Replica could be a couple of sequence numbers behind, as it needs to pull the updates from the Master, apply the updates, and then update the sequence number. Thus, by tracking the sequence number, the Replica keeps track of the Replication, order of the updates, and understands the Replication lag.

![Sequence Number: Replica](https://user-images.githubusercontent.com/4745789/130345784-8892f5f4-7ed1-4588-bbac-08ce39b7c752.png)

Since the Replica persists the sequence number on disk, even if the server reboots, it can continue to resume the Replication since the reboot.

## Why do we need a point-in-time snapshot?

Now that we know how a Replica keeps track of the replication, we answer an interesting question; do we really need a point-in-time snapshot of Master to create a Replica?

The answer to this situation is simple; it is not mandatory to take a point-in-time snapshot of Master and create Replica out of it. We can also do it on a blank data node with no hiccups at all. The only caveat here is that when we set up Replication on a blank data node, it will have to pull in all the update operations on the Master node and apply them to its own copy of the data.

When a Replica needs to pull in literally every single update operation and apply, it will take a far longer time to catch up with the Master. The Replica will start with an extremely high Replica lag, but eventually, this lag will reduce. Nonetheless, it will take a lot of time to catch the Master, rendering this approach unsuitable.

When the point-in-time snapshot is taken, the sequence number of the Master, at that instant, is also captured. This way, when the Replication is set up on this copy of data, it will have far fewer operations to replicate before it catches up with the Master. Hence, instead of creating Replica from scratch, setting it up from a recent point-in-time snapshot of Master makes the Replica quickly catch up with the Master.

## How does a Replica catch up with the Master?

Replica pulls the replication log from the Master node and applies the changes on its own copy of data. If Replica is already serving live read requests, how it actually catches up with the Master?

The entire Replication process is run by a separate Replication thread that pulls the data from the Replication Log of the Master and applies the updates on its own copy of the data. For Replication to happen, the thread needs to be scheduled on the CPU. The more CPU this Replication thread gets, the faster the replication would happen. This is how the Replica continues to Replicate the updates from the Master while serving the live traffic.

## Is it possible for a Replica never to catch the Master?

If the progress of Replication depends on the CPU cycles that the Replication thread gets, does this mean it is possible for a Replica never to catch the Master?

Yes. It is very much possible for a replica to never catch up with the Master. Since the Replica typically also serves the live read traffic, if some queries are CPU intensive or take massive locks on the tables, there are chances that the Replication thread might get a minimal CPU to continue to replication.

Another popular reason a Replica might never catch up with the Master is when the Master is overwhelmed with many write operations. The influx of write is more than what Replica can process, leading to an ever-increasing Replica lag.

Hence whenever a Replica sees a big enough Replica lag, the remediation is

- to kill read queries that are waiting for a long time, or
- to not let it serve any live traffic for some time, or
- to kill CPU intensive read queries, or
- to kill queries that have taken locks on critical data

We ensure that the Replication thread gets CPU that it deserves to continue the replication by taking some or all of the above actions. Our intention while fighting high replica lag is to reduce somehow the load on the CPU, whatever it takes.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/replication-formats

Understanding replication formats in a master-replica setup

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# Understanding replication formats in a master-replica setup

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

[Master-Replica](https://arpitbhayani.me/blogs/master-replica-replication) architecture is one the most common high-level architectural pattern prevalent in distributed systems. We can find it in use across databases, brokers, and custom-built storage engines.

So, when we are employing a Master-Replica pattern to improve availability, throughput, and fault-tolerance, the big question that pops up is how the writes happening on the Master propagates to the Replica. In this essay, we will talk about exactly this and find out about Replication Formats.

# Write Propagation

Once the write operation is successful on the Master node, the changes must be propagated to all the Replicas. This is done via a log known as Replication Log, Commit Log, or Binary Log. Once the write on the Master is successful, an event is recorded in this Log file; and this file is then **pulled** by the Replicas.

Once the Replica gets this log file, it goes through the events and starts applying the necessary changes on its copy of the data. This way, it continuously follows the changes happening on the Master node. The time elapsed between the write operation on the Master and the operation taking effect to the Replica is called Replication Lag.

Now that we have a solid understanding of Write Propagation, we focus on the primary agenda of this essay, Replication Format, and talk about how these formats share the entire Replication process.

# Replication Formats

Any write operation happening on the Master is logged in the Replication log file as an event. The format in which these events are logged in the Log file is called Replication Format. The two Replication formats that are widely used across distributed data stores are Statement-based and Row-based formats.

## Statement-based Format

In Statement-based format, the Master node records the operation as an event in its log, and when the Replica reads this log, it executes the same operation on its copy of data. This way, the operation on the Master node is executed on the Replica, which keeps it in sync with the Master.

Say the Client fires an operation on the Master to bulk update all the `5` tasks of a user to be marked as `done`. The operation fired by the Client on the Master node would look something like this.

```
  UPDATE tasks SET is_done = true WHERE user_id = 53;
```

When the write operation is completed on the Master, this exact operation is recorded as an event in the Replication Log file. When the Replica reads this log file, the node executes this operation and updates the same `5` tasks on its own copy of the data.

![statement based replication](https://user-images.githubusercontent.com/4745789/129456634-be745df6-541b-4e75-a1e3-4f4f625cc45e.png)

### Advantages

The events recorded in the Replication Log are the actual operations that happen on the Master. Hence, the log files take up the bare minimum storage space required. It will not matter if the operation affects one row or thousand; it will be recorded as one event in the Log file.

Another great benefit of this format is that it can be used to audit the operations on the database because we are recording the operations verbatim in the Log file.

### Disadvantages

The biggest and the most significant disadvantage of the Statement-based format show up when the non-deterministic operations are fired on the Master. The operations such as `UUID()`, `RAND()`, `NOW()`, etc, generate value depending on factors that are not under our control. When these operations fire on the Replica, they might generate values different from the value they yielded on the Master, leading to data corruption.

Since the Replica node, apart from replicating from the Master, is also actively handling requests, some locks might be taken on some of its entities by the executing queries. When a conflicting query is fired from the replication thread, it could result in unpredictable deadlock or stalls.

## Row-based Format

In Row-based format, the Master node logs the updates on the individual data item instead of the operation. So the entry made in the Log file would indicate how the data has changed on the Master. Hence, when the Replica reads this log, it updates its copy of the data by applying the changes on its data items. This way, the operation on the Master node happens on the Replica, and the Replica nodes remain in sync with the Master.

Say the Client fires an operation on the Master to bulk update all the `5` tasks of a user to be marked as `done`. The operation fired by the Client on the Master node would look something like this.

```
  UPDATE tasks SET is_done = true WHERE user_id = 53;
```

In the row-based format, instead of recording the operation, the Master node records the updates made on the data items. Since the operation in question updated `5` rows, the events recorded in the Replication Log file would contain `5` entries, one for each data item changed and would look something similar to

```
tasks:121 is_done=true
tasks:142 is_done=true
tasks:643 is_done=true
tasks:713 is_done=true
tasks:862 is_done=true
```

Hence, one operation on the Master is fanned out as series of updates on the data items and is consumed by the Replica. The Replica then reads these events and applies the changes on its copy of the data.

![row based replication](https://user-images.githubusercontent.com/4745789/129456632-ad7b67ae-7ff0-4d35-97b0-0ea6d6a3bd87.png)

### Advantages

The major advantage of the Row-based format is that all the changes can be safely and predictably applied on the Replica. This approach is safe even with the non-deterministic operations because what gets written is the computed value.

### Disadvantages

When the Master node completes the operation, it takes the lock on the Replication Log file and then records the events. Since the number of events recorded in the log file will be the number of data items changed, the lock taken by the Master node will be longer, choking the throughput.

Another obvious disadvantage in this approach is fan-out. If an operation changes 5000 data items, it will result in 5000 events in the Log file, and if such operations are frequent, this will make Logfile take a lot of storage space.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/replication-strategies

Two ways to replicate data across database cluster

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# Two ways to replicate data across database cluster

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

In a distributed system, when replication is set up between data nodes, there are typically three replication strategies - Synchronous, Asynchronous, and Semi-synchronous. Depending on the criticality of data, its consistency, and the use-case at hand, the system chooses to apply one over another. In this essay, we take a quick yet verbose look into these strategies and understand their implications.

Before we jump into the replication strategies, let’s first understand the need for them. When the data is replicated, multiple copies of the same data are created and placed on multiple machines (nodes). A system replicates the data to

- improve availability, or
- prepare for disaster recovery, or
- improve performance by leveraging parallel reads across replicated data, or
- keep the data geographically close to the user, e.g., CDN

The Replication comes into action when the Client initiates the write on the Master node. Once the Master updates its copy of the data, the replication strategy dictates how the data would be replicated across the Replicas.

## Synchronous Replication

In Synchronous Replication, once the Master node updates its own copy of the data, it initiates the write operation on its replicas. Once the Replicas receive the update, they apply the change on their copy of data and then send the confirmation to the Master. Once the Master receives the confirmation from all the Replicas, it responds to the client and completes the operation.

![synchronous replication](https://user-images.githubusercontent.com/4745789/128765459-67347320-5b77-4722-884b-015fc1b0c5fb.png)

If there is more than one Replica in the setup, the Master node can propagate the write sequentially or parallelly. Still, in either case, it will continue to wait until it gets a confirmation, which will continue to keep the client blocked. Thus having a large number of Replicas means a longer block for the Client, affecting its throughput.

Synchronous Replication ensures that the Replicas are always in sync and consistent with the Master; hence, this setup is fault-tolerant by default. Even if the Master crashes, the entire data is still available on the Replicas, so the system can easily promote any one of the Replicas as the new Master and continue to function as usual.

A major disadvantage of this strategy is that the Client and the Master can remain blocked if a Replica becomes non-responsive due to a crash or network partition. Due to the strong consistency check, the Master will continue to block all the writes until the affected Replica becomes available again, thus bringing the entire system to a halt.

## Asynchronous Replication

In Asynchronous Replication, once the Master node updates its own copy of the data, it immediately completes the operation by responding to the Client. It does not wait for the changes to be propagated to the Replicas, thus minimizing the block for the Client and maximizing the throughput.

The Master, after responding to the client, asynchronously propagates the changes to the Replicas, allowing them to catch up eventually. This replication strategy is most common and is the default configuration of most distributed data stores out there.

![asynchronous replication](https://user-images.githubusercontent.com/4745789/128765466-944bf36e-6817-4cf3-9ea4-0ffa724f0d58.png)

The key advantage of using a fully Asynchronous Replication is that the client will be blocked only for the duration that the write happens on the Master, post which the Client can continue to function as before, thus elevating the system’s throughput.

One major disadvantage of having a fully Asynchronous Replication is the possibility of data loss. What if the write happened on the Master node, and it crashed before the changes could propagate to any of the Replicas. The changes in data that are not propagated are lost permanently, defeating durability. Although Durability is the most important property of any data store,

Asynchronous Replication is the default strategy for most data stores because it maximizes the throughput. The third type of replication strategy addresses durability without severely affecting throughput, and it is called Semi-synchronous Replication.

## Semi-synchronous Replication

In Semi-synchronous Replication, which sits right between the Synchronous and Asynchronous Replication strategies, once the Master node updates its own copy of the data, it synchronously replicates the data to a subset of Replicas and asynchronously to others.

![semi-synchronous replication](https://user-images.githubusercontent.com/4745789/128833772-d0bbae7d-5e00-4771-90e5-996326affb60.png)

The Semi-synchronous Replication thus addresses the durability of data, in case of Master crash, at the cost of degrading the Client’s throughput by a marginal factor.

Most of the distributed data stores available have configurable replication strategies. Depending on the problem at hand and the criticality of the data, we can choose one over the other.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/master-replica-replication

What is a master-replica setup and why it matters?

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Distributed Databases](/knowledge-base/database-engineering)

# What is a master-replica setup and why it matters?

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Master-Replica architecture is one the most common high-level architectural pattern prevalent in distributed systems. We can find it in use across databases, brokers, and custom-built storage engines. In this essay, we talk about everything we should know about the Master-Replica replication pattern.

A system that adheres to the Master-Replica replication architecture contains multiple nodes and each node, called Replica, holds an identical copy of the entire data. Thus if there are N nodes in the system, there will be N copies of the data.

![Master-Replica Replication](https://user-images.githubusercontent.com/4745789/128564165-92d3413a-a329-4456-b055-177ed83e989a.png)

# Scaling Reads

With N nodes capable of serving the data, we easily scale up the reads by a factor of N. Hence, this pattern is commonly put in place to amplify and scale the reads that the system can handle.

# Handling Writes

With N nodes that hold and own the data, the writes become tricky to handle. In the Master-Replica setup, the writes go to one of the pre-decided nodes that act as the Master. This Master node is responsible for taking up all the writes that happen in the system.

Master is not any special node; rather, it is just one of the Replicas with this added responsibility. Thus in the system of N nodes, 1 node is the Master that takes in all the writes, while the other N - 1 node caters to the read requests coming from the clients.

# Write Propagation

Once the write operation is successful on the Master node, the changes are propagated to all the Replicas through Replication Log (Commit Log, Bin Log, etc.), letting the system eventually catch up.

The time elapsed between the write operation on the Master and the operation propagating to the Replica is called Replication Lag. This is one of the core metrics that is observed 100% of the time.

# What happens when the Master goes down?

Since the Master node takes in all the write operations, it going down is a massive event. The write operation that happens when the Master is facing an outage results in an error.

When the system detects such an outage, it tries to auto-recover by promoting one active Replica as the new Master by running a Leader Election algorithm. All the healthy Replicas participate in this election and, through a consensus, decide the new Master.

Once the new Master is elected, the system starts accepting and processing the writes again.

# Master-Replica in action

This is a widespread pattern that we can find across almost all the databases and distributed systems. Some of the most common examples are:

- Relational databases like MySQL, PostgreSQL, Oracle, etc.
- Non-relational databases like MongoDB, Redis, etc.
- Distributed brokers like Kafka, etc.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/durability

Understanding Durability in ACID

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

# Understanding Durability in ACID

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

After discussing the “[A](https://arpitbhayani.me/blogs/atomicity)”, the “[C](https://arpitbhayani.me/blogs/consistency)”, and the “[I](https://arpitbhayani.me/blogs/isolation)”, it is time to take a look at the “D” of ACID - [Durability](<https://en.wikipedia.org/wiki/Durability_(database_systems)>).

Durability seems to be a taken-for-granted requirement, but to be honest, it is the most important one. Let’s deep dive and find why it is so important? How do databases achieve durability in the midst of thousands of concurrent transactions? And how to achieve durability in a distributed setting?

# What is Durability?

In the context of Database, Durability ensures that once the transactions commit, the changes survive any outages, crashes, and failures, which means any writes that have gone through as part of the successful transaction should never abruptly vanish.

This is exactly why Durability is one of the essential qualities of any database, as it ensures zero data loss of any transactional data under any circumstance.

A typical example of this is your purchase order placed on Amazon, which should continue to exist and remain unaffected even after their database faced an outage. So, to ensure something outlives a crash, it has to be stored in non-volatile storage like a Disk; and this forms the core idea of durability.

# How do databases achieve durability?

The most fundamental way to achieve durability is by using a fast transactional log. The changes to be made on the actual data are first flushed on a separate transactional log, and then the actual update is made.

This flushed transactional log enables us to reprocess and replay the transaction during database reboot and reconstruct the system’s state to the one that it was in right before the failure occurred - typically the last consistent state of the database. The write to a transaction log is made fast by keeping the file append-only and thus minimizing the disk seeks.

![Durability in ACID](https://user-images.githubusercontent.com/4745789/126114187-0febc1ad-e35f-4d49-991c-8a5d8a0d9221.png)

# Durability in a distributed setting

If the database is distributed, it supports Distributed Transactions, ensuring durability becomes even more important and trickier to handle. In such a setting, the participating database servers coordinate before the commit using a Two-Phase Commit Protocol.

The distributed computation is converged into a step-by-step process where the coordinator communicates the commit to all the participants, waits for all acknowledgments, and then further communicates the commit or rollback. This entire process is split into two phases - Prepare and Commit.

# References

- [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)
- [Durability - Wikipedia](<https://en.wikipedia.org/wiki/Durability_(database_systems)>)
- [Two-phase commit protocol](https://en.wikipedia.org/wiki/Two-phase_commit_protocol)
- [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/isolation

Understanding Isolation in ACID

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

# Understanding Isolation in ACID

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

After talking about the “A” and the “C” in ACID, let’s talk about the “I” in ACID - Isolation. In this one, we do a micro-dive into Isolation in the context of database. We will take a detailed look into Isolation, understand its importance, functioning, and how the database implements it.

# What is Isolation?

Isolation is the ability of the database to concurrently process multiple transactions in a way that changes made in one does not affect the other. A simple analogy is how we have to make our data structures and variables thread-safe in a multi-threaded (concurrent) environment.

And similar to how we use Mutex and Semaphores to protect variables, the database uses locks (shared and exclusive) to protect transactions from one another.

![https://user-images.githubusercontent.com/4745789/124764636-caf07280-df52-11eb-8d6b-d9d316d31102.png](https://user-images.githubusercontent.com/4745789/124764636-caf07280-df52-11eb-8d6b-d9d316d31102.png)

# Why is Isolation important?

Isolation is one of the most important properties of any database engine, the absence of which directly impacts the integrity of the data.

## Example 1: Cowin Portal

When 500 slots open for a hospital, the system has to ensure that a max of 500 people can book their slots.

## Example 2: Flash Sale

When Xiaomi conducts a flash sale with 100k units, the system has to ensure that orders of a max of 100k units are placed.

## Example 3: Flight Booking

If a flight has a seating capacity of 130, the airlines cannot have a system that allows ticket booking of more than that.

## Example 4: Money transfers

When two or more transfers happen on the same account simultaneously, the system has to ensure that the end state is consistent with no mismatch of the amount. Sum of total money across all the parties to remain constant.

The isolation property of a database engine allows the system to put these checks on the database, which ensures that the data never goes into an inconsistent state even when hundreds of transactions are executing concurrently.

# How is isolation implemented?

A transaction before altering any row takes a lock (shared or exclusive) on that row, disallowing any other transaction to act on it. The other transactions might have to wait until the first one either commits or rollbacks.

The granularity and the scope of locking depend on the isolation level configured. Every database engine supports multiple Isolation levels, which determines how stringent the locking is. The 4 isolation levels are

- Serializable
- Repeatable reads
- Read committed
- Read uncommitted

We will discuss Isolation Levels in detail in some other essay.

# References

- [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)
- [Isolation - Wikipedia](<https://en.wikipedia.org/wiki/Isolation_(database_systems)>)
- [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)
- [ACID properties of transactions](https://www.ibm.com/docs/en/cics-ts/5.4?topic=processing-acid-properties-transactions)
- [ACID Compliance: What It Means and Why You Should Care](https://mariadb.com/resources/blog/acid-compliance-what-it-means-and-why-you-should-care/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/atomicity

Understanding Atomicity in ACID

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

# Understanding Atomicity in ACID

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

In this short essay, we dive deep and understand the “A” of ACID - Atomicity.

In this quick read, we will take a detailed look into [Atomicity](<https://en.wikipedia.org/wiki/Atomicity_(database_systems)>), understand its importance, and learn about implementing it at various levels.

# What is atomicity?

A single database transaction often contains multiple statements to be executed on the database. In Relational Databases, these are usually multiple SQL statements, while in the case of non-Relational Databases, these could be multiple database commands.

Atomicity in ACID mandates that each transaction should be treated as a single unit of execution, which means either all the statements/commands of that transaction are executed, or none of them are.

At the end of the successful transaction or after a failure while applying the transaction, the database should never be in a state where only a subset of statements/commands is applied.

An atomic system thus guarantees atomicity in every situation, including successful completion of transactions or after power failures, errors, and crashes.

![Atomicity ACID](https://user-images.githubusercontent.com/4745789/124223798-0e497c00-db22-11eb-868d-8faefc44361c.png)

A great example of seeing why it is critical to have atomicity is Money Transfers.

Imagine transferring money from bank account A to B. The transaction involves subtracting balance from A and adding balance to B. If any of these changes are partially applied to the database, it will lead to money either not debited or credited, depending on when it failed.

# How is atomicity implemented?

## Atomicity in Databases

Most databases implement Atomicty using logging; the engine logs all the changes and notes when the transaction started and finished. Depending on the final state of the transactions, the changes are either applied or dropped.

Atomicity can also be implemented by keeping a copy of the data before starting the transaction and using it during rollbacks.

## Atomicity in File Systems

At the file system level, atomicity is attained by atomically opening and locking the file using system calls: open and flock. We can choose to lock the file in either Shared or Exclusive mode.

## Atomicity at Hardware Level

At the hardware level, atomicity is implemented through instructions such as Test-and-set, Fetch-and-add, Compare-and-swap.

## Atomicity in Business Logic

The construct of atomicity can be implemented at a high-level language or business logic by burrowing the concept of atomic instructions; for example, you can use compare and swap to update the value of a variable shared across threads concurrently.

Atomicity is not just restricted to Databases; it is a notion that can be applied to any system out there.

✨ Next up is “C” in ACID - Consistency. Stay tuned.

# References

- [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)
- [Atomicity - Wikipedia](<https://en.wikipedia.org/wiki/Atomicity_(database_systems)>)
- [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)
- [ACID properties of transactions](https://www.ibm.com/docs/en/cics-ts/5.4?topic=processing-acid-properties-transactions)
- [ACID Compliance: What It Means and Why You Should Care](https://mariadb.com/resources/blog/acid-compliance-what-it-means-and-why-you-should-care/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/consistency

Understanding Consistency in ACID

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

# Understanding Consistency in ACID

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

In this short essay, we dive deep and understand the “C” in ACID - Consistency.

In this quick read, we will take a detailed look into [Consistency](<https://en.wikipedia.org/wiki/Consistency_(database_systems)>), understand its importance, functioning, and how the database implements it.

# What is Consistency?

In the context of databases, Consistency is Correctness, which means that under no circumstance will the data lose its correctness.

Database systems allow us to define rules that the data residing in our database are mandated to adhere to. Few handy rules could be

- balance of an account should never be negative
- no orphan mapping: there should not be any mapping of a person whose entry from the database is deleted.
- no orphan comment: there should not be any comment in the database that does not belong to an existing blog.

These rules can be defined on a database using Constraints, [Cascades](https://en.wikipedia.org/wiki/Foreign_key#CASCADE), and Triggers; for example, [Foreign Key constraints](https://en.wikipedia.org/wiki/Foreign_key), [Check constraints](https://en.wikipedia.org/wiki/Check_constraint), On Delete Cascades, On Update Cascades, etc.

![Consistency ACID Database](https://user-images.githubusercontent.com/4745789/124226533-e7417900-db26-11eb-8e88-1c50a9391c44.png)

### Role of the database engine in ensuring Consistency

An ACID-compliant database engine has to ensure that the data residing in the database continues to adhere to all the configured rules. Thus, even while executing thousands of concurrent transactions, the database always moves from one consistent state to another.

### What happens when the database discovers a violation?

Database Engine rollbacks the changes, which ensures that the database is reverted to a previous consistent state.

### What happens when the database does not find any violation?

Database Engine will continue to apply the changes, and once the transaction is marked successful, this state of the database becomes the newer consistent state.

# Why is consistency important?

The answer is very relatable. Would you ever want your account to have a negative balance? No. This is thus defined as a rule that the database engine would have to enforce while applying any change to the data.

# How does the database ensure Consistency?

Integrity constraints are checked when the changes are being applied to the data.

Cascade operations are performed synchronously along with the transaction. This means that the transaction is not complete until the primary set of queries, along with all the eligible cascades, are applied. Most database engines also provide a way to make them asynchronous, allowing us to keep our transactions leaner.

✨ Next up is “I” in ACID - Isolation. Stay tuned.

# References

- [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)
- [Consistency](<https://en.wikipedia.org/wiki/Consistency_(database_systems)>)
- [Foreign Key Constraints](https://en.wikipedia.org/wiki/Foreign_key)
- [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)
- [ACID properties of transactions](https://www.ibm.com/docs/en/cics-ts/5.4?topic=processing-acid-properties-transactions)
- [ACID Compliance: What It Means and Why You Should Care](https://mariadb.com/resources/blog/acid-compliance-what-it-means-and-why-you-should-care/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/architectures-in-distributed-systems

Common architectural patterns in distributed systems

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [System Design](/knowledge-base/system-design)
- [Foundational Concepts](/knowledge-base/system-design)

- [KB](/knowledge-base)
- [Syst...](/knowledge-base/system-design)
- [Foundational Concepts](/knowledge-base/system-design)

# Common architectural patterns in distributed systems

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

While designing a Distributed System, it is essential to pick the right kind of architecture. Usually, architectures are evolving, but picking the right one at the inception could make your system thrive.

So, here are 4 common architectures in Distributed Systems

## Client-server

To render some information to the end-user, the client contacts the server (holding the data) with a request; the server sends back the data requested. The client is smart enough to understand how to request the data, post-process it, format it, and then serve it to the end-user.

This kind of architecture is not common in our day-to-day web applications. Still, it is prevalent when multiple services share a common database (server) and request data directly from the database.

## 3-tier

This is one of the most widely used topologies out there. Unlike client-server architecture, the clients in a 3-tier architecture are not smart and are stateless. This architecture introduces a middle layer holding the business logic. The client talks to the business layer, and this business layer talk to the server (holding the data).

Most of the web applications are 3-tier applications where your client (browser) talks to the business layer (webserver), which in turn queries the server (database) for the data. The business layer processes and formats the data (optional) and sends it back to the client. The client does not know how the data is being fetched from the server, making it stateless.

## n-tier

As an extension to the 3-tier architecture, the n-tier application is where your middle layer (business layer) talks to another service to get information. This is typically seen when there is multiple independent business logic in the system.

A classic example of an n-tier architecture is Microservices based architecture. Each service is responsible for its information, and the service communicates with other services to get the required data. Thus, a 3-tier application typically evolves into an n-tier application.

## Peer-to-peer

Peer-to-peer architecture is typically a decentralized system wherein no special machines hold all the responsibilities; instead, the responsibility is split across all the machines equally. The peers act as both clients and servers, and they communicate with each other to serve the request.

A couple of popular examples of P2P architecture are BitTorrent and Bitcoin networks. The n-tier architecture can optionally evolve into a P2P, but this evolution is not that popular. Usually, going P2P is a choice that is made during the inception of the service.

# References

- [Distributed Systems - Wikipedia](https://en.wikipedia.org/wiki/Distributed_computing)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/mistaken-beliefs-of-distributed-systems

Assumptions people make while designing distributed systems

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [System Design](/knowledge-base/system-design)
- [Foundational Concepts](/knowledge-base/system-design)

- [KB](/knowledge-base)
- [Syst...](/knowledge-base/system-design)
- [Foundational Concepts](/knowledge-base/system-design)

# Assumptions people make while designing distributed systems

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

The only way to infinitely scale your system is by making it distributed, which means adding more servers to serve your requests, more nodes to perform computations in parallel, and more nodes to store your partitioned data. But while building such a complex system, we tend to assume a few things to be true, which, in reality, are definitely not true.

These mistaken beliefs were documented by [L Peter Deutsch](https://en.wikipedia.org/wiki/L._Peter_Deutsch) and others at Sun Microsystems, and it describes a set of false assumptions that programmers new to distributed applications invariably make.

## Myth 1: The network is reliable;

No. The network is not reliable. There are packet drops, connection interruptions, and data corruptions when they are transferred over the wire. In addition, there are network outages, router restarts, and switch failures to make the matter worse. Such an unreliable network has to be considered while designing a robust Distributed System.

## Myth 2: Latency is zero;

Network latency is real, and we should not assume that everything happens instantaneously. For every 10 meters of fiber optic wire, we add 3 nanoseconds to the network latency. Now imagine your data moving across the transatlantic communications cable. This is why we keep components closer wherever possible and have to handle out-of-order messages.

## Myth 3: Bandwidth is infinite;

The bandwidth is not infinite; neither of your machine, or the server, or the wire over which the communication is happening. Hence we should always measure the number of packets (bytes) of data transferred in and out of your systems. When unregulated, this results in a massive bottleneck, and if untracked, it becomes near impossible to spot them.

## Myth 4: The network is secure;

We put our system in a terrible shape when we assume that the data flowing across the network is secure. Many malicious users are constantly trying to sniff every packet over the wire and de-code what is being communicated. So, ensure that your data is encrypted when at rest and also in transit.

## Myth 5: Topology doesn’t change;

Network topology changes due to software or hardware failures. When the topology changes, you might see a sudden deviation in latency and packet transfer times. So, these metrics need to be monitored for any anomalous behavior, and our systems would be ready to embrace this change.

## Myth 6: There is one administrator;

There is one internet, and everyone is competing for the same resources (optic cables and other communication channels). So, when building a super-critical Distributed system, you need to know which path your packets are following to avoid high-traffic competing and congested areas.

## Myth 7: Transport cost is zero;

There is a hidden cost of hardware, software, and maintenance that we all bear when using a distributed system. For example, if we use a public cloud-like AWS, then the data transfer cost is real. This cost looks near zero from a bird’s eye view, but it becomes significant when operating at scale.

## Myth 8: The network is homogeneous.

The network is not homogeneous, and your packets travel to all sorts of communication channels like optic cables, 4G bands, 3G bands, and even 2G bands before reaching the user’s device. This is also true when the packets move within your VPC through different types of connecting wires and network cards. When there is a lot of heterogeneity in the network, it becomes harder to find the bottleneck; hence having a setup that gives us enough transparency is the key to a good Distributed System design.

# References

- [Fallacies of distributed computing](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing)
- [The Eight Fallacies of Distributed Computing](https://www.youtube.com/watch?v=JG2ESDGwHHY)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/fork-bomb

Fork Bomb

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

# Fork Bomb

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

In this essay, we explore a simple yet effective [DoS attack](https://en.wikipedia.org/wiki/Denial-of-service_attack) called [Fork Bomb](https://en.wikipedia.org/wiki/Fork_bomb), also called Rabbit Virus. This attack forks out processes infinitely, starving them for any resources.

Online Coding Platforms and Code Evaluators are susceptible to this attack as they accept raw code from the user and execute it. So, if you are building one, do ensure you are protected against it and infinite loops. We will also discuss how to prevent and defend against Fork Bomb in the final section of this essay.

# How do they work?

Fork bombs can be summarized in just three words: _Fork until possible_. When executed, fork bombs continuously fork out non-terminating processes, demanding machine resources like CPU (mostly) and memory.

![Fork Bomb](https://user-images.githubusercontent.com/4745789/121252662-e752ae00-c8c5-11eb-9524-a1c7d4fc24fc.png)

With so many processes competing for the CPU and other resources (if provisioned), the scheduler and CPU are put under tremendous load. After a specific limit, the entire system stalls.

# Implementing Fork Bombs

Before we take a look at how to prevent or stop a Fork Bomb, let’s look at something more interesting - how to implement a Fork Bomb?

A quick detour, let’s see what `fork` does: Upon every invocation, the forked child process is an exact duplicate of the parent process except for a [few details](https://man7.org/linux/man-pages/man2/fork.2.html), but nonetheless what matters to us is that it runs the exact same code as the parent.

## C implementation

A simple C implementation of a Fork Bomb could be, to fork child processes within an infinite for loop, resulting in exponential forking of child processes.

```
#include <unistd.h>
int main(void) {
    for (;;) {
        fork();
    }
}
```

With the `fork` being invoked inside the infinite for loop, every single child process and the parent process will continue to remain stuck in the infinite loop while continuously forking out more and more child processes that execute the same code and stuck in the same loop; and thus resulting in exponential child forks.

These child processes start consuming the resources and blocking the legitimate programs. This prevents the creation of any new processes. This also freezes the process that responds to Keystrokes, putting the entire system to a standstill.

## Bash implementation

There is a very famous Fork Bomb implementation in Bash, and the code that does this has no alphabets or numbers in it, just pure symbols.

```
:(){ :|:& };:
```

Although the shell statement looks gibberish, it is effortless to understand. In the statement above, we are defining a function named `:` having body `:|:&` and at the end invoking it using the name `:`, just like any usual shell function.

As part of the function body, we are again invoking the same function `:` and piping its output to the input of a background process executing another instance of `:`. This way, we are recursively invoking the same function (command) and stalling it by creating a pipe between the two.

A cleaner way to redefine this very implementation of Fork Bomb would be

```
bomb() {
    bomb| bomb&
};bomb
```

# How to prevent them?

To protect our system against Fork Bombs, we can cap the processes owned by a certain user, thus blocking process creation at that cap.

Using the \*nix utility called `ulimit`, we can set the maximum number of processes that a user can execute in the system, using the flag `-u`. By setting this value to an appropriate (lower) value, we can cap the process creation for a user, ensuring we can never be fork bombed by that user.

# References

- [Fork Bomb](https://en.wikipedia.org/wiki/Fork_bomb)
- [ulimit - Man Page](https://linuxcommand.org/lc3_man_pages/ulimith.html)
- [Understanding Bash Fork Bomb](https://www.cyberciti.biz/faq/understanding-bash-fork-bomb/)
- [Preventing Fork Bombs on Linux](https://resources.cs.rutgers.edu/docs/preventing-fork-bomb-on-linux/)
- [Fork bomb attack (Rabbit virus) - Imperva](https://www.imperva.com/learn/ddos/fork-bomb/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/chained-operators-python

How Python evaluates chained comparison operators

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Language Internals](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

- [KB](/knowledge-base)
- [Lang...](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

# How Python evaluates chained comparison operators

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Python supports chaining of comparison operators, which means if we wanted to find out if `b` lies between `a` and `c` we can do `a < b < c`, making code super-intuitive. Python evaluates such expressions like how we do in mathematics. which means `a < b < c` is evaluated as `(a < b) and (b < c)`. C language on the other hand, evaluates `a < b < c` as `((a < b) < c)`.

Depending on how we evaluate such expressions, the final evaluated changes. So, in python, if we evaluate `-3 < -2 < -1`, we get `True` and if evaluate `3 > 2 == 1` we get `False`.

```
>>> -3 < -2 < -1
True
>>> 3 > 2 == 1
False
```

But on the other hand, if we evaluate this very expression in C language the output is `False`.

```
#include <stdio.h>

int main(int argc, char *argv[]) {
    printf("%d\n", -3 < -2 < -1);
    printf("%d\n", 3 > 2 == 1);
    return 0;
}

$ gcc test.cpp
$ ./a.out
0
1
```

It does so because `(-3 < -2) = True = 1` and `1 < -1 is False`. Also, to get a better understanding of how such expressions evaluate, try playing around with different values and see if your predicted value matches the actual output.

This essay is going to be extra special; in this one, we find out

- how Python evaluates chained comparison operators?
- how Python implements short-circuiting?
- how could you make Python-like evaluation a C-like evaluation? implying at the end of this essay we alter the CPython source code such that the expression `-3 < -2 < -1` will evaluate to `False`.

I know this sounds tempting, so let’s jump right into it.

# Chaining comparison operators

Python has a plethora of comparison operators like `<`, `>`, `<=`, `>=`, `==`, `!=`, `in`, and `is`. The output of the comparison operator is a boolean value - `True` or `False`. Python allows chaining of comparison operators which means, to check if `b` lies between `a` and `c`, we can simply do

```
>>> a < b < c
```

This is possible because internally Python evaluates this chained expression `a < b < c` as `(a < b) and (b < c)`. To make this efficient, the sub-expression `b` is evaluated only once and the evaluation also follows short-circuit evaluation; which means, if `(a < b)` is evaluated as `False` then Python would not evaluate further sub-expressions - `c` and `(b < c)`. Now that we have set the context, let’s find out what happens under the hood.

# Chaining under the hood

Disassembling the expression would give out the set of Python instructions that will be executed on the runtime’s execution stack. If we disassemble `a < b < c`, we get the following set of instructions. By the way, this is a great way to jump into the internals of anything in Python.

```
>>> import dis
>>> dis.dis('a < b < c')

  1           0 LOAD_NAME                0 (a)
              2 LOAD_NAME                1 (b)
              4 DUP_TOP
              6 ROT_THREE
              8 COMPARE_OP               0 (<)
             10 JUMP_IF_FALSE_OR_POP    18
             12 LOAD_NAME                2 (c)
             14 COMPARE_OP               0 (<)
             16 RETURN_VALUE
        >>   18 ROT_TWO
             20 POP_TOP
             22 RETURN_VALUE
```

Here is the summary of what each of the above instructions does; having this understanding will help us understand the entire execution process.

- `LOAD_NAME`: Loads the variable on the top of the stack
- `DUP_TOP`: Duplicates the top of the stack
- `ROT_THREE`: Rotates the top 3 elements of the stack by 1, such that the second element becomes the top, the third becomes the second while the top becomes the third.
- `COMPARE_OP`: Pops the top two elements from the stack, compare them (depending on the operator), compute the output, and puts it on the top of the stack.
- `JUMP_IF_FALSE_OR_POP`: Checks if the top of the stack is `False` if it is false then jumps to provided offset, and if it is `True` it pops the value.
- `RETURN_VALUE`: Pops the top of the stack and returns it
- `ROT_TWO`: Rotates the top two elements of the stack such that the top elements become the second, while the second becomes the top.
- `POP_TOP`: Pops the top element from the stack, kind of discarding it.

You can find details about these opcodes in the file [ceval.c](https://github.com/python/cpython/blob/master/Python/ceval.c). Now, let’s do an instruction by instruction walkthrough for the expression `1 < 2 < 3` to see how it evaluates to `True`.

## Evaluating `1 < 2 < 3`

When we run disassembler on `1 < 2 < 3` we get a similar disassembled code. It starts with the loading of two constant values `1` and `2` on the stack. Then it duplicates the top which makes our stack `2, 2, 1`. Now upon `ROT_THREE` the `2` on the top of the stack goes at the third spot while the other moves up one place. at this instruction, our stack looks like `2, 1, 2`.

![https://user-images.githubusercontent.com/4745789/116093465-508ab300-a6c4-11eb-8587-2922c4095eaa.png](https://user-images.githubusercontent.com/4745789/116093465-508ab300-a6c4-11eb-8587-2922c4095eaa.png)

Now, the `COMPARE_OP` operation pops out two elements from the stack and performs the comparison. The first popped value becomes the right operand while the second popped becomes the left operand. Post comparison the evaluated value is put on top of the stack again. Since `1 < 2`, the expression is evaluated as `True` and this `True` is put on to of the stack. So, after the `COMPARE_OP` instruction, the stack would look like `True, 2`.

Then comes the instruction `JUMP_IF_FALSE_OR_POP` which checks the top of the stack. Since the top of the stack is `True` (not `False`), it pops the value, making our stack `2`. Now `3` is loaded onto the stack making our stack `3, 2`.

Now `COMPARE_OP` pops out two elements, compares them, and since `2 < 3` it evaluates to `True` and this `True` is stacked on top. After this operation, the stack has just one element `True`.

The next instruction is `RETURN_VALUE`, which pops out the top of the stack i.e. `True`, and returns it; and this is how the expression `1 < 2 < 3` is evaluated to `True`.

## Short-circuit Evaluation

A very interesting instruction is sitting right in the middle - `JUMP_IF_FALSE_OR_POP`. This instruction is the one that is doing short-circuiting. Once the runtime encounters this instruction it checks the top of the stack,

- if top == `False` the flow jumps to the last few instructions, bypassing the loading and comparing other sub-expressions.
- if top == `True` it does not jump, but rather continues its evaluation of the next instructions.

To get a better understanding, try doing an instruction by instruction walkthrough for the expression `6 > 7 > 8` and you will find out how it bypasses the evaluating next sub-expressions.

![https://user-images.githubusercontent.com/4745789/116093532-60a29280-a6c4-11eb-9ad9-24a73dddb683.png](https://user-images.githubusercontent.com/4745789/116093532-60a29280-a6c4-11eb-9ad9-24a73dddb683.png)

Now we know, why the [official documentation](https://docs.python.org/3/reference/expressions.html#comparisons) says,

> Comparisons can be chained arbitrarily, e.g., x < y <= z is equivalent to x < y and y <= z, **except that y is evaluated only once (but in both cases z is not evaluated at all when x < y is found to be false)**.

## How does it “and” sub-expressions?

We have established and also seen in action how Python evaluates chained comparison operators. We also understand that it evaluates `1 < 2 < 3` as `(1 < 2) and (2 < 3)` but exactly where is this very logic implemented? The magic happens with two instructions `DUP_TOP` and `ROT_THREE`.

So, if we keenly observe, to evaluate `1 < 2 < 3` as `(1 < 2) and (2 < 3)` we would need to repeat the middle operand and keep it ready as the first operand of the second comparison. Now, to “repeat” the middle operand, we call `DUP_TOP`.

Once the two operands are loaded on the stack we see that the right operand sits on the top and by invoking `DUP_TOP` we are copying the middle operand and putting it on the top of the stack. This copied top (middle operand) needs to be preserved to be used as the first operand in the next comparison, and to do this we call `ROT_THREE` that puts the stack top to the third from the top.

After the first comparison is evaluated the stack contains - the copied middle operand and on top of it the evaluated value. The evaluated value is discarded or returned depending on if it is `True` or `False`, keeping the copied middle operand on the stack, making it the first operand of the next comparison.

# Make chain evaluation C-like

Now that we have understood how Chained Operators are evaluated and what how the evaluation is made “mathematics” like, let’s manipulate the code to make the evaluation C-like; which means we have to evaluate operands left to right and use the evaluated value as the first operand for next comparison. To be honest, if you have understood the importance of `DUP_TOP` and `ROT_THREE` making evaluation C-like is fairly straightforward.

The code that generates instructions for comparison expressions is in file [Python/compile.c](https://github.com/python/cpython/blob/master/Python/compile.c#L4031). The snippet that interests us is the function `compiler_compare` which can be seen below

```
static int
compiler_compare(struct compiler *c, expr_ty e)
{
    ...
        for (i = 0; i < n; i++) {
            VISIT(c, expr,
                (expr_ty)asdl_seq_GET(e->v.Compare.comparators, i));
            ADDOP(c, DUP_TOP);
            ADDOP(c, ROT_THREE);
            ADDOP_COMPARE(c, asdl_seq_GET(e->v.Compare.ops, i));
            ADDOP_JUMP(c, JUMP_IF_FALSE_OR_POP, cleanup);
            NEXT_BLOCK(c);
        }
    ...
        ADDOP(c, ROT_TWO);
        ADDOP(c, POP_TOP);
    ...
    }
    return 1;
}
```

To make the evaluation C-like we

- should not copy the middle operand
- ensure that the evaluated value (output from the first compare) remains on top - so that it becomes the first operand of the next expression

To achieve this, all we have to do is comment out `3` lines that do exactly that. Post changes the snippet would look something like this.

```
static int
compiler_compare(struct compiler *c, expr_ty e)
{
    ...
        for (i = 0; i < n; i++) {
            VISIT(c, expr,
                (expr_ty)asdl_seq_GET(e->v.Compare.comparators, i));

            // ADDOP(c, DUP_TOP);
            // ADDOP(c, ROT_THREE);

            ADDOP_COMPARE(c, asdl_seq_GET(e->v.Compare.ops, i));

            // ADDOP_JUMP(c, JUMP_IF_FALSE_OR_POP, cleanup);

            NEXT_BLOCK(c);
        }
    ...
        ADDOP(c, ROT_TWO);
        ADDOP(c, POP_TOP);
    ...
    }
    return 1;
}
```

Recall that the expression `-3 < -2 < -1` on a usual Python interpreter evaluates to `True` because `-2` is between `-3` and `-1`. But post these changes, if we build the binary and start the interpreter we would see the output of expression `-3 < -2 < -1` as `False`, just like C; as it evaluated the expression from left to right and kept reusing the output of the previous comparison as the first operand of the next one.

Here is the disassembled code and an instruction by instruction execution post our changes.

![https://user-images.githubusercontent.com/4745789/116272065-37eecb80-a79e-11eb-8345-cae342a8441c.png](https://user-images.githubusercontent.com/4745789/116272065-37eecb80-a79e-11eb-8345-cae342a8441c.png)

The engine first evaluated `-3 < -2`, and put the result `True` on top of the stack and then loaded `-1` to perform the comparison `True < -1`. Since `True == 1` the expression `True < -1` is evaluated as `False` and hence the output of the entire statement is `False`, just like in C. This also means that the expression `3 > 2 == 1` should evaluate to `True` and it actually does.

![https://user-images.githubusercontent.com/4745789/116272093-3d4c1600-a79e-11eb-9981-655744c86348.png](https://user-images.githubusercontent.com/4745789/116272093-3d4c1600-a79e-11eb-9981-655744c86348.png)

# References

- [Under the Hood: Python Comparison Breakdown](https://pybit.es/guest-python-comparison-breakdown.html)
- [How do chained comparisons in Python actually work?](https://stackoverflow.com/questions/28754726/how-do-chained-comparisons-in-python-actually-work)
- [Comparisons: Python Language Reference](https://docs.python.org/3/reference/expressions.html#comparisons)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/taxonomy-on-sql

How to design a taxonomy on a Relational DB

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [System Design](/knowledge-base/system-design)
- [Prototypes](/knowledge-base/system-design)

- [KB](/knowledge-base)
- [Syst...](/knowledge-base/system-design)
- [Prototypes](/knowledge-base/system-design)

# How to design a taxonomy on a Relational DB

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

In this essay, we will model [taxonomy](https://en.wikipedia.org/wiki/Taxonomy) on top of a relational database, and as a specific example, we will try to build [Udemy’s Taxonomy](https://www.udemy.com/). The primary focus of this essay is to understand how to design taxonomy on top of [SQL based relational DB](https://en.wikipedia.org/wiki/Relational_database), define and write queries that are computationally efficient along with deciding indexes on the designed tables.

In the process, we will also understand a very interesting SQL construct like Window Functions that helps us solve seemingly complex use-cases with a single SQL query.

# Udemy’s Taxonomy

[Udemy’s Taxonomy](https://www.udemy.com/) is very simple; it features top-level categories - like Software Engineering, Arts, and Business - each category has multiple sub-categories - like Programming Languages, Databases, Sketching - and each sub-category has niche topics like - Python, Javascript, MySQL, etc.

To keep things simpler, we restrain that one topic can be part of only one sub-category and one sub-category can belong to only one top-level category; and that makes the maximum levels in this taxonomy as `3`.

![https://user-images.githubusercontent.com/4745789/115139853-fcdbf200-a051-11eb-94f1-00382bd26db1.png](https://user-images.githubusercontent.com/4745789/115139853-fcdbf200-a051-11eb-94f1-00382bd26db1.png)

# Database Design

Out of our intuition, we can have one table for categories, one for holding sub-categories, and one for topics, and a bunch of [foreign keys](https://en.wikipedia.org/wiki/Foreign_key) that weaves them together. But is this the best we can come up with? A few issues with this design is

- all the 3 tables will have an identical schema
- if we were to introduce a new level, say `concept` that sits between sub-category and topic, we will have to create a new table to accommodate it, making this design cumbersome to future features and extensions.
- what if for a few topics we want it to be a child of a category, leaving out sub-categories altogether; handling this with this design will be very tricky.

So, we need a better design, that is robust and extensible and hence we go for a single table called `topics` that holds categories, sub-categories, and topics differentiated with a column called `type` distinguishing between the 3. The schema of this table `topics` would be

![SQL Schema - Taxonomy Udemy](https://user-images.githubusercontent.com/4745789/115140362-8260a180-a054-11eb-8820-2a830dcc025e.png)

Now that we have the table `topics` ready, we see how the following two topics are stored

- Software Engineering > Programming Languages > Python
- Software Engineering > Programming Languages > Javascript

![Sample Data - Taxonomy Udemy](https://user-images.githubusercontent.com/4745789/115140389-b340d680-a054-11eb-8a6d-b39a9f15fde8.png)

# Indexes on `topics`

Picking the right set of indexes is one of the most critical decisions that you will be taking while designing this system. A good set of indexes boosts the overall performance of the system, while poor and/or missing ones will put your database under a terrible load, especially at scale.

But how do we pick which indexes do we want on `topics`? The answer here is very simple, it depends on the kind of queries we have to support. So, let’s list down queries that we will need and then determine indexes to make them efficient.

## Get topic by ID

The most common query that we’d need is getting a topic by its `id` and this is very well facilitated by making `id` as a [primary key](https://en.wikipedia.org/wiki/Primary_key) of the table.

```
SELECT * FROM topics WHERE id = 531;
```

## Get the topic path

Getting a topic path is an interesting use case. While rendering any category, sub-category, or topic page we would need to render breadcrumbs that hold the path of it in the taxonomy. For example, for Python’s page, we will need to render a path like

```
Software Engineering > Programming Languages > Python
```

This path helps users explore and discover new categories, sub-categories, or topics. So, with our current schema, how could we compute the topic path for a given topic id.

Doing it on the application side is the first approach that comes to mind but it is a poor one because we would be making `n` selects for `n` levels. In the case of our current system, we will be making `3` selects to compute the topic path; with the application pseudocode looking something like this

```
def get_topicpath(topic_id):
    path = []

    topic = get_topic_by_id(topic_id)
    path.append(topic)

    while topic.parent_id:
        topic = get_topic_by_id(topic.parent_id)
        path.append(topic)

    return path
```

We can do a lot better than this. Since we know that the hierarchy has at max 3 levels, we can just do this in one SQL query with minor `NULL` handling on the application side.

The SQL query to get the topic path would have to join `3` instances of the `topics` table, each one handling one level in the hierarchy and joining with its parent on `parent_id`. The SQL query would fetch the `id` and the `name` of the topics in the topic path.

```
SELECT topics_level1.id, topics_level1.name,
       topics_level2.id, topics_level2.name,
       topics_level3.id, topics_level3.name

FROM topics AS topics_level3
    LEFT JOIN topics AS topics_level2
        ON topics_level2.id = topics_level3.parent_id
    LEFT JOIN topics AS topics_level1
        ON topics_level1.id = topics_level2.parent_id

WHERE topics_level3.id = 610;
```

In the SQL query above we fetch the topic path for topic id `610`. We join table `topics` twice (3 instances of topics table) each handling a distinct level. Since we are using JOIN, if a `parent_id` is `NULL` and the join parameter would not match anything which would result `NULL` selects for those columns. These `NULL` values come in very handy when we compute the topic path for sub-categories and categories.

If the topic with `610` id is of type `topic` then

- `topics_level1.id`, `topics_level1.name` will be category
- `topics_level2.id`, `topics_level2.name` will be sub-category
- `topics_level3.id`, `topics_level3.name` will be topic

If the topic with `610` id is of type `sub-category` then

- `topics_level1.id`, `topics_level1.name` will be `NULL`
- `topics_level2.id`, `topics_level2.name` will be category
- `topics_level3.id`, `topics_level3.name` will be sub-category

If the topic with `610` id is of type `category` then

- `topics_level1.id`, `topics_level1.name` will be `NULL`
- `topics_level2.id`, `topics_level2.name` will be `NULL`
- `topics_level3.id`, `topics_level3.name` will be category

So, in the application code, we still access all the selected columns but we create the topic path skipping the `NULL` values accordingly.

To support this query, our table only requires [Primary Key](https://en.wikipedia.org/wiki/Primary_key) on `id` and [Foreign Key](https://en.wikipedia.org/wiki/Foreign_key) on `parent_id`.

## Get all the children of a category or a sub-category

Getting all the children of a category or a sub-category will be heavily used to drive the “Browse and Explore” page, where users would want to drill down and explore the kind of topics Udemy covers. SQL Query for this has to support pagination and will be required to output all children for a given parent, in order of `score` such that more popular children are returned first.

```
SELECT * FROM topics WHERE parent_id = 123 ORDER BY score DESC;
```

The SQL query above fetches all the child topics of a given parent topic with `id` = `123`. Since we are ordering by `score`, for this query to be efficient we create a [composite index](<https://en.wikipedia.org/wiki/Composite_index_(database)>) on `(parent_id, score)`.

## Get category hierarchy

Udemy, on its home page, puts out all the categories under a dropdown menu enabling users to explore top categories and topics in a glimpse.

One peculiar behavior of this is it shows all categories and top `k` sub-categories within each. Once we hover upon a sub-category it makes a network call to fetch top topics within that sub-category. This means we need to write a query that fetches all categories and `k` sub-categories within each category from the entire `topics` table.

Although it looks very complicated at first, it is very easy to do with a single SQL query.

```
SELECT t1_id, t1_name, t2_id, t2_name, t2_score
FROM (
    SELECT topics1.id AS t1_id, topics1.name AS t1_name,
           topics2.id AS t2_id, topics2.name AS t2_name,
           ROW_NUMBER() OVER (PARTITION BY topics1.id) row_num

    FROM topics AS topics1
        LEFT JOIN topics AS topics2 ON topics1.id = topics2.parent_id

    WHERE topics1.type = 1 and topics2.type = 2

    ORDER BY topics1.score DESC, topics2.score DESC

) t
WHERE row_num <= 10;
```

Above SQL query picks all categories and top `10` sub-categories from each category and returns it as part of `SELECT`. It uses a very interesting SQL construct called Window Functions, specifically [`ROW_NUMBER`](https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html#function_row-number) and `PARTITION BY`.

We perform the usual join on `topics` once where the left operand is categories (topics with `type = 1`) and the right one is a sub-category (topics with `type = 2`). We then partition this join by category `id` and then compute `ROW_NUMBER` for sub-categories within it.

The row numbers are computed for each partition separately so it goes as `1, 2, 3, ..., n` for `n` rows within each category. We then apply a simple `WHERE` clause check on this row number to be `<= k` which then typically matches the first `k` row within each partition i.e category.

Note: to get “top” `k` sub-categories we just apply for an additional `ORDER BY` on `score` that sorts the sub-categories ensuring top sub-categories are fetched first. This way the first `k` rows we consider from the partition are essentially the top sub-categories within the category.

To make this SQL query efficient we would need a [foreign key](https://en.wikipedia.org/wiki/Foreign_key) on `parent_id` and an index on `score` to make `ORDER BY` efficient.

## Summary of indexes we need on `topics`

- Primary Key on `id`
- Foreign Key on `parents_id`
- Index on `type`
- Composite Index on `(parent_id, score)`

# Explore more

Although we covered quite a bit of this DB design there is always something interesting in exploring something new around this topic; so

- explore [Nested Set Model](https://en.wikipedia.org/wiki/Nested_set_model) to design Taxonomy on relational databases
- explore how DB engines behave when there are no indexes, you can use `EXPLAIN` to understand the behavior
- find if there could be a better alternative to paginate results apart from `LIMIT/OFFSET`

Thus we designed a neat [Taxonomy](https://en.wikipedia.org/wiki/Taxonomy) on top of SQL-based relational databases like MySQL, Postgres, etc; wrote queries for some common scenarios, and determined the indexes to make taxonomy efficient.

# References

- [Window Functions - MySQL](https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html)
- [Partitioning Types - MySQL](https://dev.mysql.com/doc/mysql-partitioning-excerpt/8.0/en/partitioning-types.html)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/the-weird-walrus

Changing the Python's walrus operator - a deep dive into CPython

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Language Internals](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

- [KB](/knowledge-base)
- [Lang...](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

# Changing the Python's walrus operator - a deep dive into CPython

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Python in version 3.8 introduced [Assignment Expressions](https://realpython.com/lessons/assignment-expressions/) which can be used with the help of the Walrus Operator `:=`. This expression does assign and return in the same expression helping in writing a concise code.

Say you are building your own shell in Python. It takes commands and input from the prompt, executes it on your shell, and renders the output. The shell should stop the execution as soon as it receives the `exit` command. This seemingly complicated problem can be built using just 4 lines of Python code.

```
command = input(">>> ")
while command != "exit":
    os.system(command)
    command = input(">>> ")
```

Although the above code runs perfectly fine, we can see that the `input` is taken twice, once outside the loop and once within the loop. This kind of use case is very common in Python.

Walrus Operator fits perfectly here; now instead of initializing `command` with `input` outside and then checking if `command != 'exit'`, we can merge the two logic in one expression. The 4 lines of code above can be rewritten into the most intuitive 2 lines

```
while (command := input(">>> ")) != "exit":
    os.system(command)
```

# What’s weird with the Walrus operator?

Now that we have established how useful the Walrus Operator could be for us, let’s dive into the weird stuff. Since the Walrus operator has functioning similar to an assignment operator `=`, we would expect the following code to work fine, but it actually gives an error, not just any but a `SyntaxError`.

```
>>> a := 10
  File "<stdin>", line 1
    a := 10
      ^
SyntaxError: invalid syntax
```

If you thought, that was weird wait till we wrap the exact same statement with parenthesis and execute it.

```
>>> (a := 10)
10
```

What! it worked! How? What happened here? Just by wrapping the statement by parenthesis made an invalid Syntax valid? Isn’t it weird? This behavior is pointed out in a Github repository called [wtf-python](https://github.com/satwikkansal/wtfpython#-first-things-first-). The theoretical explanation for this behavior is simple; Python disallows non-parenthesized Assignment Expressions but it allows non-parenthesized assignment statements.

In this essay, we dig deep into CPython and find out hows and the whys.

# The hows and the whys

Few points to note:

- The Walrus Operator or Assignment Expressions are called Named Expressions in CPython.
- The branch of the CPython we are referring to here is for version `3.8`

## The Grammar

If `a := 10` is giving us a Syntax Error then it must be linked to the Grammar specification of the language. The grammar of Python can be found in the file [Grammar/Grammar](https://github.com/python/cpython/blob/3.8/Grammar/Grammar). So if we grep `namedexpr` in the Grammar file we get the following rules

```
namedexpr_test: test [':=' test]

atom: ('(' [yield_expr|testlist_comp] ')' |
       '[' [testlist_comp] ']' |
       '{' [dictorsetmaker] '}' |
       NAME | NUMBER | STRING+ | '...' | 'None' | 'True' | 'False')

testlist_comp: (namedexpr_test|star_expr) ( comp_for | (',' (namedexpr_test|star_expr))* [','] )

if_stmt: 'if' namedexpr_test ':' suite ('elif' namedexpr_test ':' suite)* ['else' ':' suite]

while_stmt: 'while' namedexpr_test ':' suite ['else' ':' suite]
```

The above Grammar rules give us a good gist of how Named Expressions are supposed to be used. Here are some observations about it -

- can be used in `while` statements
- can be used along with `if` statements
- named expressions are part of a rule called `testlist_comp`, which seems related to list comprehensions

We can see that the `atom` rules put in a hard check that `testlist_comp` should be either surrounded by `()` or `[]` and since `testlist_comp` can have `namedexpr_test` this puts in the check that Named Expressions should be surrounded by `()` or `[]`.

```
>>> (a := 1)
1
>>> [a := 1]
[1]
```

So when we run `a := 1`, none of the Grammar rules is satisfied and hence this results in a `SyntaxError`.

## What about `if` and `while`?

According to the rule `if_stmt` and `while_stmt` you can have named expressions right after `if` without needing any brackets surrounding it. This means the following statement is valid, but still chose to put parenthesis around `:=`, why?

```
while command := input(">>> ") != "exit":
```

The answer is simple, [Operator Precedence](https://en.wikipedia.org/wiki/Order_of_operations); because of the configured precedence the above statement sets `command` as `bool` after evaluating `input(">>> ") != "exit"` but we do not want this behaviour. Instead, we want `command` to be set as a command given as an input through `input` call and hence we wrap the expression with parenthesis for specifying explicit precedence.

# Allowing `a := 10`

Till now we saw how doing `a := 10` on a fresh Python prompt gives us a `SyntaxError`, so how about altering the CPython to allow `a := 10`? Sounds fun, isn’t it?

## Changing the Grammar

To achieve what we want to we will have to alter the Grammar rules. A good point to note here is that as a standalone statement, `:=` works and behaves very similar to a regular assignment statement having an `=`. So let’s first find out, where have we allowed regular assignment statements

```
stmt: simple_stmt | compound_stmt
simple_stmt: small_stmt (';' small_stmt)* [';'] NEWLINE
small_stmt: (expr_stmt | del_stmt | pass_stmt | flow_stmt |
             import_stmt | global_stmt | nonlocal_stmt | assert_stmt)
expr_stmt: testlist_star_expr (annassign | augassign (yield_expr|testlist) |
                     [('=' (yield_expr|testlist_star_expr))+ [TYPE_COMMENT]] )
```

The regular assignment statements are allowed as per `expr_stmt` rule which is, in turn, a `small_stmt`, `simple_stmt`, and `stmt`. Rules are self-explanatory and skimming them would help you understand what exactly is happening in there.

In order to mimic the behavior of `:=` to be the same as `=` how about adding a new rule in `expr_stmt` that suggests matching the same pattern as `=`. So we make the following change in `expr_stmt`.

```
expr_stmt: testlist_star_expr (annassign | augassign (yield_expr|testlist) |
                     [('=' (yield_expr|testlist_star_expr))+ [TYPE_COMMENT]] |
                     [(':=' (yield_expr|testlist_star_expr))+ [TYPE_COMMENT]] )
```

When we change anything in the `Grammar` file, we have to regenerate the parser code; and this can be done using the following command

```
$ make regen-grammar
```

Once the above command is successful, we generate a fresh Python binary and see our changes in action.

```
$ make && ./python.exe
```

On the fresh prompt that would have popped up try putting in `a := 10`, once you do this you will find out that this does not give any error and it executes seamlessly and it works just like a normal assignment statement, the behavior that we were seeking.

So with these changes, we have our Python interpreter that supports all three statements without any Error.

```
>>> a = 10
>>> (b := 10)
10
>>> c := 10
```

All of these changes were made on my own [fork of CPython](https://github.com/arpitbbhayani/cpython) and the PR can be found [here](https://github.com/arpitbbhayani/cpython/pull/8).

# References

- [CPython Source Code Guide](https://realpython.com/cpython-source-code-guide/)
- [Exploring CPython’s Internals](https://devguide.python.org/exploring/)
- [wtfpython - Github Repository](https://github.com/satwikkansal/wtfpython)
- [Assignment Expressions: The Walrus Operator](https://realpython.com/lessons/assignment-expressions/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/fully-persistent-arrays

Fully Persistent Arrays - a datastructure that let's us time travel

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Fully Persistent Arrays - a datastructure that let's us time travel

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

[Persistent Data Structures](https://arpitbhayani.me/blogs/persistent-data-structures-introduction) preserve previous versions of themselves allowing us to revisit and audit any historical version. While we already did an exhaustive introduction in the [previous essay](https://arpitbhayani.me/blogs/persistent-data-structures-introduction), in this essay, we take a detailed look into how we can implement the **_Fully Persistent_** variant of the most common data structure - **_Arrays_**.

The essay is loosely based on the paper titled [Fully Persistent Arrays for Efficient Incremental Updates and Voluminous Reads](https://link.springer.com/chapter/10.1007/3-540-55253-7_7) by Tyng-Ruey Chuang and we implement it using Backer’s trick elaborated in the paper [A Persistent Union-Find Data Structure](https://www.lri.fr/~filliatr/ftp/publis/puf-wml07.pdf).

# Fully Persistent Arrays

An array is an abstract data structure consisting of `n` slots to hold a maximum of `n` elements such that each element is identified by at least one index. A typical array allows the following functions - `create(n)`, `update(index, value)` and `get(index)`.

The simplest form of an array, that we all are familiar with, is the Linear Array that is designed to hold elements in consecutive memory locations, leveraging spatial locality for faster and more efficient retrievals and scans. Before we jump into the implementation details of Fully Persistent Arrays, let’s reiterate what exactly are Fully Persistent Data Structures.

Persistent Data Structures preserve previous versions of themselves allowing us to revisit and audit any historical version. Fully Persistent Data Structures allows access and modification to all the historical versions as well. It does not restrict any modifications whatsoever. This means we can typically revisit any historical version of the data structure, modify it like we are forking out a new branch.

![https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png](https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png)

Fully Persistent Arrays are arrays that support Full Persistence which means it supports usual array operations while also allowing us to go back in time and make updates to any of the previous versions. We define the following operations on Fully Persistent Arrays -

- `create(n)` - returns an array of size `n` having all the slots uninitialized
- `update(array, index, value)` - returns a new array identical to `array` except for the element at the position `index`. The parent array `array` remains unaffected and is still accessible.
- `get(array, index)` - returns the element present at the index `index` in array `array`

# Implementing Fully Persistent Array

A naive way of implementing these arrays is to do a [Copy-on-Write](https://arpitbhayani.me/blogs/copy-on-write) and keep track of historical versions. This approach very inefficient as it requires `m` times the memory required to hold `n` elements, where `m` is the total number of versions of the array.

![https://user-images.githubusercontent.com/4745789/107803148-3cebd380-6d88-11eb-9889-bb551e83c00a.png](https://user-images.githubusercontent.com/4745789/107803148-3cebd380-6d88-11eb-9889-bb551e83c00a.png)

A better way of implementing these arrays is by using the [Backer’s Trick](https://www.lri.fr/~filliatr/ftp/publis/spds-rr.pdf) which enables the required functionality with just one array and a tree of modifications.

# Fully Persistent Arrays using Backer’s Trick

A more efficient way of implementing Fully Persistent Arrays is by using a single instance of an in-memory Array and in conjunction use a tree of modifications. Instead of storing all the versions separately, Backer’s trick allows us to compute any version of the array by replaying all the changes asked for.

## Tree of modifications

The tree of modifications is an `n`-ary tree that holds all the versions of the array by storing only the modifications made to the elements. Each version is derived from a parent version and the root points to the in-memory _cache_ array which holds the initial version of it.

![https://user-images.githubusercontent.com/4745789/107856766-71c35d80-6e50-11eb-9f59-c3744cdc884f.png](https://user-images.githubusercontent.com/4745789/107856766-71c35d80-6e50-11eb-9f59-c3744cdc884f.png)

Each node of the tree holds three fields - `index`, `value`, and a pointer to the `parent`, making this tree pointing upwards towards to root. Thus each node holds the changed `value`, where did the change happen `index` and on which version the change happened `parent`.

Say we changed the element at the index `1` of the array `9, 6, 3, 5, 1` to `7` we get array `9, 7, 3, 5, 1`. The tree of modifications has 2 nodes one root node `a0` pointing to the initial array, and another node `a1` denoting the updated version.

![https://user-images.githubusercontent.com/4745789/107858298-996af380-6e59-11eb-99dc-7a68ea25f5b4.png](https://user-images.githubusercontent.com/4745789/107858298-996af380-6e59-11eb-99dc-7a68ea25f5b4.png)

The node `a1` has 3 fields, `index` set to `1`, `value` set to `7` and `parent` pointing to `a0`. The node implies that it was derived from `a0` by changing the value of the element at the index `1` to `7`. If we try to branch off `a0` with another change say index `4` set to value `9` we would have 3 nodes in the tree. Thus we see how an update translates into just creating a new node and adding it at the right place in the tree.

Now we see with this design how we implement the three functions of an array `create`, `update`, and `get`.

## Implementing `create`

The `create` function allocates a linear array of size `n` to hold `n` elements. This is a usual array allocation. While doing this we also create the root node of our tree of modifications. The root node, as established earlier, points to the _cache_ array.

```
# The function creates a new persistent array of size `n`
def create(n: int) -> Array:
    # 1. allocate in-memory cache
    # 2. initialize the tree of modifications
    # 3. make the root of the tree point to the cache
    pass
```

The overall complexity of this operation is `O(n)` space and `O(n)` time.

## Implementing `update`

The `update` operation takes in the `index` that needs to be updated, the `value`, and the version of the array on which update is to be made.

```
# The function updates the element at the index `index` with value
# `value` on array `array` and returns the newly updated array
# keeping the old one accessible.
def update(array: Array, index: int, value: object) -> Array:
    # 1. create a node in the tree and store index, the value in it
    # 2. point this new node to the parent array
    pass
```

To do this efficiently, we create a new node in the tree whose parent is set to the array version on which update is performed, index and value are set what was passed during invocation. Thus we see that the `update` operation takes a constant `O(1)` space and `O(1)` time to create and represent a new version of the array.

With the update operation being made efficient we have to trade-off `get` operation.

## Implementing `get`

The `get` operation takes in the `index` that needs to be fetched and the version of the array from which the element is to be fetched. The `get` operation seeks no extra space but takes time proportional to the distance between the array version and the root. In the worst case, this distance will be as long as the total number of versions of the array.

```
# The function fetches the element from index `index`
# from the array `array` and returns it.
def get(array: Array, index: int) -> object:
    # 1. Start from the requested array and traverse to the root node.
    # 2. Allocate a new register to store the requested value
    # 3. During traversal, if the node.index == `index` update the
    # register with the value.
    # 4. return the value of the register
    pass
```

The overall complexity of this operation is `O(1)` space and `O(n)` time.

# Optimizing successive reads on the same version

We established that the update operation takes constant time and reads are expensive. If our system is write-heavy, then this is pretty handy but if the system has more reads then operation taking `O(n)` time hampers the overall performance of the system. So as to optimize this use case we take a look at the operation called _Rerooting._

## Rerooting

The initial array (the first version of the array) has no significance to be the root forever. We can reroot the entire tree such that any child node could become the root and the value it points to - _cache_ - represents the true copy of the array. Rerooting is a sequence of rotations to make the desired array version the root.

The algorithm for rerooting is a classic Backtracking algorithm that requires updates in all the nodes coming in the path from the old node to the new node.

![https://user-images.githubusercontent.com/4745789/107935855-ee1c8480-6fa7-11eb-9870-2ae90d6460a9.png](https://user-images.githubusercontent.com/4745789/107935855-ee1c8480-6fa7-11eb-9870-2ae90d6460a9.png)

The rerooting operation takes time proportional to the distance between the old and new root ~ `O(n)`. Since the successive reads are happening on the same version the `get` operation becomes `O(1)` as well. Thus depending on the kind of usage of the system we can add rerooting step in either `get` or `update` operation.

# References

- [Persistent Arrays - Wikipedia](https://en.wikipedia.org/wiki/Persistent_array)
- [Semi-Persistent Data Structures](https://www.lri.fr/~filliatr/ftp/publis/spds-rr.pdf)
- [Fully Persistent Arrays for Efficient Incremental Updates and Voluminous Reads](https://link.springer.com/chapter/10.1007/3-540-55253-7_7)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/persistent-data-structures-introduction

Introduction to Persistent Data Structures

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Introduction to Persistent Data Structures

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Ordinary data structures are ephemeral implying that any update made to it destroys the old version and all we are left with is the updated latest one. Persistent Data Structures change this notion and allow us to hold multiple versions of a data structure at any given instant of time. This enables us to go back in “time” and access any version that we want.

In this essay, we take a detailed look into the world of Persistent Data Structures and see the basics of its implementation along with where exactly we can find them in action. This essay is meant to act as an exhaustive introduction to the topic and in future essays, we will dive deeper into the specifics of each data structure.

This essay is loosely based on the iconic paper published in 1986 titled [Making Data Structures Persistent](https://www.cs.cmu.edu/~sleator/papers/making-data-structures-persistent.pdf) by James R. Driscoll, Neil Sarnak, Daniel D. Sleator, and Robert E. Tarjan.

# Persistence

Persistent Data Structures preserve previous versions of themselves allowing us to revisit and audit any historical version. Depending on the operations allowed on the previous versions, persistence is classified into three categories

### Partially Persistent

Partially Persistent Data Structures allows access to all the historical versions but allows modification to only the newest one. This typically makes historical versions of the data structure immutable (read-only).

![https://user-images.githubusercontent.com/4745789/107117960-e4a66480-68a3-11eb-97a0-c8c412527471.png](https://user-images.githubusercontent.com/4745789/107117960-e4a66480-68a3-11eb-97a0-c8c412527471.png)

### Fully Persistent

Fully Persistent Data Structures allows access and modification to all the historical versions. It does not restrict any modifications whatsoever. This means we can typically revisit any historical version and modify it and thus fork out a new branch.

![https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png](https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png)

### Confluently Persistent

Confluently Persistent Data Structures allow modifications to historical versions while also allowing them to be merged with existing ones to create a new version from the previous two.

![https://user-images.githubusercontent.com/4745789/107117954-da846600-68a3-11eb-9c34-b9489c170710.png](https://user-images.githubusercontent.com/4745789/107117954-da846600-68a3-11eb-9c34-b9489c170710.png)

# Applications of Persistent Data Structures

Persistent Data Structures find their applications spanning the entire spectrum of Computer Science, including but not limited to - Functional Programming Languages, Computational Geometry, Text Editors, and many more.

## Functional Programming Languages

Functional Programming Languages are ideal candidates for incorporating Persistent Data Structures as they forbid, while some discourage, the mutability of underlying structures. These languages pass around states within functions and expect that they do not update the existing one but return a new state. Programming languages like Haskell, Clojure, Elm, Javascript, Scala have native Persistent implementations of data structures like Lists, Maps, Sets, and Trees.

## Computational Geometry

One of the fundamental problems in Computational Geometry is the [Point Location Problem](https://en.wikipedia.org/wiki/Point_location) which deals with identifying the region where the query point lies. A simpler version of the problem statement is to find if a point lies within or outside a given polygon. A popular solution to determine a solution to the Point Location problem statement uses [Persistent Red-Black Trees](https://en.wikipedia.org/wiki/Red%E2%80%93black_tree).

## Text and File Editing

The most common operation required by any Text or File editing tool is _Undo and Redo_ and having persisted all historical versions through a persistent data structure makes these most frequent operations very efficient and a breeze.

# Implementing Partial Persistence

There are a few generic techniques that help in implementing Partial Persistence. Just to reiterate, partial persistence allows access to all the historical versions but allows modification to only the newest one, to create a newer updated copy of data.

## Copy on Write Semantics

A naive way to implement Partial Persistence is by utilizing the Copy-on-Write semantics and naively creating a deep copy upon every update. This technique is inefficient because upon every writes the entire data structure is deep copied.

There are certain methods built upon certain storage paradigms that copy only what matters making the entire CoW efficient. I have already gone in-depth of [Copy-on-Write semantics](https://arpitbhayani.me/blogs/copy-on-write) and I encourage you to check that out.

## The Fat Node Method

Instead of creating copies of the entire data structure, the Fat Node method suggests that each cell holding the value within the data structure is modified to hold multiple values (one for each version) making it arbitrarily _fat_. Each value node thus holds a value and a version stamp.

```
class Node:
    def __init__(self, value: object):
        # references to other Nodes creating a Node topology
        # that forms the core of the data structure
        # this could be `next` and `prev` pointers in a linked list
        # or `left`, `right` in case of a binary search tree.
        self.refs: List[] = []

        # holding all the values against the version (key)
        self.values: Dict[int, object] = {}
```

## The Node-Copying Method

The Node-Copying method eliminates all the problems with the Fat Node method. It allows each node to hold only a fixed number of values in it. Upon exhaustion of space, a new copy of Node is created and it holds only the newest values in it. The old node also holds pointers to the newly created node allowing browsing.

Having this structure helps in making update operation slightly efficient by reducing the number of nodes to be copied during writes, considering the in-degrees to each node is bounded.

## Path-Copying Method

The path-copying method copies all the nodes coming in the path from the root to the node being modified. This way it tries to minimize the copy and promotes reusing some of the unmodified data. This method comes in super handy in Linked Data Structures like Lists and Trees.

![https://user-images.githubusercontent.com/4745789/107144006-33b0d000-695e-11eb-9e13-959eaeba44f4.png](https://user-images.githubusercontent.com/4745789/107144006-33b0d000-695e-11eb-9e13-959eaeba44f4.png)

> Implementation of Full Persistence is a mammoth of a topic on its own and deserves its own essay. Hence skipping it from the scope of this one.

# Reducing the memory footprint

Since persistent data structures thrive on high memory usage, they require some garbage collection system to prevent memory leaks. Algorithms like [Reference Counting](https://en.wikipedia.org/wiki/Reference_counting) or [Mark and Sweep](https://en.wikipedia.org/wiki/Mark_and_sweep) serves the purpose pretty well.

Thus when a historical version is not referenced anymore in the program space, the corresponding objects and nodes are freed up.

# References

- [Making Data Structures Persistent](https://www.cs.cmu.edu/~sleator/papers/making-data-structures-persistent.pdf)
- [Persistent data structure - Wikipedia](https://en.wikipedia.org/wiki/Persistent_data_structure)
- [Geometric Data Structures](http://www.sccg.sk/~samuelcik/dgs/geom_structures.pdf)
- [Point Location Problem - Wikipedia](https://en.wikipedia.org/wiki/Point_location)
- [Text Editor: Data Structures - HackerNews](https://news.ycombinator.com/item?id=15381886)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/constant-folding-python

How python optimises the runtime using constant folding

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Language Internals](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

- [KB](/knowledge-base)
- [Lang...](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

# How python optimises the runtime using constant folding

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Every programming language aims to be performant in its niche and achieving superior performance requires a lot of compiler level optimizations. One famous optimization technique is [Constant Folding](https://en.wikipedia.org/wiki/Constant_folding) where during compile time the engine tries to recognize constant expressions, evaluate them, and replaces the expression with this newly evaluated value, making the runtime leaner.

In this essay, we dive deep and find what exactly is Constant Folding, understand the scope of it in the world of Python and finally go through Python’s source code - [CPython](https://github.com/python/cpython/) - and find out how elegantly Python actually implements it.

# Constant Folding

In [Constant Folding](https://en.wikipedia.org/wiki/Constant_folding), the engine finds and evaluates constant expressions at compile time rather than computing them at runtime, making the runtime leaner and faster.

```
>>> day_sec = 24 * 60 * 60
```

When the compiler encounters a constant expression, like above, it evaluates the expression and replaces it with the evaluated value. The expression is usually replaced by the evaluated value in the [Abstract Syntax Tree](https://en.wikipedia.org/wiki/Abstract_syntax_tree), but the implementation is totally up to the language. Hence the above expression is effectively executed as

```
>>> day_sec = 86400
```

# Constant Folding in Python

In Python, we could use the [Disassembler module](https://docs.python.org/3/library/dis.html#module-dis) to get the CPython bytecode giving us a good peek at how things will be executed. When we disassemble the above constant expression using the `dis` module, we get the following bytecode

```
>>> import dis
>>> dis.dis("day_sec = 24 * 60 * 60")

        0 LOAD_CONST               0 (86400)
        2 STORE_NAME               0 (day_sec)
        4 LOAD_CONST               1 (None)
        6 RETURN_VALUE
```

We see that the bytecode, instead of having two binary multiply operations followed by one `LOAD_CONST`, is having just one `LOAD_CONST` with the already evaluated value of `86400`. This indicates that the CPython interpreter during parsing and building of Abstract Syntax Tree folded the constant expression, `24 * 60 * 60` and replaced it with the evaluated value `86400`.

### Scope of Constant Folding

Python tries to fold every single constant expression present but there are some cases where even though the expression is constant, but Python chooses not to fold it. For example, Python does not fold `x = 4 ** 64` while it does fold `x = 2 ** 64`.

Apart from the arithmetic expressions, Python also folds expressions involving Strings and Tuples, where constant string expressions till the length `4096` are folded.

```
>>> a = "-" * 4096   # folded
>>> a = "-" * 4097   # not folded
>>> a = "--" * 4096  # not folded
```

# Internals of Constant Folding

Now we shift our focus to the internals and find exactly where and how CPython implements Constant Folding. All AST optimizations, including Constant Folding, can be found in file [ast_opt.c](https://github.com/python/cpython/blob/master/Python/ast_opt.c). The base function starting it all is `astfold_expr` which folds any and every expression that Python source has. The function recursively goes through the AST and tries to fold every constant expression, as seen in the snippet below.

![https://user-images.githubusercontent.com/4745789/103898628-38922200-511b-11eb-965f-fb4d46d3c45c.png](https://user-images.githubusercontent.com/4745789/103898628-38922200-511b-11eb-965f-fb4d46d3c45c.png)

The `astfold_expr` before folding the expression at hand, tries to fold its child expressions (operands) and then delegates the folding to the corresponding specific expression folding function. The operation-specific folding function evaluates the expression and returns the evaluated constant value, which is then put into the AST.

For example, whenever `astfold_expr` encounters a binary operation, it recursively folds the two child operands (expressions) before evaluating the expression at hand using `fold_binop`. The function `fold_binop` returns the evaluated constant value as seen in the snippet below.

![https://user-images.githubusercontent.com/4745789/103898745-670ffd00-511b-11eb-88a9-f741157473b3.png](https://user-images.githubusercontent.com/4745789/103898745-670ffd00-511b-11eb-88a9-f741157473b3.png)

`fold_binop` function folds the binary operation by checking the kind of operator at hand and then invoking the corresponding evaluation function on them. For example, if the operation at hand is an addition then, to evaluate the final value, it invokes `PyNumber_Add` on both its left and right operands.

### What makes this elegant?

Instead of writing special logic to handle certain patterns or types to fold constant expressions efficiently, CPython invokes the same general code. For example, it invokes the same usual `PyNumber_Add` function while folding that it does to perform the usual addition operation.

CPython has thus eradicated the need to write special functions to handle constant folding by making sure its code and evaluation process is structured in such a way that the general-purpose code itself can handle the evaluation of constant expressions.

# References

- [Constant Folding](https://en.wikipedia.org/wiki/Constant_folding)
- [CPython Optimizations](https://stummjr.org/post/cpython-optimizations/)
- [Python dis module and constant folding](https://yasoob.me/2019/02/26/python-dis-module-and-constant-folding/)
- [The simple way CPython does constant folding](https://utcc.utoronto.ca/~cks/space/blog/python/CPythonConstantFolding)
- [A constant folding optimization pass for the AST](https://bugs.python.org/issue1346238)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/string-interning-python

String interning in Python - a deep dive into CPython

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Language Internals](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

- [KB](/knowledge-base)
- [Lang...](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

# String interning in Python - a deep dive into CPython

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Every programming language aims to be performant in its niche and achieving superior performance requires a bunch of compiler and interpreter level optimizations. Since character Strings are an integral part of any programming language, having the ability to perform string operations quickly elevates the overall performance.

In this essay, we dive deep into Python internals and find out how Python makes its interpreter performant using a technique called [String Interning](https://en.wikipedia.org/wiki/String_interning). This essay not only aims to put forth Python internals but also aims to make the reader comfortable in navigating through Python’s source code; so expect a lot of code snippets taken from [CPython](https://github.com/python/cpython/).

# String Interning

String Interning is a compiler/interpreter optimization method that makes common string processing tasks space and time efficient by [caching](<https://en.wikipedia.org/wiki/Cache_(computing)>) them. Instead of creating a new copy of string every time, this optimization method dictates to keep just one copy of string for every _appropriate_ [immutable](https://en.wikipedia.org/wiki/Immutable_object) distinct value and use the pointer reference wherever referred.

The single copy of each string is called its **_intern_** and hence the name String Interning. The lookup of string intern, may or may not be exposed as a public interfaced method. Modern programming languages like Java, Python, PHP, Ruby, Julia, and many more, performs String Interning to make their compilers and interpreters performant.

![https://user-images.githubusercontent.com/4745789/102705512-d1691680-42ae-11eb-825f-1e032a7c12c5.png](https://user-images.githubusercontent.com/4745789/102705512-d1691680-42ae-11eb-825f-1e032a7c12c5.png)

## Why should Strings be interned?

_String Interning speeds up string comparisons_. Without interning if we were to compare two strings for equality the complexity of it would shoot up to `O(n)` where we examine every character from both the strings to decide their equality. But if the strings are interned, instead of checking every character, equal strings will have the same object reference so just a pointer quality check would be sufficient to say if two string literals are equal. Since this is a very common operation, this is typically implemented as a pointer equality test, using just a single machine instruction with no memory reference at all.

_String Interning reduces the memory footprint._ Instead of filling memory with redundant String objects, Python optimizes memory footprint by sharing and reusing already defined objects as dictated by the [flyweight design pattern](https://en.wikipedia.org/wiki/Flyweight_pattern).

# String Interning in Python

Just like most other modern programming languages, Python also does [String Interning](https://en.wikipedia.org/wiki/String_interning) to gain a performance boost. In Python, we can find if two objects are referring to the same in-memory object using the `is` operator. So if two string objects refer to the same in-memory object, the `is` operator yields `True` otherwise `False`.

```
>>> 'python' is 'python'
True
```

We can use this particular operator to test which all strings are interned and which are not. In CPython, String Interning is implemented through the following function, declared in [unicodeobject.h](https://github.com/python/cpython/blob/master/Include/unicodeobject.h) and defined in [unicodeobject.c](https://github.com/python/cpython/blob/master/Objects/unicodeobject.c).

```
PyAPI_FUNC(void) PyUnicode_InternInPlace(PyObject **);
```

In order to check if a String is interned, CPython implements a macro named `PyUnicode_CHECK_INTERNED`, again defined in [unicodeobject.h](https://github.com/python/cpython/blob/master/Include/unicodeobject.h). The macro suggests that the Python maintains a member named `interned` in `PyASCIIObject` structure whose value suggests if the corresponding String is interned or not.

```
#define PyUnicode_CHECK_INTERNED(op) \
    (((PyASCIIObject *)(op))->state.interned)
```

## Internals of String Interning

In CPython, the String references are stored, accessed, and managed using a Python dictionary named `interned`. This dictionary is lazily initialized upon the first String Intern invocation and holds the reference to all the interned String objects.

### Interning the String

The core function responsible for interning the String is named `PyUnicode_InternInPlace` defined in [unicodeobject.c](https://github.com/python/cpython/blob/master/Objects/unicodeobject.c) that upon invocation lazily builds the main dictionary `interned` to hold all interned strings and then registers the object into it with the key and the value both set as the same object reference. The following function snippet shows the String Interning process as implemented in Python.

```
void
PyUnicode_InternInPlace(PyObject **p)
{
    PyObject *s = *p;

    .........

    // Lazily build the dictionary to hold interned Strings
    if (interned == NULL) {
        interned = PyDict_New();
        if (interned == NULL) {
            PyErr_Clear();
            return;
        }
    }

    PyObject *t;

    // Make an entry to the interned dictionary for the
    // given object
    t = PyDict_SetDefault(interned, s, s);

    .........

    // The two references in interned dict (key and value) are
    // not counted by refcnt.
    // unicode_dealloc() and _PyUnicode_ClearInterned() take
    // care of this.
    Py_SET_REFCNT(s, Py_REFCNT(s) - 2);

    // Set the state of the string to be INTERNED
    _PyUnicode_STATE(s).interned = SSTATE_INTERNED_MORTAL;
}
```

### Cleanup of Interned Strings

The cleanup function iterates over all the Strings held in the `interned` dictionary, adjusts the reference counts of the object, and marks them as `NOT_INTERNED` allowing them to be garbage collected. Once all the strings are marked as `NOT_INTERNED`, the `interned` dictionary is cleared and deleted. The cleanup function is defined in [unicodeobject.c](https://github.com/python/cpython/blob/master/Objects/unicodeobject.c) by the name `_PyUnicode_ClearInterned`.

```
void
_PyUnicode_ClearInterned(PyThreadState *tstate)
{
    .........

    // Get all the keys to the interned dictionary
    PyObject *keys = PyDict_Keys(interned);

    .........

    // Interned Unicode strings are not forcibly deallocated;
    // rather, we give them their stolen references back
    // and then clear and DECREF the interned dict.

    for (Py_ssize_t i = 0; i < n; i++) {
        PyObject *s = PyList_GET_ITEM(keys, i);

        .........

        switch (PyUnicode_CHECK_INTERNED(s)) {
        case SSTATE_INTERNED_IMMORTAL:
            Py_SET_REFCNT(s, Py_REFCNT(s) + 1);
            break;
        case SSTATE_INTERNED_MORTAL:
            // Restore the two references (key and value) ignored
            // by PyUnicode_InternInPlace().
            Py_SET_REFCNT(s, Py_REFCNT(s) + 2);
            break;
        case SSTATE_NOT_INTERNED:
            /* fall through */
        default:
            Py_UNREACHABLE();
        }

        // marking the string to be NOT_INTERNED
        _PyUnicode_STATE(s).interned = SSTATE_NOT_INTERNED;
    }

    // decreasing the reference to the initialized and
    // access keys object.
    Py_DECREF(keys);

    // clearing the dictionary
    PyDict_Clear(interned);

    // clearing the object interned
    Py_CLEAR(interned);
}
```

## String Interning in Action

Now that we understand the internals of String Interning and Cleanup, we find out what all Strings are interned in Python. To discover the spots all we do is grep for the function invocation for `PyUnicode_InternInPlace` in the CPython source code and peek at the neighboring code. Here is a list of interesting spots where String Interning happens in Python.

### Variables, Constants, and Function Names

CPython performs String Interning on constants such as Function Names, Variable Names, String Literals, etc. Following is the snippet from [codeobject.c](https://github.com/python/cpython/blob/master/Objects/codeobject.c) that suggests that when a new `PyCode` object is created the interpreter is interning all the compile-time constants, names, and literals.

```
PyCodeObject *
PyCode_NewWithPosOnlyArgs(int argcount, int posonlyargcount, int kwonlyargcount,
                          int nlocals, int stacksize, int flags,
                          PyObject *code, PyObject *consts, PyObject *names,
                          PyObject *varnames, PyObject *freevars, PyObject *cellvars,
                          PyObject *filename, PyObject *name, int firstlineno,
                          PyObject *linetable)
{

    ........

    if (intern_strings(names) < 0) {
        return NULL;
    }

    if (intern_strings(varnames) < 0) {
        return NULL;
    }

    if (intern_strings(freevars) < 0) {
        return NULL;
    }

    if (intern_strings(cellvars) < 0) {
        return NULL;
    }

    if (intern_string_constants(consts, NULL) < 0) {
        return NULL;
    }

    ........

}
```

### Dictionary Keys

CPython also interns thee Strings which keys of any dictionary object. Upon putting an item in the dictionary the interpreter String Interning on the key against which item is stored. The following code is taken from [dictobject.c](https://github.com/python/cpython/blob/master/Objects/dictobject.c) showcasing the exact behavior.

Fun Fact: There is a comment next to the `PyUnicode_InternInPlace` function call that suggests if we really need to intern all the keys in all the dictionaries.

```
int
PyDict_SetItemString(PyObject *v, const char *key, PyObject *item)
{
    PyObject *kv;
    int err;
    kv = PyUnicode_FromString(key);
    if (kv == NULL)
        return -1;

    // Invoking String Interning on the key
    PyUnicode_InternInPlace(&kv); /* XXX Should we really? */

    err = PyDict_SetItem(v, kv, item);
    Py_DECREF(kv);
    return err;
}
```

### Attributes of any Object

Objects in Python can have attributes that can be explicitly set using `setattr` function or are implicitly set as part of Class members or as pre-defined functions on data types. CPython interns all these attribute names, so as to make lookup blazing fast. Following is the snippet of the function `PyObject_SetAttr` responsible for setting a new attribute to a Python object, as defined in the file [object.c](https://github.com/python/cpython/blob/master/Objects/object.c).

```
int
PyObject_SetAttr(PyObject *v, PyObject *name, PyObject *value)
{

    ........

    PyUnicode_InternInPlace(&name);

    ........
}
```

### Explicit Interning

Python also allows explicit String Interning through the function `intern` defined in `sys` module. When this function is invoked with any String object, the provided String is interned. Following is the code snippet from the file [sysmodule.c](https://github.com/python/cpython/blob/master/Python/sysmodule.c) that shows String Interning happening in `sys_intern_impl`.

```
static PyObject *
sys_intern_impl(PyObject *module, PyObject *s)
{

    ........

    if (PyUnicode_CheckExact(s)) {
        Py_INCREF(s);
        PyUnicode_InternInPlace(&s);
        return s;
    }

    ........
}
```

## Extra nuggets on String Interning

_Only compile-time strings are interned_. Strings that are specified during interpretation or compile-time are interned while dynamically created strings are not.

_Strings having ASCII letters and underscores are interned_. During compile time when string literals are observed for interning, [CPython](https://github.com/python/cpython/blob/master/Objects/codeobject.c) ensures that it only interns the literals matching the regular expression `[a-zA-Z0-9_]*` as they closely resemble Python identifiers.

Comments on how CPython does String Interning internally (as discussed in the [Video](https://youtu.be/QpGK69LzfpY)) can be found in [this PR](https://github.com/arpitbbhayani/cpython/pull/9%5D).

# References

- [String Interning](https://en.wikipedia.org/wiki/String_interning)
- [CPython Optimizations](https://stummjr.org/post/cpython-optimizations/)
- [Python Objects Part III: String Interning](https://medium.com/@bdov_/https-medium-com-bdov-python-objects-part-iii-string-interning-625d3c7319de)
- [The internals of Python string interning](http://guilload.com/python-string-interning/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/recursion-visualizer-python

Building a simple recursion tree visualizer for Python

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

# Building a simple recursion tree visualizer for Python

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

One programming paradigm, that is hardest to visualize, as almost every single programmer out there will agree on, is [Recursion](<https://en.wikipedia.org/wiki/Recursion_(computer_science)>). We usually use pen and paper to visualize the flow and check how recursion is behaving. But what if, this could be done programmatically, and today we address this very problem and try to come up with a simple yet effective solution.

This essay is going to be a little different from the usuals; instead of taking a look into a research paper or an algorithm, we will implement a simple and easy recursion visualizer for Python.

# Recursion Tree

Recursion helps in solving a larger problem by breaking it into smaller similar ones. The classic implementation of recursion in the world of programming is when a function invokes itself using reduced parameters while having a base terminating condition.

```
def fib(n):
  # base condition mimicking the first two numbers
  # in the sequence
  if n == 0: return 0
  if n == 1: return 1

  # every number is summation of the previous two
  return fib(n - 1) + fib(n - 2)
```

The most common problem that is solved using recursion is computing the `n`th [Fibonacci Number](https://en.wikipedia.org/wiki/Fibonacci_number). A trivial recursive Python function that spits out `n`th Fibonacci Number is as shown below

The most effective way of visualizing recursion is by drawing a recursion tree. It is very useful for visualizing what happens when a recurrence is iterated. The recursion tree for the above function `fib` for input `n = 3` is as illustrated below

![https://user-images.githubusercontent.com/4745789/102004754-74fb7980-3d39-11eb-991e-0f54fa7f20c6.png](https://user-images.githubusercontent.com/4745789/102004754-74fb7980-3d39-11eb-991e-0f54fa7f20c6.png)

# Decorating to visualize

Instead of printing an actual tree-like recursion tree, we take some liberty and print a close-enough version of it running top-down. To keep track of recursive function calls we use [Python Decorators](https://realpython.com/primer-on-python-decorators/) that essentially wraps the function allowing us to invoke statements before and after the function call.

The decorator that wraps the recursive function and prints the recursion tree is as illustrated below.

```
def recviz(fn):
    """Decorator that pretty prints the recursion tree with
       args, kwargs, and return values.
    """

    # holds the current recursion level
    recursion_level = 1

    def wrapper(*args, **kwargs):

        # we register a nonlocal recursion_level so that
        # it binds with the recursion_level variable.
        # in this case, it will bind to the one defined
        # in recviz function.
        nonlocal recursion_level

        # Generate the pretty printed function string
        fn_str = pretty_func(fn, args, kwargs)

        # Generate the whitespaces as per the recursion level
        whitespace = "   " * (recursion_level - 1)

        # Pretty print the function with the whitespace
        print(f"{whitespace} -> {fn_str}")

        # increment the recursion level
        recursion_level += 1

        # Invoke the wrapped function and hold the return value
        return_value = fn(*args, **kwargs)

        # Post function evaluation we decrease the recursion
        # level by 1
        recursion_level -= 1

        # Pretty print the return value
        print(f"{whitespace} <- {repr(return_value)}")

        # Return the return value of the wrapped function
        return return_value

    return wrapper
```

We use `recursion_level` to keep track of the current recursion level using which we decide the indentation. The value of this variable is increased every time we are about the invoke the function while it is reduced post the execution. In order to pretty-print the invoked function, we have a helper method called `pretty_func` whose implementation can be found [here](https://github.com/arpitbbhayani/recviz/blob/master/src/recviz/rec.py).

When we decorate our previously defined `fib` function and invoke it with `n = 3` we get the following output.

```
 -> fib(3)
    -> fib(2)
       -> fib(1)
       <- 1
       -> fib(0)
       <- 1
    <- 2
    -> fib(1)
    <- 1
 <- 3
```

The above output renders how recurrence is evaluated and is pretty printed to make it more human-readable. The right arrow `->` defines a function invocation while the left arrow `<-` indicates the return value post invocation.

## Publishing it on PyPI

Everything mentioned above is published in a Python Package and hosted on [PyPI](https://pypi.org/) at [pypi/recviz](https://pypi.org/project/recviz/). So in order to use this, simply install the package `recviz` like a usual Python package using `pip` and decorate the recursive function.

```
from recviz import recviz

@recviz
def fib(n):
  # base condition mimicking the first two numbers
  # in the sequence
  if n == 0: return 0
  if n == 1: return 1

  # every number is summation of the previous two
  return fib(n - 1) + fib(n - 2)

fib(3)
```

# References

- [The nonlocal statement](https://docs.python.org/3/reference/simple_stmts.html#the-nonlocal-statement)
- [Primer on Python Decorators](https://realpython.com/primer-on-python-decorators/)
- [Python Debugging with Decorators](https://paulbutler.org/2008/python-debugging-with-decorators/)
- [Recursion Trees and the Master Method](https://www.cs.cornell.edu/courses/cs3110/2012sp/lectures/lec20-master/lec20.html)
- [Easy tracing of nested function calls in Python](https://eli.thegreenplace.net/2012/08/22/easy-tracing-of-nested-function-calls-in-python)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/flajolet-martin

Flajolet Martin algorithm for approximate counting

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Flajolet Martin algorithm for approximate counting

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Measuring the number of distinct elements from a stream of values is one of the most common utilities that finds its application in the field of Database Query Optimizations, Network Topology, Internet Routing, Big Data Analytics, and Data Mining.

A deterministic count-distinct algorithm either demands a large auxiliary space or takes some extra time for its computation. But what if, instead of finding the cardinality deterministically and accurately we just approximate, can we do better? This was addressed in one of the first algorithms in approximating count-distinct introduced in the seminal paper titled [Probabilistic Counting Algorithms for Data Base Applications](http://algo.inria.fr/flajolet/Publications/FlMa85.pdf) by Philippe Flajolet and G. Nigel Martin in 1984.

In this essay, we dive deep into this algorithm and find how wittily it approximates the count-distinct by making a single pass on the stream of elements and using a fraction of auxiliary space.

# Deterministic count-distinct

The problem statement of determining count-distinct is very simple -

_Given a stream of elements, output the total number of distinct elements as efficiently as possible._

![https://user-images.githubusercontent.com/4745789/101273043-250c3800-37b8-11eb-9e0c-435e386f3529.png](https://user-images.githubusercontent.com/4745789/101273043-250c3800-37b8-11eb-9e0c-435e386f3529.png)

In the illustration above the stream has the following elements `4`, `1`, `7`, `4`, `2`, `7`, `6`, `5`, `3`, `2`, `4`, `7` and `1`. The stream has in all `7` unique elements and hence it is the count-distinct of this stream.

Deterministically computing count-distinct is an easy affair, we need a data structure to hold all the unique elements as we iterate the stream. Data structures like [Set](<https://en.wikipedia.org/wiki/Set_(abstract_data_type)>) and [Hash Table](https://en.wikipedia.org/wiki/Hash_table) suit this use-case particularly well. A simple pythonic implementation of this approach is as programmed below

```
def cardinality(elements: int) -> int:
    return len(set(elements))
```

Above deterministic approach demands an auxiliary space of `O(n)` so as to accurately measure the cardinality. But when we are allowed to approximate the count we can do it with a fraction of auxiliary space using the Flajolet-Martin Algorithm.

# The Flajolet-Martin Algorithm

The Flajolet-Martin algorithm uses the position of the rightmost set and unset bit to approximate the count-distinct in a given stream. The two seemingly unrelated concepts are intertwined using probability. It uses extra storage of order `O(log m)` where `m` is the number of unique elements in the stream and provides a practical estimate of the cardinalities.

## The intuition

Given a good uniform distribution of numbers, the probability that the rightmost set bit is at position `0` is `1/2`, probability of rightmost set bit is at position `1` is `1/2 * 1/2 = 1/4`, at position `2` it is `1/8` and so on.

![https://user-images.githubusercontent.com/4745789/101275842-e635ac80-37ce-11eb-9e00-357b966cbac6.png](https://user-images.githubusercontent.com/4745789/101275842-e635ac80-37ce-11eb-9e00-357b966cbac6.png)

In general, we can say, the probability of the rightmost set bit, in binary presentation, to be at the position `k` in a uniform distribution of numbers is

![https://user-images.githubusercontent.com/4745789/101275886-357bdd00-37cf-11eb-9bc6-346332031eb2.png](https://user-images.githubusercontent.com/4745789/101275886-357bdd00-37cf-11eb-9bc6-346332031eb2.png)

The probability of the rightmost set bit drops by a factor of `1/2` with every position from the Least Significant Bit to the Most Significant Bit.

![https://user-images.githubusercontent.com/4745789/101276356-1cc0f680-37d2-11eb-858d-3f40061988f0.png](https://user-images.githubusercontent.com/4745789/101276356-1cc0f680-37d2-11eb-858d-3f40061988f0.png)

So if we keep on recording the position of the rightmost set bit, `ρ`, for every element in the stream (assuming uniform distribution) we should expect `ρ = 0` to be `0.5`, `ρ = 1` to be `0.25`, and so on. This probability should become `0` when bit position, `b` is `b > log m` while it should be non-zero when `b <= log m` where `m` is the number of distinct elements in the stream.

Hence, if we find the rightmost unset bit position `b` such that the probability is `0`, we can say that the number of unique elements will approximately be `2 ^ b`. This forms the core intuition behind the Flajolet Martin algorithm.

## Ensuring uniform distribution

The above intuition and approximation are based on the assumption that the distribution of the elements in the stream is uniform, which cannot always be true. The elements can be sparse and dense in patches. To ensure uniformity we hash the elements using a multiplicative hash function

![https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png](https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png)

where `a` and `b` are odd numbers and `c` is the capping limit of the hash range. This hash function hashes the elements uniformly into a hash range of size `c`.

## The procedure

The procedure of the Flajolet-Martin algorithm is as elegant as its intuition. We start with defining a closed hash range, big enough to hold the maximum number of unique values possible - something as big as `2 ^ 64`. Every element of the stream is passed through a hash function that permutes the elements in a uniform distribution.

For this hash value, we find the position of the rightmost set bit and mark the corresponding position in the bit vector as `1`, suggesting that we have seen the position. Once all the elements are processed, the bit vector will have `1`s at all the positions corresponding to the position of every rightmost set bit for all elements in the stream.

Now we find the position, `b`, of the rightmost `0` in this bit vector. This position `b` corresponds to the rightmost set bit that we have not seen while processing the elements. This corresponds to the probability `0` and hence as per the intuition will help in approximating the cardinality as `2 ^ b`.

```
# Size of the bit vector
L = 64

def hash_fn(x: int):
    return (3 * x + 5) % (2 ** L)

def cardinality_fm(stream) -> int:
    # we initialize the bit vector
    vector = 0

    # for every element in the stream
    for x in skream:

        # compute the hash value bounded by (2 ** L)
        # this hash value will ensure uniform distribution
        # of elements of the stream in range [0, 2 ** L)
        y = hash_fn(x)

        # find the rightmost set bit
        k = get_rightmost_set_bit(y)

        # set the corresponding bit in the bit vector
        vector = set_bit(vector, k)

    # find the rightmost unset bit in the bit vector that
    # suggests that the probability being 0
    b = rightmost_unset_bit(vector)

    # return the approximate cardinality
    return 2 ** b
```

Although the above algorithm does a decent job of approximating count-distinct it has a huge error margin, which can be fixed by averaging the approximations with multiple hash functions. The original Flajolet-Martin algorithm also suggests that the final approximation needs a correction by dividing the approximation by the factor `ϕ = 0.77351`.

The algorithm was [run](https://github.com/arpitbbhayani/flajolet-martin/blob/master/flajolet-martin.ipynb) on a stream size of `1048` with a varying number of distinct elements and we get the following plot.

![https://user-images.githubusercontent.com/4745789/101244923-58ef4b00-372f-11eb-9193-8e9d6dc6a227.png](https://user-images.githubusercontent.com/4745789/101244923-58ef4b00-372f-11eb-9193-8e9d6dc6a227.png)

From the illustration above we see that the approximated count-distinct using the Flajolet-Martin algorithm is very close to the actual deterministic value.

A great feature of this algorithm is that the result of this approximation will be the same whether the elements appear a million times or just a few times, as we only consider the rightmost set bit across all elements and do not sample.

### Unique words in Thee Jungle Book

The algorithm was run on the text dump of [The Jungle Book](https://en.wikipedia.org/wiki/The_Jungle_Book) by Rudyard Kipling. The text was converted into a stream of tokens and it was found that the total number of unique tokens was `7150`. The approximation of the same using the Flajolet-Martin algorithm came out to be `7606` which in fact is pretty close to the actual number.

# References

- [Probabilistic Counting Algorithms for Data Base Applications](http://algo.inria.fr/flajolet/Publications/FlMa85.pdf)
- [Flajolet-Martin algorithm by Ravi Bhide](http://ravi-bhide.blogspot.com/2011/04/flajolet-martin-algorithm.html)
- [The Jungle Book by Rudyard Kipling - Project Gutenberg](https://www.gutenberg.org/files/236/236-h/236-h.htm)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/2q-cache

2Q-cache algorithm for disk-backed databases

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Algorithms](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Algorithms](/knowledge-base/database-engineering)

# 2Q-cache algorithm for disk-backed databases

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

LRU is one of the most widely used cache eviction algorithms that span its utility across multiple database systems. Although popular, it suffers from a bunch of limitations especially when it is used for managing caches in disk-backed databases like MySQL and Postgres.

In this essay, we take a detailed look into the sub-optimality of LRU and how one of its variants called 2Q addresses and improves upon it. 2Q algorithm was first introduced in the paper - [2Q: A low overhead high-performance buffer management replacement algorithm](https://www.semanticscholar.org/paper/2Q%3A-A-Low-Overhead-High-Performance-Buffer-Johnson-Shasha/5fa357b43c8351a5d8e7124429e538ad7d687abc) by Theodore Johnson and Dennis Shasha.

# LRU

The [LRU eviction algorithm](<https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)>) evicts the page from the buffer which has not been accessed for the longest. LRU is typically implemented using a [Doubly Linked List](https://en.wikipedia.org/wiki/Doubly_linked_list) and a [Hash Table](https://en.wikipedia.org/wiki/Hash_table). The intuition of this algorithm is so strong and implementation is so simple that until the early ’80s, LRU was the algorithm of choice in nearly all the systems. But as stated above, there are certain situations where LRU performs sub-optimal.

![https://user-images.githubusercontent.com/4745789/100534745-43ae8400-3238-11eb-8855-752a6ef2f3c6.png](https://user-images.githubusercontent.com/4745789/100534745-43ae8400-3238-11eb-8855-752a6ef2f3c6.png)

## Sub-optimality during DB scans

If the database table is bigger than the LRU cache, the DB process, upon scanning the table will wipe out the entire LRU cache and fill it with the pages from just one scanned table. If these pages are not referenced again, this is a total loss and the performance of the database takes a massive hit. The performance will pickup once these pages are evicted from the cache and other pages make an entry.

## Sub-optimality in evictions

LRU algorithm works with a single dimension - recency - as it removes the pages from the buffer on the basis of recent accesses. Since it does not really consider any other factor, it can actually evict a warmer page and replace it with a colder one - a page that could and would be accessed just once.

# 2Q Algorithm

2Q addresses the above-illustrated issues by introducing parallel buffers and supporting queues. Instead of considering just recency as a factor, 2Q also considers access frequency while making the decision to ensure the page that is really warm gets a place in the LRU cache. It admits only hot pages to the main buffer and tests every page for a second reference.

The golden rule that 2Q is based on is - _Just because a page is accessed once does not entitle it to stay in the buffer. Instead, it should be decided if it is accessed again then only keep it in the buffer._

Below we take a detailed look into two versions of the 2Q algorithm - simplified and improved.

## Simplified 2Q

Simplified 2Q algorithm works with two buffers: the primary LRU buffer - `Am` and a secondary FIFO buffer - `A1`. New faulted pages first go to the secondary buffer `A1` and then when the page is referenced again, it moves to the primary LRU buffer `Am`. This ensures that the page that moves to the primary LRU buffer is hot and indeed requires to be cached.

![https://user-images.githubusercontent.com/4745789/100536835-41a0f100-3249-11eb-920b-0bcaff905906.png](https://user-images.githubusercontent.com/4745789/100536835-41a0f100-3249-11eb-920b-0bcaff905906.png)

If the page residing in `A1` is never referenced again, it eventually gets discarded, implying the page was indeed cold and did not deserve to be cached. Thus this simplified 2Q provides protection against the two listed sub-optimality of the simple LRU scheme by adding a secondary buffer and testing pages for a second reference. The pseudocode for the Simplified 2Q algorithm is as follows:

```
def access_page(X: page):
    # if the page already exists in the LRU cache
    # in buffer Am
    if X in Am:
         Am.move_front(X)

    # if the page exists in secondary storage
    # and not it gets access.
    # since the page is accessed again, indicating interest
    # and long-term need, move it to Am.
    elif X in A1:
         A1.remove(X)
         Am.add_front(X)

    # page X is accessed for the first time
    else:
         # if A1 is full then free a slot.
         if A1.is_full():
             A1.pop()

         # add X to the front of the FIFO A1 queue
         A1.add_front(X)
```

Tuning Simplified 2Q buffer is difficult - if the maximum size of `A1` is too small, the test for hotness becomes too strong and if it is too large then due to memory constraint `Am` will get relatively smaller memory making the primary LRU cache smaller, eventually degrading the database performance.

The full version 2Q algorithm remediates this limitation and eliminates tuning to a massive extent without taking any hit in performance.

## 2Q Full Version

Although Simplified 2Q algorithm does a decent job there is still scope of improvement when it comes to handling common database access pattern, that suggests, a page generally receives a lot of references for a short period of time and then no reference for a long time. If a page truly needs to be cached then after it receives a lot (not just one) of references in a short span it continues to receive references and hits on regular intervals.

To handle this common database access pattern, the 2Q algorithm splits the secondary buffer `A1` into two buffers `A1-In` and `A1-Out`, where the new element always enters `A1-In` and continues to stay in `A1-In` till it gets accesses ensuring that the most recent first accesses happen in the memory.

Once the page gets old, it gets thrown off the memory but its disk reference is stored in the `A1-Out` buffer. If the page, whose reference is, residing in `A1-Out` is accessed again the page is promoted to `Am` LRU implying it indeed is a hot page that will be accessed again and hence required to be cached.

![https://user-images.githubusercontent.com/4745789/100538168-0bb53a00-3254-11eb-8f69-ddcaf8d33a84.png](https://user-images.githubusercontent.com/4745789/100538168-0bb53a00-3254-11eb-8f69-ddcaf8d33a84.png)

The `Am` buffer continues to be the usual LRU which means when any page residing in `Am` is accessed it is moved to the head and when a page is needed to be discarded the eviction happens from the tail end.

# 2Q in Postgres

Postgres uses 2Q as its cache management algorithm due to [patent issues](http://www.varlena.com/GeneralBits/96.php) with IBM. Postgres used to have [ARC](https://en.wikipedia.org/wiki/Adaptive_replacement_cache) as its caching algorithm but with IBM getting a patent over it, Postgres moved to 2Q. Postgres also claims that the performance of 2Q is similar to ARC.

# References

- [LRU - Wikipedia](<https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)>)
- [The Saga of the ARC Algorithm and Patent](http://www.varlena.com/GeneralBits/96.php)
- [2Q: A low overhead high-performance buffer management replacement algorithm](https://www.semanticscholar.org/paper/2Q%3A-A-Low-Overhead-High-Performance-Buffer-Johnson-Shasha/5fa357b43c8351a5d8e7124429e538ad7d687abc)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/israeli-queues

Israeli Queues

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Algorithms](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Algorithms](/knowledge-base/database-engineering)

# Israeli Queues

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

A queue is a data structure that holds up elements for a brief period of time until a peripheral processing system is ready to process them. The most common implementation of a queue is a FIFO queue - First In First Out - that evicts the element that was inserted the first i.e. it evicts the one that has spent the most time in the queue. There are other variations of Queues one of which is called Priority Queue.

In Priority Queue, every element is associated with a priority, usually provided by the user during enqueueing; This associated priority is used during eviction where the element with the highest priority is evicted first during dequeuing.

In this essay, we take a detailed look into a variation of Priority Queue, fondly called Israeli Queues, where the priority of the element is defined by the affinity of it with one of its “friends” in the queue. Israeli Queues were first introduced in the paper [Polling with batch service](https://pure.tue.nl/ws/files/2152975/632939.pdf) by Boxma, O. J., Wal, van der, J., & Yechiali, U in the year 2007.

# Israeli Queues

Queues in Israel are usually unorganized, due to which people tend to find their friends, who are already waiting, and instead of adhering to the usual protocol of joining at the back end, they cut through and directly join their friends. Israeli Queues mimic this behavior and hence get this [punny name](https://www.tandfonline.com/doi/abs/10.1080/15326340802427497).

![https://user-images.githubusercontent.com/4745789/99894937-fddc4380-2cac-11eb-8a73-a4dc5c490d2b.png](https://user-images.githubusercontent.com/4745789/99894937-fddc4380-2cac-11eb-8a73-a4dc5c490d2b.png)

Israeli Queues are a variation of [Priority Queues](https://en.wikipedia.org/wiki/Priority_queue) where instead of associating priority with the element to be enqueued, the priority is implicitly derived using the “friend” element and it joins right at the back end of the group that the friend belongs to. The function signature of the enqueue operation is as shown below, while other operations like `dequeue` and `peek` remains fairly similar.

```
// Enqueues the element `e`, a friend of element `f`,
// into the queue `q`.
void enqueue(israeli_queue * q, element * e, element * f);
```

## How could this help?

Every Data Structures is designed to solve a niche use case efficiently and Israeli Queues are no different as they prove to be super-efficient where one could batch and process similar elements or where the _set-up_ cost for a task is high.

Consider a system where a queue is used to hold up heterogeneous tasks and there is a single machine taking care of processing. Now if some of these tasks are similar and have a high _set-up or preparation cost_, for example downloading large metafiles, or spinning up a parallel infrastructure, or even setting up persistent connections with device farms, queuing them closer and processing them sequentially or in batch helps in reducing redundant processing and computation by promoting reuse.

## Issue of starvation

By enqueuing elements in between Israeli Queues reduces redundant processing, but by doing that it makes itself vulnerable to the classical case of starvation. Elements stuck at the rear end of the list could potentially starve for longer durations if elements having “friends” in the queue keep coming in at high frequency.

The original implementation of Israeli Queues suggests batch processing where instead of processing tasks one at a time, it processes a batch (a group of friends) in one go. This proves to be super-handy when the time required to processes a single task is much lower than the set-up cost for it.

## Implementation Guidelines

The best way to implement Israeli Queues is by using a [Doubly Linked List](https://en.wikipedia.org/wiki/Doubly_linked_list) with a bunch of pointers pointing to the head and tail of groups within it. Insertion to an existing group happens at the tail of it while if the element has no friend element, then it goes at the tail end of the list and forms its own group.

A constraint that could be added during implementation is that the friend element should always be the leader (head) element of the group. Details of the implementation could be tweaked so long the core concept remains unaltered.

# The original use case of Israeli Queues

Israeli Queues were the outcome of a problem statement dealing with Polling Systems. Polling System usually contains `N` queues `Q1`, `Q2`, …, `Qn` where the processing unit visits each queue in cyclic order processing one element at a time i.e. `Q1`, `Q2`, …, `Qn`, `Q1`, `Q2`, …, `Qn`, etc.

When the server attends a queue instead of processing just one element from it, it processes the entire batch present in the queue utilizing the setup-cost efficiently assuming that time to process an element from a queue is much lesser than the set-up cost.

# References

- [Polling with batch service](https://pure.tue.nl/ws/files/2152975/632939.pdf)
- [The Israeli Queue with priorities](http://www.math.tau.ac.il/~uriy/Papers/IQ-with-Priorities.pdf)
- [Israeli Queues: Exploring a bizarre data structure](https://rapidapi.com/blog/israeli-queues-exploring-a-bizarre-data-structure/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/1d-terrain

How video games programatically generate terrains?

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Generative Art](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Generative Art](/knowledge-base/algorithms-and-explorations)

# How video games programatically generate terrains?

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Terrains are at the heart of every Computer Game - be it Counter-Strike, Age of Empires, or even Minecraft. The virtual world that these games generate is the key to a great gaming experience. Generating terrain, manually, requires a ton of effort and hence it makes sense to auto-generate a pseudorandom terrain using some procedure. In this essay, we take a detailed look into generating pseudorandom one-dimensional terrain that is very close to real ones.

# 1D Terrain

A one-dimensional terrain is a bunch of heights defined at every point along the X-axis. An example of this could be seen in games like Flappy Bird, Galaxy, and many more. Such terrain could also be traced as the skylines of mountain ranges.

![https://user-images.githubusercontent.com/4745789/99189082-70b55e00-2785-11eb-87c0-6d95568f709a.png](https://user-images.githubusercontent.com/4745789/99189082-70b55e00-2785-11eb-87c0-6d95568f709a.png)

The illustration above shows a one-dimensional terrain and is taken as a sketch of a distant mountain range. The procedure we define should generate terrains as close to natural ones as possible.

# Generating terrain using `random` values

A naive solution to generate a random terrain is by using the `random` function for each point along the X-axis. The `random` function yields random values in the interval `[0, 1]`, and we map these values to the required height, for example `[0, 100]`. The 1D terrain generation algorithm using the default random function is thus defined as

```
import random

def mapv(v, ol, oh, nl, nh):
    """maps the value `v` from old range [ol, oh] to new range [nl, nh]
    """
    return nl + (v * ((nh - nl) / (oh - ol)))

def terrain_naive(count) -> List[float]:
    """returns the list of integers representing height at each point.
    """
    return [
        mapv(random.random(), 0, 1, 0, 100)
        for i in range(count)
    ]
```

The `mapv` function defined above comes in very handy as it helps us to map value from a source range to a target range. The terrain generation function `terrain_naive` takes in `count` as input, suggesting the number of points along the X-axis, and it returns a list of float values representing height at each of the `count` number of points along the terrain.

![https://user-images.githubusercontent.com/4745789/99189731-9db74000-2788-11eb-98d3-0138a7378d77.png](https://user-images.githubusercontent.com/4745789/99189731-9db74000-2788-11eb-98d3-0138a7378d77.png)

The above illustration shows the plot of the one-dimensional terrain using the above `terrain_naive` function. The terrain generated using this procedure has a lot of spikes and abrupt changes in height and it clearly does not mimic the terrains in the real world. Real-world terrains, although random, does not have a lot of sharp spikes, instead, the changes in height are very gradual and ensure some degree of smoothness.

In order to bring smoothness to `terrain_naive` generated terrains, we take a look at a famous estimation technique called [interpolation](https://en.wikipedia.org/wiki/Interpolation) which estimates intermediate values given a bunch of known points.

# Interpolation

Interpolation is a method of constructing new data points within a range of a discrete set of known data points. Interpolation methods estimate the intermediate data points ensuring a “smoothened” transition from one known point to another. There are several interpolation methods but we restrict our focus to [Linear](https://en.wikipedia.org/wiki/Linear_interpolation) and [Cosine](https://en.wikipedia.org/wiki/Trigonometric_interpolation) interpolation methods.

### Linear Interpolation

Linear interpolation estimates the intermediate points between the known points assuming collinearity. Thus given two known points `a` and `b`, using linear interpolation, we estimate an intermediate point `c` at a relative distance of `mu` from `a` using the function defined below

```
def linp(a, b, mu):
    """returns the intermediate point between `a` and `b`
    which is `mu` factor away from `a`.
    """
    return a * (1 - mu) + b * mu
```

The value of the parameter `mu` ranges in the interval `[0, 1]` where `0` implies the point being estimated and interpolated is at `a` while `1` implies it is at the second point `b`.

![https://user-images.githubusercontent.com/4745789/99184683-d0ead680-276a-11eb-9b4d-6c78dbf3c197.png](https://user-images.githubusercontent.com/4745789/99184683-d0ead680-276a-11eb-9b4d-6c78dbf3c197.png)

### Cosine Interpolation

Linear interpolation is not always desirable as the plot sees a lot of discontinuous and sharp transitions. When the plot is expected to be smoother, it is where the Cosine Interpolation comes in handy. Instead of assuming intermediate points are collinear with the known ones, Cosine Interpolation, plots them on a cosine curve passing through the known points, providing a much smoother transition. This is could be seen in the illustration above.

```
import math

def cosp(a, b, mu):
    """returns the intermediate point between `a` and `b`
    which is `mu` factor away from `a`.
    """
    mu2 = (1 - math.cos(mu * math.pi)) / 2
    return a * (1 - mu2) + b * mu2
```

The above code snippet computes and estimates intermediate point `c` at a relative distance of `mu` from the first point `a` using Cosine Interpolation. Note there are other interpolation methods, but we can solve all major of our use cases using these two.

# Smoothing via Interpolation

We can apply interpolation to our naively generated terrain and make transitions smoother leading to fewer spikes. In the real-world, the transitions in the terrain are gradual with peaks being widespread; we can mimic the pattern by sampling `k` points from the naive terrain which could become our desired peaks, and interpolate the rest of the points lying between them.

This should ideally help us reduce the sudden spikes and make the terrain look much closer to really like. A simple python code that outputs linearly interpolated terrain that samples every `sample` points from the naive one is as follows

```
def terrain_linp(naive_terrain, sample=4) -> List[float]:
    """Using naive terrain `naive_terrain` the function generates
    Linearly Interpolated terrain on sample data.
    """
    terrain = []

    # get every `sample point from the naive terrain.
    sample_points = naive_terrain[::sample]

    # for every point in sample point denoting
    for i in range(len(sample_points)):

        # add current peak (sample point) to terrain.
        terrain.append(sample_points[i])

        # fill in `sample - 1` number of intermediary points using
        # linear interpolation.
        for j in range(sample - 1):
            # compute relative distance from the left point
            mu = (j + 1)/sample

            # compute interpolated point at relative distance of mu
            a = sample_points[i]
            b = sample_points[(i + 1) % len(sample_points)]
            v = linp(a, b, mu)

            # add an interpolated point to the terrain terrain.append(v)

    # return the terrain
    return terrain
```

The above code snippet generates intermediate points using Linear Interpolation but we can very easily change the interpolation function to Cosine Interpolation and see the effect in action. The sampling and interpolating on a naively generated terrain for different values of sample is as shown below

![https://user-images.githubusercontent.com/4745789/99227920-e7983880-2811-11eb-9c79-bed5cad2eed3.png](https://user-images.githubusercontent.com/4745789/99227920-e7983880-2811-11eb-9c79-bed5cad2eed3.png)

For each interpolation, the 5 plots shown above are sampled for every `1`, `2`, `3`, `4`, and `5` points respectively. We can clearly see the plots of Cosine Interpolation are much smoother than Linear Interpolated ones. The technique does a good job over naive implementation but it still does not mimic what we see in the real world. To make things as close to the real-world as possible we use the concept of [Superposition](https://en.wikipedia.org/wiki/Superposition_principle).

# Superposition Sampled Terrains

Sampling and Interpolation are effective in reducing the spikes and making transitions gradual. The concern with this approach is that sudden changes are not really gone. In order to address this situation, we use the principle of Superposition upon multiple such sampled terrains.

The approach we take here is to generate `k` such terrains with different sampling frequencies and then perform a normalized weighted sum. This way we get the best of both worlds i.e smoothness from the terrain with the least sampled points and aberrations from the one with the most sampled points.

Now the only piece that remains is choosing the weights. The weights and sampling frequency varies by the power of `2`. The number of terrains to be sampled depends on the kind of terrain needed and is to be left for experimentation, but we can assume it to be 6 for most use cases.

The terrain that samples all the points should be given the least weight as it aims to contribute sudden spikes; and as we increase the sampling frequency by the power of `2` we increase the weight by the power of `2` as well. Hence for the first terrain, the scale is `0.03125` where were sample all the points `256`, the next terrain is sampled with `128` points and has a scale of `0.0625`, and so on till we reach sampled points to be `8` with scale as `1`, giving it the highest weight.

Once these terrains are generated we perform a normalized weighted sum and generate the final terrain as shown in the illustration below.

![https://user-images.githubusercontent.com/4745789/99264852-0107a780-2847-11eb-9bd1-2e2994c56028.png](https://user-images.githubusercontent.com/4745789/99264852-0107a780-2847-11eb-9bd1-2e2994c56028.png)

The illustration above shows 6 scaled sampled terrains with different sampling frequencies along with the final super-positioned terrain generated from the procedure. It is very clear that the terrain generated using this procedure is much more closer to the real world terrain than any other we have seen before. Python code that generated the above terrain is as below

```
def terrain_superpos_linp(naive_terrain, iterations=8) -> List[float]:
    """Using naive terrain `naive_terrain` the function generates
    Linearly Interpolated Superpositioned terrain that looks real world like.
    """
    terrains = []

    # holds the sum of weights for normalization
    weight_sum = 0

    # for every iteration
    for z in range(iterations, 0, -1):
        terrain = []

        # compute the scaling factor (weight)
        weight = 1 / (2 ** (z - 1))

        # compute sampling frequency suggesting every `sample`th
        # point to be picked from the naive terrain.
        sample = 1 << (iterations - z)

        # get the sample points
        sample_points = naive_terrain[::sample]

        weight_sum += weight

        for i in range(len(sample_points)):

            # append the current sample point (scaled) to the terrain
            terrain.append(weight * sample_points[i])

            # perform interpolation and add all interpolated values to
            # to the terrain.
            for j in range(sample - 1):
                # compute relative distance from the left point
                mu = (j + 1) / sample

                # compute interpolated point at relative distance of mu
                a = sample_points[i]
                b = sample_points[(i + 1) % len(sample_points)]
                v = linp(a, b, mu)

                # add interpolated point (scaled) to the terrain
                terrain.append(weight * v)

        # append this terrain to list of terrains preparing
        # it to be superpositioned.
        terrains.append(terrain)

    # perform super position and normalization of terrains to
    # get the final terrain
    return [sum(x)/weight_sum for x in zip(*terrains)]
```

If the terrain to be generated is needed to be smoother then instead of using Linear Interpolation switch to Cosine Interpolation and the resultant terrain will be much smoother and curvier as seen in the illustration below.

![https://user-images.githubusercontent.com/4745789/99264869-06fd8880-2847-11eb-83d7-80d0ab5509da.png](https://user-images.githubusercontent.com/4745789/99264869-06fd8880-2847-11eb-83d7-80d0ab5509da.png)

> This approach is very similar to [Perlin Noise](https://en.wikipedia.org/wiki/Perlin_noise) that is used for generating multi-dimensional terrains. [Ken Perlin](https://en.wikipedia.org/wiki/Ken_Perlin) was awarded an Academy Award for Technical Achievement for creating the algorithm.

# References

- [Superposition](https://en.wikipedia.org/wiki/Superposition_principle)
- [Interpolation Methods](http://paulbourke.net/miscellaneous/interpolation/)
- [One Lone Coder - Perlin-like Noise](https://github.com/OneLoneCoder/videos/blob/master/OneLoneCoder_PerlinNoise.cpp)
- [IPython notebook with the source code](https://github.com/arpitbbhayani/1d-terrain/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/jaccard-minhash

Quantifying set similarity using Jaccard similarity coefficient and MinHash

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Quantifying set similarity using Jaccard similarity coefficient and MinHash

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Set similarity measure finds its application spanning the Computer Science spectrum; some applications being - user segmentation, finding near-duplicate webpages/documents, clustering, recommendation generation, sequence alignment, and many more. In this essay, we take a detailed look into a set-similarity measure called - Jaccard’s Similarity Coefficient and how its computation can be optimized using a neat technique called MinHash.

# Jaccard Similarity Coefficient

Jaccard Similarity Coefficient quantifies how similar two _finite_ sets really are and is defined as the size of their intersection divided by the size of their union. This similarity measure is very intuitive and we can clearly see that it is a real-valued measure bounded in the interval `[0, 1]`.

![https://user-images.githubusercontent.com/4745789/98461673-302d7180-21d4-11eb-9722-41f473c1fe84.png](https://user-images.githubusercontent.com/4745789/98461673-302d7180-21d4-11eb-9722-41f473c1fe84.png)

The coefficient is `0` when the two sets are mutually exclusive (disjoint) and it is `1` when the sets are equal. Below we see the one-line python function that computes this similarity measure.

```
def similarity_jaccard(a: set, b: set) -> float:
    return len(a.intersection(b)) / len(a.union(b))
```

## Jaccard Similarity Coefficient as Probability

Jaccard Coefficient can also be interpreted as the probability that an element picked at random from the universal set `U` is present in both sets `A` and `B`.

![https://user-images.githubusercontent.com/4745789/98462221-8dc3bd00-21d8-11eb-95bf-5a9267e88b97.png](https://user-images.githubusercontent.com/4745789/98462221-8dc3bd00-21d8-11eb-95bf-5a9267e88b97.png)

Another analogy for this probability is the chances of throwing a dart and it hitting the intersection. Thus we see how we can transform the Jaccard Similarity Coefficient into a simple probability statement. This will come in very handy when we try to optimize the computation at scale.

## Problem at Scale

Computing Jaccard Similarity Coefficient is very simple, all we require is a union operation and an intersection operation on the participating sets. But these computations go haywire when things run at scale.

Computing set similarity is usually a subproblem fitting in a bigger picture, for example, near-duplicate detection which finds near-duplicate articles across millions of documents. When we tokenize the documents and apply raw Jaccard Similarity Coefficient for every two combinations of documents we find that the computation will take [years](https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/).

Instead of finding the true value for this coefficient, we can rely on an approximation if we can get a considerable speedup and this is where a technique called MinHash fits well.

# MinHash

MinHash algorithm gives us a fast approximation to the Jaccard Similarity Coefficient between any two finite sets. Instead of computing the unions and the intersections every single time, this method once creates _MinHash Signature_ for each set and use it to approximate the coefficient.

## Computing single MinHash

MinHash `h` of the set `S` is the index of the first element, from a permuted Universal Set, that is present in the set `S`. But since permutation is a computation heavy operation especially for large sets we use a hashing/mapping function that typically reorders the elements using simple math operation. One such hashing function is

![https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png](https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png)

If `u` is the total number of elements in the Universal Set `U` then `a` and `b` are the random integers less than `u` and `c` is the prime number slightly higher than `u`. A sample permute function could be

```
def permute_fn(x: int) -> int:
    return (23 * x + 67) % 199
```

Now that we have defined permutation as a simple mathematical operation that spits out the new row index, we can find MinHash of a set as the element that has the minimum new row number. Hence we can define the MinHash function as

```
def minhash(s: set) -> int:
    return min([permute_fn(e) for e in s])
```

## A surprising property of MinHash

MinHash has a surprising property, according to which, the probability that the MinHash of random permutation produces the same value for the two sets equals the Jaccard Similarity Coefficient of those sets.

![https://user-images.githubusercontent.com/4745789/98463732-8229c380-21e3-11eb-9b26-04ec08bc8753.png](https://user-images.githubusercontent.com/4745789/98463732-8229c380-21e3-11eb-9b26-04ec08bc8753.png)

The above equality holds true because the probability of MinHash of two sets to be the same is the number of elements present in both the sets divided by the total number of elements in both the sets combined; which in fact is the definition of Jaccard Similarity Coefficient.

Hence to approximate Similarity Coefficient using MinHash all we have to do is find the Probability of MinHash of two sets to be the same, and this is where the MinHash Signature comes in to play.

## MinHash Signature

MinHash Signature of a set `S` is a collection of `k` MinHash values corresponding to `k` different MinHash functions. The size `k` depends on the error tolerance, keeping it higher leads to more accurate approximations.

```
def minhash_signature(s: set):
    return [minhash(s) for minhash in minhash_fns]
```

> MinHash functions usually differ in the permutation parameters i.e. coefficients `a`, `b` and `c`.

Now in order to compute `Pr[h(A) = h(B)]` we have to compare the MinHash Signature of the participating sets `A` and `B` and find how many values in their signatures match; dividing this number by the number of hash functions `k` will give the required probability and in turn an approximation of Jaccard Similarity Coefficient.

```
def similarity_minhash(a: set, b: set) -> float:
    sign_a = minhash_signature(a)
    sign_b = minhash_signature(b)
    return sum([1 for a, b in zip(sign_a, sign_b) if a == b]) / len(sign_a)
```

> MinHash Signature could well be computed just once per set.

Thus to compute set similarity, we need not perform heavy computation like Union and Intersection and that too across millions of sets at scale, rather we can simply compare `k` items of in their signatures and get a fairly good estimate of it.

# How good is the estimate?

In order to find how close the estimate is we compute the Jaccard Similarity Coefficient and its approximate using MinHash on two disjoint sets having equal cardinality. One of the sets will undergo a transition where one element of it will be replaced with one element of the other set. So with time, the sets will go from disjoint to being equal.

![https://user-images.githubusercontent.com/4745789/98465023-860e1380-21ec-11eb-8813-7cb6920bc1fd.png](https://user-images.githubusercontent.com/4745789/98465023-860e1380-21ec-11eb-8813-7cb6920bc1fd.png)

The illustration above shows the two plots and we can clearly see that the MinHash technique provides a fairly good estimate of Jaccard Similarity Coefficient with much fewer computations.

# References

- [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index)
- [MinHash Wikipedia](https://en.wikipedia.org/wiki/MinHash)
- [Using of Jaccard Coefficient for Keywords Similarity](https://www.researchgate.net/profile/Ekkachai_Naenudorn/publication/317248581_Using_of_Jaccard_Coefficient_for_Keywords_Similarity/links/592e560ba6fdcc89e759c6d0/Using-of-Jaccard-Coefficient-for-Keywords-Similarity.pdf)
- [MinHash Tutorial with Python Code](https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/ts-smoothing

Smooth out time-series data using Kurtosis

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Smooth out time-series data using Kurtosis

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Time series is a collection of numerical data points (often measurements), gathered in discrete time intervals, and indexed in order of the time. Common examples of time series data are CPU utilization metrics, Temperature of some geolocation, New User Signups of a product, etc.

Observing time-series of critical metrics helps in spotting trends, aberrations, and anomalies. Time series forecasting helps in predicting future demand and thus aids in altering and adjusting the supply to match that. Software companies continuously monitor hundreds of time series plots for anomalies that, if unattended, could result in downtime or a loss in revenue.

Unfortunately, time series data have a lot of short-term irregularities, often making it harder for the observer to spot the sudden spikes and true anomalies; and which is where the need for _smoothing_ arises. By smoothing the plot we get rid of the irregularities, to some extent, while enabling the observer to clearly see the patterns, trends, and anomalies.

In this essay, we take a detailed look into how we can optimally smooth the time series data to prioritize the user’s attention i.e. making it easier for the observer to spot the aberrations. The approach we discuss was introduced in the paper [ASAP: Automatic Smoothing for Attention Prioritization in Streaming Time Series Visualization](https://arxiv.org/abs/1703.00983) by Kexin Rong, Peter Bailis.

# Time Series and need of Smoothing

Time Series, more often than not, is very irregular in nature. Below is the plot of India’s Average Temperature - Monthly since 1870. We can clearly see the plot being very irregular making it harder for us to deduce any information out of it whatsoever. Probably the only fact we can point out is that the Monthly Average temperature in India is always between 15 - 30 degrees celsius, which everyone can agree, is not that informative enough.

![https://user-images.githubusercontent.com/4745789/94363195-3cef7d80-00de-11eb-9280-cf0ab83f2230.png](https://user-images.githubusercontent.com/4745789/94363195-3cef7d80-00de-11eb-9280-cf0ab83f2230.png)

In order to make sense of such an irregular plot and find a pattern or a trend out of it, we have to get rid of **short-term irregularities** without substantial information loss; and this process is called “smoothing”. Aggregation doesn’t work well here because it will not only hide the anomaly but will also reduce the data density making the resultant plot sparse; hence in order to spot anomalies and see long-term trends smoothing is preferred.

If we smooth the above raw plot using one of the simplest techniques out there, we get the following plot which, everyone would agree, not only looks cleaner but it also clearly shows us the long-term trend while being rich in information.

![https://user-images.githubusercontent.com/4745789/94363189-32cd7f00-00de-11eb-9012-773b42105020.png](https://user-images.githubusercontent.com/4745789/94363189-32cd7f00-00de-11eb-9012-773b42105020.png)

# Time Series Smoothing using Moving Average

The technique we used to smooth the temperature plot is known as [Simple Moving Average (SMA)](https://en.wikipedia.org/wiki/Moving_average) and it is the simplest, most effective, and one of the most popular smoothing techniques for time series data. Moving Average, very instinctively, smooths out short-term irregularities and highlights longer-term trends and patterns. Computing it is also very simple - each point in the smoothened plot is just an unweighted mean of the data points lying in the sliding window of length `n`. Because of the Sliding Window, SMA ensures that there is no substantial loss of data resolution in the smoothened plot.

![https://user-images.githubusercontent.com/4745789/94834298-d3e56e00-042d-11eb-8c1d-1b339478a7c9.png](https://user-images.githubusercontent.com/4745789/94834298-d3e56e00-042d-11eb-8c1d-1b339478a7c9.png)

We apply SMA, with window length `11`, to another time series plot and we clearly find the smoothened plot to be visually cleaner with fewer short-term irregularities.

![https://user-images.githubusercontent.com/4745789/94832462-8f58d300-042b-11eb-8d39-f9a12e441519.png](https://user-images.githubusercontent.com/4745789/94832462-8f58d300-042b-11eb-8d39-f9a12e441519.png)

# Making Aberrations Stand Out

When an observer is looking at the plot, the primary motive is to spot any aberrations and anomalies. If the plot has irregularities (i.e. it is not smooth enough), spotting anomalies or aberrations becomes tough, and hence smoothing plays a vital role here.

Simple Moving Average is a very effective smoothing technique but choosing the optimal window size is a challenge. Picking a smaller window size will not help in getting rid of irregularities while picking the window size that is too large will mask all the anomalies.

![https://user-images.githubusercontent.com/4745789/94897527-76910180-04ad-11eb-92ab-d38574428dbe.png](https://user-images.githubusercontent.com/4745789/94897527-76910180-04ad-11eb-92ab-d38574428dbe.png)

From the over-smoothened plot illustrated above it is clear that having a large window size leads to a heavy information loss and in most cases hides the anomalies and aberrations. Hence we reduce our problem statement to _find the optimal window size for a given plot such that we make anomalies and aberrations standout_.

## Aberrations and Anomalies

In any data distribution, the anomalies and aberrations form in the long tail which means they are some extreme values that are far away from the mean. Being part of the long tail makes these anomalies - outliers i.e. data points that do not really fit the distribution.

Hence in order to find out optimal window size that gets rid of short-term irregularities but makes anomalies stand out, we have to make the resultant distribution “tail heavy” implying the presence of anomalies. This is exactly where [Kurtosis](https://en.wikipedia.org/wiki/Kurtosis) - a famous concept from Statistics comes into the picture.

## Kurtosis

[Kurtosis](https://en.wikipedia.org/wiki/Kurtosis) is the measure of “tailedness” of the probability distribution (data distribution) and it helps in describing the shape of the plot. Kurtosis is the fourth standardized moment and is defined as

![https://user-images.githubusercontent.com/4745789/94909588-0a1ffd80-04c1-11eb-9b7d-c89bf9dbfb39.png](https://user-images.githubusercontent.com/4745789/94909588-0a1ffd80-04c1-11eb-9b7d-c89bf9dbfb39.png)

The high value of kurtosis implies that the distribution is heavy on either tail and this is evident when we compute Kurtosis of various distributions with and without any tail noise - mimicking anomalies.

![https://user-images.githubusercontent.com/4745789/94403183-ac22ab80-018a-11eb-9bca-72f6b2e5f98e.png](https://user-images.githubusercontent.com/4745789/94403183-ac22ab80-018a-11eb-9bca-72f6b2e5f98e.png)

In the illustration above, a small variation (anomaly) is added to the tail of the individual distribution and is encircled in red; and we can clearly see that even a tiny tailedness (anomaly and aberration) that makes the distribution deviate from the mean has a heavy impact on the Kurtosis, making it go much higher.

## Finding the Optimal Window Size

As established earlier, anomalies and aberrations are extreme values that largely deviate from the mean and hence occupy a position on either tail of the distribution. Hence in order to find the optimal window size that neither under-smooths nor over-smooths the plot while ensuring that it makes anomalies and aberrations stand out, we need to **find the window size that maximizes the Kurtosis**.

```
from scipy.stats import kurtosis

optimal_window, max_kurt = 1, kurtosis(raw_plot)

for window_size in range(2, len(raw_plot), 1):
    # we get the smoothened plot from the `raw_plot` by applying
    # Simple Moving Average for a window of length `window_size`
    smoothened_plot = moving_average_plot(raw_plot, window=window_size)

    # measure the kurtosis of the smoothened_plot
    kurt = kurtosis(smoothened_plot)

    # if kurtosis of the current smoothened plot is greater than the
    # max we have seen, then we update the optimal window the max_kurt
    if kurt > max_kurt:
        max_kurt, optimal_window = kurt, window_size
```

The pseudocode above computes the optimal window size that maximizes the Kurtosis and in turn ensuring that the smoothened plot has a heavy tail, making anomalies and aberrations stand out.

Finding the global optimal window size, that maximizes Kurtosis, is not always a good idea, because doing so can totally distort the plot leading to heavy information loss. A better way is to find local optimum within pre-defined limits; for example, an optimal point for window size between 10 and 40. These limits totally depend on the data at hand. Doing this not only leads to a smooth plot that highlights anomalies but also converges the computation to a local optimum much quicker.

# References

- [Kurtosis - Wikipedia](https://en.wikipedia.org/wiki/Kurtosis)
- [Moving Average - Wikipedia](https://en.wikipedia.org/wiki/Moving_average)
- [ASAP: Automatic Smoothing for Attention Prioritization in Streaming Time Series Visualization](https://arxiv.org/abs/1703.00983)
- [Climate Change Earth Surface Temperature Data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/lfu

Constant time implementation of the LFU cache eviction algorithm

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Algorithms](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Algorithms](/knowledge-base/database-engineering)

# Constant time implementation of the LFU cache eviction algorithm

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

A common strategy to make any system super-performant is _[Caching](<https://en.wikipedia.org/wiki/Cache_(computing)>)._ Almost all software products, operating at scale, have multiple layers of caches in their architectures. Caching, when done right, does wonder to the response time and is one of the main reasons why products work so well at a massive scale. Cache engines are limited by the amount of memory available and hence once it gets full the engine has to decide which item should be evicted and that is where an eviction algorithm, like [LFU](https://en.wikipedia.org/wiki/Least_frequently_used) and [LRU](<https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)>). kicks in.

[LRU](<https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)>) (Least Recently Used) cache eviction strategy is one of the most popular strategies out there. LFU (Least Frequently Used) strategy fares well in some use cases but a concern with its most popular implementation, where it uses a min-heap, is that it provides a running time complexity of `O(log n)` for all the three operations - insert, update and delete; because of which, more often than not, it is replaced with a sub-optimal yet extremely fast `O(1)` LRU eviction scheme.

In this essay, we take a look at Constant Time LFU implementation based on the paper [An O(1) algorithm for implementing the LFU cache eviction scheme](http://dhruvbird.com/lfu.pdf) by Prof. Ketan Shah, Anirban Mitra and Dhruv Matani, where instead of using a min-heap, it uses a combination of [doubly-linked lists](https://en.wikipedia.org/wiki/Doubly_linked_list) and [hash table](https://en.wikipedia.org/wiki/Hash_table) to gain a running time complexity of `O(1)` for all the three core operations.

# The LFU cache eviction strategy

LFU, very commonly, is implemented using a [min-heap](https://en.wikipedia.org/wiki/Min-max_heap) which is organized as per the frequency of access of each element. Each element of this heap holds a pair - cached value and the access frequency; and is structured in order of this frequency such that the cached value with the minimum access frequency sits at the top, making it quick to identify the element to be evicted.

![min-heap LFU](https://user-images.githubusercontent.com/4745789/89717235-0fd1f900-d9d2-11ea-968d-9ed67f52a2db.png)

Although the identification of the element to be evicted is quick, but in order for the heap to maintain its property - element with lowest access frequency be at the top - it demands a rebalance, and this rebalancing process has a running complexity of `O(log n)`. To make things worse, rebalancing is required every single time the frequency of an item is changed; which means that in the cache that implements LFU, every time an item is either inserted, accessed or evicted, a rebalance is required - making all the three core operations to have the time complexity of `O(log n)`.

# Constant time implementation

The LFU cache can be implemented with `O(1)` complexity for all the three operations by using one [Hash Table](https://en.wikipedia.org/wiki/Hash_table) and a bunch of [Doubly Linked Lists](https://en.wikipedia.org/wiki/Doubly_linked_list). As stated by the [RUM Conjecture](https://arpitbhayani.me/blogs/rum), in order to get a constant time reads and updates operations, we have to make a compromise with memory utilization. This is exactly what we observe in this implementation.

## The Hash Table

The Hash Table stores the mapping of the cached key to the Value Node holding the cached value. The value against the key is usually a pointer to the actual Value Node. Given that the lookup complexity of the hash table is `O(1)`, the operation to access the value given the key from this Hash Table could be accomplished in constant time.

![LFU hash table](https://user-images.githubusercontent.com/4745789/90469594-e2561f80-e136-11ea-9ff4-8369a7ea3df3.png)

The illustration above depicts that the Hash Table holding cache keys `k1`, `k2`, etc are mapped to the nodes holding the values `v1` and `v2` through direct pointers. The nodes are allocated on the heap using dynamic allocation, hence are a little disorganized. The Value Node to which the key maps to, not only hold the cached value, but it also holds a few pointers pointing to different entities in the system, enabling constant-time operations.

## Doubly Linked Lists

This implementation of LFU requires us to maintain one Doubly Linked List of frequencies, called `freq_list`, holding one node for each unique frequency spanning all the caches values. This list is kept sorted on the frequency the node represents such that, the node representing the lowest frequency is on the one end while the node representing the highest frequency is at the other.

Every Frequency Node holds the frequency that it represents in the member `freq` and the usual `next` and `prev` pointers pointing to the adjacent Frequency Nodes; it also keeps a `values_ptr` which points to another doubly-linked list holding Value Nodes (referred in the hash table) having the same access frequency `freq`.

![lists LFU](https://user-images.githubusercontent.com/4745789/90469593-e08c5c00-e136-11ea-995b-e4590981dd89.png)

The overall schematic representation of doubly-linked lists and its arrangement is as shown in the illustration above. The doubly-linked list holding Frequency Nodes is arranged horizontally while the list holding the Value Nodes is arranged vertically, for clearer view and understanding.

Since the cached values `v1` and `v7` both have been accessed `7` times, they both are chained in a doubly-linked list and are hooked with the Frequency Node representing the frequency of `7`. Similarly, the Value Nodes holding values `v5`, `v3`, and `v9` are chained in another doubly-linked list and are hooked with the Frequency Node representing the frequency of `18`.

The Value Node contains the cached value in member `data`, along with the usual `next` and `prev` pointers pointing to the adjacent Value Nodes in the list. It also holds a `freq_pointer` pointing back to the Frequency Node to which the list if hooked at. Having all of these pointers helps us ensure all the three operations happen in constant time.

Now that we have put all the necessary structures in place, we take a look at the 3 core operations along with their pseudo implementation.

## Adding value to the cache

Adding a new value to the cache is a relatively simpler operation that requires a bunch of pointer manipulations and does the job with a constant time running complexity. While inserting the value in the cache, we first check the existence of the key in the table, if the key is already present and we try to put it again the function raises an error. Then we ensure the presence of the Frequency Node representing the frequency of `1`, and in the process, we might also need to create a new frequency node also. Then we wrap the value in a Value Node and adds it to the `values_list` of this Frequency Node; and at last, we make an entry in the table acknowledging the completion of the caching process.

```
def add(key: str, value: object):
    # check if the key already present in the table,
    # if so then return the error
    if key in table:
        raise KeyAlreadyExistsError

    # create the Value Node out of value
    # holding all the necessary pointers
    value_node = make_value_node(value)

    first_frequency_node = freq_list.head
    if first_frequency_node.freq != 1:
        # since the first node in the freq_list does not represent
        # frequency of 1, we create a new node
        first_frequency_node = make_frequency_node(1)

        # update the `freq_list` that holds all the Frequency Nodes
        # such that the first node represents the frequency 1
        # and other list stays as is.
        first_frequency_node.next = freq_list.head
        freq_list.head.prev = first_frequency_node
        freq_list.head = first_frequency_node

    # Value Node points back to the Frequency Node to which it belongs
    value_node.freq_pointer = first_frequency_node

    # add the Value Node in `first_frequency_node`
    first_frequency_node.values.add(value_node)

    # update the entry in the hash table
    table[key] = value_node
```

As seen in the pseudocode above the entire procedure to add a new value in the cache is a bunch of memory allocation along with some pointer manipulations, hence we observe that the running complexity of `O(1)` for this operation.

## Evicting an item from the cache

Eviction, similar to insertion, is a trivial operation where we simply pick the frequency node with lowest access frequency (the first node in the `freq_list`) and remove the first Value Node present in its `values_list`. Since the entire eviction also requires pointer manipulations, it also exhibits a running complexity of `O(1)`.

```
def evict():
    if freq_list.head and freq_list.head.values:
        first_value_node = freq_list.head.values.first
        second_value_node = first_value_node.next

        # make the second element as first
        freq_list.head = second_value_node

        # ensure second node does not have a dangling prev link
        second_value_node.prev = None

        # delete the first element
        delete_value_node(first_value_node)

    if freq_list.head and not freq_list.head.values:
        # if the Frequency Node after eviction does not hold
        # any values we get rid of it
        delete_frequency_node(freq_list.head)
```

## Getting a value from the cache

Accessing an item from the cache has to be the most common operation of any cache. In the LFU scheme, before returning the cached value, the engine also has to update its access frequency. Ensuring the change in access frequency of one cached value does not require some sort of rebalancing or restructuring to maintain the integrity, is what makes this implementation special.

The engine first makes a get call to the Hash Table to check that the key exists in the cache. Before returning the cached value from the retrieved Value Node, the engine performs the following operations - it accesses the Frequency Node and its sibling corresponding to the retrieved Value Node. It ensures that the frequency of the sibling is 1 more than that of the Frequency Node; if not it creates the necessary Frequency Node and place it as the new sibling. The Value Node then changes its affinity to this sibling Frequency Node so that it correctly matches the access frequency. In the end, the back pointer from the Value Node to the new Frequency Node is set and the value is returned.

```
def get(key: str) -> object:
    # get the Value Node from the hash table
    value_node = table.get(key, None)

    # if the value does not exist in the cache then return an error
    # stating Key Not Found
    if not value_node:
        raise KeyNotFoundError

    # we get the Frequency Node from the Value Node using the
    # freq_pointer member.
    frequency_node = value_node.freq_pointer

    # we also get the next Frequency Node to the current
    # Frequency Node so that we could make a call about
    # the need to create a new node.
    next_frequency_node = frequency_node.next

    if next_frequency_node.freq != frequency_node.freq + 1:
        # create a new Frequency Node
        new_frequency_node = make_frequency_node(frequency_node.freq + 1)

        # place the Frequency Node at the correct position in the list
        frequency_node.next = new_frequency_node
        new_frequency_node.prev = frequency_node

        next_frequency_node.prev = new_frequency_node
        new_frequency_node.next = frequency_node

        # going forward we call the new Frequency Node as next
        # because it represents the the next Frequency Node
        next_frequency_node = new_frequency_node

    # we add the Value Node in the nex
    next_frequency_node.values.add(value_node)

    # we change the parent and adjecent nodes to this Value Nodes
    value_node.freq_pointer = next_frequency_node
    value_node.next = None
    value_node.prev = next_frequency_node.values.last

    # if the Frequency Node has no elements then deleting the node
    # so as to avoid the memory leak
    if len(frequency_node.values) == 0:
        delete_frequency_node(frequency_node)

    # returning the value
    return value_node.value
```

Again, since this operation also only deals with pointer manipulations through direct pointers, the running time complexity of this operation is also constant time. Thus we see the Constant Time LFU implementation where the necessary time complexity is achieved by using Hash Tables and Doubly-Linked Lists.

# References

- [An O(1) algorithm for implementing the LFU cache eviction scheme](http://dhruvbird.com/lfu.pdf)
- [When and Why to use a Least Frequently Used (LFU) cache with an implementation in Golang](https://ieftimov.com/post/when-why-least-frequently-used-cache-implementation-golang/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/morris-counter

Morris's algorithm for approximate counting

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Morris's algorithm for approximate counting

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Approximate Counting algorithms are techniques that allow us to count a large number of events using a very small amount of memory. It was invented by [Robert Morris](https://en.wikipedia.org/wiki/Robert_Tappan_Morris) in 1977 and was published through his paper [Counting large number of events in small registers](http://www.inf.ed.ac.uk/teaching/courses/exc/reading/morris.pdf). The algorithm uses probabilistic techniques to increment the counter, although it does not guarantee the exactness it does provide a fairly good estimate of the true value while inducing a minimal and yet fairly constant relative error. In this essay, we take a detailed look at Morris’ Algorithm and the math behind it.

Robert Morris, while working at Bell Laboratories, had a problem at hand; he was supposed to write a piece of code that counts a large number of events and all he had was one 8-bit counter. Since the number of events easily crossed 256, counting them was infeasible using ordinary methods and this constraint led him to build this approximate counter that instead of providing the exact count, does a pretty good job providing an approximate one.

# Counting and coin flips

A very simple solution, to build an approximate counter, is to count every alternate event. In order to make the decision of whether to count an event or not - we use a coin flip, which means, every time we see a new event, we flip a coin and if it lands _heads_ we increase the count otherwise we don’t. This way the value preset in our counter register will, on average, represent half of the total events. When we multiply the count (in the register) by 2, we get a good approximation of the actual number of events.

This coin flip based counting technique is a Binomial Distribution with parameters `(n, p)` where `n` is the total number of events seen and `p` is the success probability i.e. probability of getting _heads_ during a coin flip. The expected value `v` in the counter register corresponding to the number of events `n` is given by

![value in coin-flip counter](https://user-images.githubusercontent.com/4745789/89116526-a6725780-d4b2-11ea-9143-a562d6c3ca93.png)

The standard deviation of this binomial distribution will help us find the error in our measurement i.e. value retrieved using our counter vs the actual number of the events seen. For a binomial distribution twice the standard deviation on either side of the mean covers **95%** of the distribution; and we use this to find the relative and absolute error in our counter value as illustrated below.

![coin-flip binomial distribution](https://user-images.githubusercontent.com/4745789/89117327-82b30f80-d4ba-11ea-8346-a39ae6cb639a.png)

As illustrated in the image we can say that if the counter holds the value `200`, the closest approximate to the actual number of events that we can make is `2 * 200` = `400`. Even though `400` might not be the actual number of events seen, we can say, with **95%** confidence, that the true value lies in the range `[180, 220]`.

With this coin flip based counter, we have actually doubled our capacity to count and have also ensured that our memory requirements stay constant. On an 8-bit register, ordinarily, we would have counted till 256, but with this counter in place, we can approximately count till 512.

This approach can be extended to count even larger numbers by changing the value of `p`. The absolute error observed here is small but the relative error is very high for smaller counts and hence this creates a need for a technique that has a near-constant relative error, something that is independent of `n`.

# The Morris’ Algorithm

Instead of keeping track of the total number of events `n` or some constant multiple of `n`, Morris’ algorithm suggests that the value we store in the register is

![value in counter Morris' algorithm](https://user-images.githubusercontent.com/4745789/89117993-edb31500-d4bf-11ea-9879-1f0032950ff4.png)

Here we try to exploit the core property of _logarithm_ - _the growth of logarithmic function is inverse of the exponential_ - which means the value `v` will grow faster for the smaller values of `n` - providing better approximations. This ensures that the relative error is near-constant i.e. independent of `n` and it does not matter if the number of events is fewer or larger.

Now that we have found a function that suits the needs of a good approximate counter, it is time we define what exactly would happen to the counter when we see a new event.

## Incrementing the value

Since we are building a counter, all we know is the value of the counter `v` and have no knowledge of the actual number of events seen. So when the new event comes in we have to decide if `v` needs to change or not. Given the above equation, our best estimate of `n` given `v` can be computed as

![estimate n](https://user-images.githubusercontent.com/4745789/89120289-c7e33b80-d4d2-11ea-92b8-d307b0aa9032.png)

Now that we have seen a new event we want to find the new value `v` for the counter. This value should always, on an average, converge to `n + 1` and we find it as

![next value](https://user-images.githubusercontent.com/4745789/89120467-2eb52480-d4d4-11ea-89ac-d8cacd14d952.png)

Since the value computed using the above method is generally not an integer, performing either round-up or round-down every time will induce a serious error in counting.

For us to determine if we should increment the value of `v` or not, we need to find the cost (inaccuracy) that we might incur if we made an incorrect call. We establish a heuristic that if the change in the value of `n` by change in `v` is huge, we should have a lower probability of making an increment to `v` and vice versa. We this define `d` to be reciprocal of this jump i.e. difference between `n` corresponding to `v + 1` and `v`.

![defining d](https://user-images.githubusercontent.com/4745789/89120957-38d92200-d4d8-11ea-975f-323b36da325c.png)

The value of `d` will always be in the interval `(0, 1)` . Smaller the jump between two `n`s larger will be the value of `d` and larger the jump, smaller will be the value of `d`. This also implies that as `n` grows the value of `d` will become smaller and smaller making it harder for us to make the change in `v`.

So we pick a random number `r` uniformly generated in the interval `[0, 1)` and using this random number `r` and previously defined `d` we state that if this `r` is less than `d` increase the counter `v` otherwise, we keep it as is. As `n` increases, `d` decreases making it tougher for the odds of pick `r` in the range `[0, d)`.

![defiing d 2](https://user-images.githubusercontent.com/4745789/89120929-f9aad100-d4d7-11ea-8f0e-066fc059c066.png)

The proof that the expected value of `n`, post this probabilistic decision, is `n + 1` can be found in the paper - [Counting large number of events in small registers](http://www.inf.ed.ac.uk/teaching/courses/exc/reading/morris.pdf). After tweaking some parameters and making process stabler the function that Morris came up was

![morris function v](https://user-images.githubusercontent.com/4745789/89121058-3fb46480-d4d9-11ea-9d93-5af712ac08e7.png)

When we plot values produced by Morris’ Algorithm vs the actual number of events we find that Morris’ algorithm indeed generates better approximate values to smaller values of `n` but as `n` increases the absolute error grows but the relative error remains fairly constant. The illustrations shown below describe these facts.

![Morris comparison](https://user-images.githubusercontent.com/4745789/89123322-13eeaa00-d4ec-11ea-9539-ada7f5de9af1.png)

> _Python based implementation of Morris’ algorithm can be found at [github.com/arpitbbhayani/morris-counter](https://github.com/arpitbbhayani/morris-counter/blob/master/morris-counter.ipynb)_

# Space Complexity

In order to count till `n` the Morris’ algorithm requires the counter to go up to `log(n)` and hence the number of bits required to count from `0 to log(n)` ordinarily is `log(log(n))` and hence we say that the space complexity of this technique is `O(log log n)`. Morris’ algorithm thus provides a very efficient way to manage cardinalities where we can afford to have approximations.

# References

- [Approximate Counting Algorithm - Wikipedia](https://en.wikipedia.org/wiki/Approximate_counting_algorithm)
- [Approximate Counting with Morris’s Algorithm](http://gregorygundersen.com/blog/2019/11/11/morris-algorithm/)
- [Counting large number of events in small registers](http://www.inf.ed.ac.uk/teaching/courses/exc/reading/morris.pdf)
- [Probabilistic Counting and Morris’ Algorithm - Texas A&M University](http://cesg.tamu.edu/wp-content/uploads/2014/09/ECEN689-lec11.pdf)
- [Python-based implementation of Morris’ algorithm](https://github.com/arpitbbhayani/morris-counter/blob/master/morris-counter.ipynb)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/slowsort

Slowsort - the slowest sorting algorithm, ever

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Slowsort - the slowest sorting algorithm, ever

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Slowsort is a sorting algorithm that is designed to be deterministically sub-optimal. The algorithm was published in 1986 by Andrei Broder and Jorge Stolfi in their paper [Pessimal Algorithms and Simplexity Analysis](https://www.researchgate.net/publication/2805500_Pessimal_Algorithms_and_Simplexity_Analysis) where they expressed a bunch of very in-efficient algorithms. These techniques and algorithms are special because they never make a wrong move while solving a problem, instead, they find ways to delay the success. In this essay, we put our focus on the Slowsort algorithm based on the _Multiply and Surrender_ paradigm.

# Multiply and Surrender

The _Multiply and Surrender (MnS)_ paradigm, is a humorous take on the popular _Divide and Conquer (DnC)_ technique. MnS splits the problem into subproblems, slightly simpler than the original, and continues doing so recursively for as long as possible. The subproblems are multiplied and the evaluation is delayed till the state that the solving could not be further postponed and then it surrenders.

MnS paradigm is very similar to DnC; but where the DnC actually splits the problems into subproblems to reach the solution quicker, MnS does it to procrastinate, making the entire process very inefficient but yet convergent.

Although Slowsort is a classic example, recursive Fibonacci with no memoization also fares under the MnS paradigm. The recursive code to find `n`th Fibonacci number is as illustrated below

```
def fib(n):
    """the function returns the `n`th fibonacci number
    """

    # the subproblem could not be postponed anymore
    # hence we surrender and return the answer
    if n < 2:
        return 1

    # we split the problem into subproblems and invoke
    # the function recursively
    return fib(n - 1) + fib(n - 2)
```

While computing the Fibonacci numbers, we split the problem into subproblems and do this recursively till we are left with elemental states i.e. `0` or `1` and which is when we return the initial values which are `1` and `1`. This approach is not DnC because we are not splitting the problem into subproblems to achieve optimality, instead are doing a lot of repetitive work and taking a non-polynomial time to generate the Fibonacci numbers.

# Slowsort Algorithm

Slowsort algorithm draws a lot of similarities to the very popular Mergesort, but while Mergesort operates in `O(n . log(n))` the complexity of Slowsort is non-polynomial `O(n ^ log(n))` and its best case performs worse than the worst case of bubble sort.

Slowsort algorithm recursively breaks the array sorting problem into two subarray sorting problems and a few extra processing steps. Once the two subarrays are sorted, the algorithm swaps the rightmost elements such that the greatest among the two becomes the rightmost element of the array i.e. the greatest among the two is placed at the correct position relative to each other, and then it invokes the sorting for all elements except this fixed maximum.

The algorithm could this be expressed as following broad steps

- sort the first half recursively
- sort the second half recursively
- find the maximum of the whole array by comparing the last elements of both the sorted halves and place it at the end of the array
- recursively sort the entire array except for the maximum one

The in-place implementation of the Slowsort algorithm is as illustrated below

```
def _slowsort(a, i, j):
    """in-place sorts the integers in the array
    spanning indexes [i, j].
    """
    # base condition; then no need of sorting if
    #  - there is one element to sort
    #  - when start and end of the array flipped positions
    if i >= j:
        return

    # find the mid index of the array so that the
    # problem could be divided intto sub-problems of
    # smaller spans
    m = (i + j) // 2

    # invoke the slowsort on both the subarrays
    _slowsort(a, i, m)
    _slowsort(a, m + 1, j)

    # once both the subproblems are solved, check if
    # last elements of both subarrays and move the
    # higher among the both to end of the right subarray
    # ensuring that the highest element is placed at the
    # correct relative position
    if a[m] > a[j]:
        a[m], a[j] = a[j], a[m]

    # now that the rightmost element of the array is at
    # the relatively correct position, we invoke Slowsort on all
    # the elements except the last one.
    _slowsort(a, i, j - 1)


def slowsort(a):
    """in-place sorts the array `a` using Slowsort.
    """
    _slowsort(a, 0, len(a) - 1)
```

The Slowsort algorithm typically replaces the `merge` function of the Mergesort with a simple swap that correctly places the largest element (local maxima) and then invokes the sort function on all but this element. So on every invocation, we keep correctly placing the largest element but in a recursive manner.

_A visualization of this algorithm could be found in this [youtube video](https://www.youtube.com/watch?v=QbRoyhGdjnA)._

# Asymptotic Analysis

The runtime of Slowsort could be computed by the following recurrence relation

![slowsort recurrence relation](https://user-images.githubusercontent.com/4745789/88473102-cb594e80-cf37-11ea-9015-217c3eda50d6.png)

When the above recurrence relation is solved and we find that the asymptotic lower bound for `T(n)` is given by

![lower bound of slowsort](https://user-images.githubusercontent.com/4745789/88473128-14a99e00-cf38-11ea-905b-f3f473a0d74c.png)

The above expression suggests that lower bound of Slowsort is non-polynomial in nature and for a sufficiently large `n` this would be more than `n ^ 2` implying that even the best case of Slowsort is worse than the worst case of Bubble sort. The illustration below compares the time taken by Slowsort and the recursive implementation of Bubblesort.

![slowsort vs recursive bubblesort](https://user-images.githubusercontent.com/4745789/88475549-8e4c8680-cf4e-11ea-8ee4-9f7ed345ff5d.png)

> The iterative implementation of Bubblesort stood no chance in terms of time taken for smaller sets of integers, hence the chart was plotted against the recursive implementation of it. The iterative bubble sort for smaller arrays is nearly flat.

# Slowsort vs Bogosort

[Bogosort](https://en.wikipedia.org/wiki/Bogosort) is a sorting algorithm that has an average case time complexity of `O(n!)` and an unbounded time in the worst case. The algorithm keeps on permuting (shuffling) the array till it is sorted which introduces an unboundedness in its implementation and hence the algorithm is considered to be the worst sorting algorithm ever.

```
import random

def is_sorted(a):
    return all(a[i] <= a[i + 1] for i in range(len(a) - 1))

def bogosort(a):
    while not is_sorted(a):
        random.shuffle(a)
```

The reason that we should rate Slowsort highly is the fact the Bogosort could sort the list in its first shuffle while Slowsort is deterministic and will take `O(n ^ log(n))` time in best case scenario.

A rule that every algorithm follows is that every step that it takes actually makes some progress towards the final goal. Bogosort does not guarantee progress, and since it randomly shuffles the array, at one iteration we could end up at a nearly sorted array while the next shuffle takes us afar.

Slowsort is deterministic and convergent. The number of swaps made (inversions) during the execution is non-increasing which means once two items are swapped they are in the correct order relative to each other. In other words, we say that Slowsort never makes a wrong move.

# References

- [Slowsort](https://wiki.c2.com/?SlowSort)
- [Slowsort - Wikipedia](https://en.wikipedia.org/wiki/Slowsort)
- [Multiply and Surrender](https://wiki.c2.com/?MultiplyAndSurrender)
- [Pessimal Algorithms and Simplexity Analysis](https://www.researchgate.net/publication/2805500_Pessimal_Algorithms_and_Simplexity_Analysis)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/bitcask

Bitcask - A Log-Structured fast KV store

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Datastore Deepdives](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Datastore Deepdives](/knowledge-base/database-engineering)

# Bitcask - A Log-Structured fast KV store

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Bitcask is one of the most efficient embedded Key-Value (KV) Databases designed to handle production-grade traffic. The paper that introduced Bitcask to the world says it is a _[Log-Structured](https://en.wikipedia.org/wiki/Log-structured_file_system) [Hash Table](https://en.wikipedia.org/wiki/Hash_table) for Fast Key/Value Data_ which, in a simpler language, means that the data will be written sequentially to an append-only log file and there will be pointers for each `key` pointing to the `position` of its log entry. Building a KV store off the append-only log files seems like a really weird design choice, but Bitcask does not only make it efficient but it also gives a really high Read-Write throughput.

Bitcask was introduced as the backend for a distributed database named [Riak](https://riak.com/) in which each node used to run one instance of Bitcask to hold the data that it was responsible for. In this essay, we take a detailed look into Bitcask, its design, and find the secret sauce that makes it so performant.

# Design of Bitcask

Bitcask uses a lot of principles from [log-structured file systems](https://en.wikipedia.org/wiki/Log-structured_file_system) and draws inspiration from a number of designs that involve log file merging, for example - merging in LSM Trees. It essentially is just a directory of append-only (log) files with a fixed structure and an in-memory index holding the keys mapped to a bunch of information necessary for point lookups - referring to the entry in the datafile.

## Datafiles

Datafiles are append-only log files that hold the KV pairs along with some meta-information. A single Bitcask instance could have many datafiles, out of which just one will be active and opened for writing, while the others are considered immutable and are only used for reads.

![Bitcask Datafiles](https://user-images.githubusercontent.com/4745789/87866701-78fdb800-c9a2-11ea-9c35-9a706ac96d97.png)

Each entry in the datafile has a fixed structure illustrated above and it stores `crc`, `timestamp`, `key_size`, `value_size`, actual `key`, and the actual `value`. All the write operations - create, update and delete - made on the engine translates into entries in this active datafile. When this active datafile meets a size threshold, it is closed and a new active datafile is created; and as stated earlier, when closed (intentionally or unintentionally), the datafile is considered immutable and is never opened for writing again.

## KeyDir

KeyDir is an in-memory hash table that stores all the keys present in the Bitcask instance and maps it to the offset in the datafile where the log entry (value) resides; thus facilitating the point lookups. The mapped value in the Hash Table is a structure that holds `file_id`, `offset`, and some meta-information like `timestamp`, as illustrated below.

![Bitcask KeyDir](https://user-images.githubusercontent.com/4745789/87866707-96cb1d00-c9a2-11ea-9730-fc7f8cb79b92.png)

# Operations on Bitcask

Now that we have seen the overall design and components of Bitcask, we can jump into exploring the operations that it supports and details of their implementations.

### Putting a new Key Value

When a new KV pair is submitted to be stored in the Bitcask, the engine first appends it to the active datafile and then creates a new entry in the KeyDir specifying the offset and file where the value is stored. Both of these actions are performed atomically which means either the entry is made in both the structures or none.

Putting a new Key-Value pair requires just one atomic operation encapsulating one disk write and a few in-memory access and updates. Since the active datafile is an append-only file, the disk write operation does not have to perform any disk seek whatsoever making the write operate at an optimum rate providing a high write throughput.

### Updating an existing Key Value

This KV store does not support partial update, out of the box, but it does support full value replacement. Hence the update operation is very similar to putting a new KV pair, the only change being instead of creating an entry in KeyDir, the existing entry is updated with the new position in, possibly, the new datafile.

The entry corresponding to the old value is now dangling and will be garbage collected explicitly during merging and compaction.

### Deleting a Key

Deleting a key is a special operation where the engine atomically appends a new entry in the active datafile with value equalling a tombstone value, denoting deletion, and deleting the entry from the in-memory KeyDir. The tombstone value is chosen as something very unique so that it does not interfere with the existing value space.

Delete operation, just like the update operation, is very lightweight and requires a disk write and an in-memory update. In delete operation as well, the older entries corresponding to the deleted keys are left dangling and will be garbage collected explicitly during merging and compaction.

### Reading a Key-Value

Reading a KV pair from the store requires the engine to first find the datafile and the offset within it for the given key; which is done using the KeyDir. Once that information is available the engine then performs one disk read from the corresponding datafile at the offset to retrieve the log entry. The correctness of the value retrieved is checked against the CRC stored and the value is then returned to the client.

The operation is inherently fast as it requires just one disk read and a few in-memory accesses, but it could be made faster using Filesystem read-ahead cache.

# Merge and Compaction

As we have seen during Update and Delete operations the old entries corresponding to the key remain untouched and dangling and this leads to Bitcask consuming a lot of disk space. In order to make things efficient for the disk utilization the engine once a while compacts the older closed datafiles into one or many merged files having the same structure as the existing datafiles.

The merge process iterates over all the immutable files in the Bitcask and produces a set of datafiles having only _live_ and _latest_ versions of each present key. This way the unused and non-existent keys are ignored from the newer datafiles saving a bunch of disk space. Since the record now exists in a different merged datafile and at a new offset, its entry in KeyDir needs an atomic updation.

# Performant bootup

If the Bitcask crashes and needs a boot-up, it will have to read all the datafiles and build a new KeyDir. Merging and compaction here do help as it reduces the need to read data that is eventually going to be evicted. But there is another operation that could help in making the boot times faster.

For every datafile a _hint_ file is created which holds everything in the datafile except the value i.e. it holds the key and its meta-information. This _hint_ file, hence, is just a file containing all the keys from the corresponding datafile. This _hint_ file is very small in size and hence by reading this file the engine could quickly create the entire KeyDir and complete the bootup process faster.

# Strengths and Weaknesses of Bitcask

## Strengths

- Low latency for read and write operations
- High Write Throughput
- Single disk seek to retrieve any value
- Predictable lookup and insert performance
- Crash recovery is fast and bounded
- Backing up is easy - Just copy the directory would suffice

## Weaknesses

The KeyDir holds all the keys in memory at all times and this adds a huge constraint on the system that it needs to have enough memory to contain the entire keyspace along with other essentials like Filesystem buffers. Thus the limiting factor for a Bitcask is the limited RAM available to hold the KeyDir.

Although this weakness sees a major one but the solution to this is fairly simple. We can typically shard the keys and scale it horizontally without losing much of the basic operations like Create, Read, Update, and Delete.

# References

- [Bitcask Paper](https://riak.com/assets/bitcask-intro.pdf)
- [Bitcask - Wikipedia](https://en.wikipedia.org/wiki/Bitcask)
- [Riak’s Bitcask - High Scalability](http://highscalability.com/blog/2011/1/10/riaks-bitcask-a-log-structured-hash-table-for-fast-keyvalue.html/)
- [Implementation of the Bitcask storage model-merge and hint files](https://topic.alibabacloud.com/a/implementation-of-the-bitcask-storage-model-merge-and-hint-files_8_8_31516931.html)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/phi-accrual

Phi φ Accrual - a realistic failure detection algorithm

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Phi φ Accrual - a realistic failure detection algorithm

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

One of the most important virtues of any distributed system is its ability to detect failures in any of its subsystems before things go havoc. Early detection of failures helps in taking preventive actions and ensuring that the system stays fault-tolerant. The conventional way of failure detection is by using a bunch of heartbeat messages with a fixed timeout, indicating if a subsystem is down or not.

In this essay, we take a look into an adaptive failure detection algorithm called _Phi Accrual Failure Detection_, which was introduced in a [paper](https://pdfs.semanticscholar.org/11ae/4c0c0d0c36dc177c1fff5eb84fa49aa3e1a8.pdf) by Naohiro Hayashibara, Xavier Défago, Rami Yared, and Takuya Katayama. The algorithm uses historical heartbeat information to make the threshold adaptive. Instead of generating a binary value, like conventional methods, it generates continuous values suggesting the confidence level it has in stating if the system crashed or not.

# Conventional Failure Detection

Accurately detecting failures is an impossible problem to solve as we cannot ever say if a system crashed or is just very slow in responding. Conventional Failure Detection algorithms output a boolean value stating if the system is down or not; there is no middle ground.

## Heartbeats with constants timeouts

The conventional Failure Detection algorithms use _heartbeat_ messages with a fixed timeout in order to determine if a system is alive or not. The monitored system periodically sends a heartbeat message to the monitoring system, informing that it is still alive. The monitoring system will suspect that the process crashed if it fails to receive any heartbeat message within a configured timeout period.

Here the value of timeout is very crucial as keeping it short means we detect failures quickly but with a lot of false positives; and while keeping it long means we reduce the false positives but the detection time takes a toll.

# Phi Accrual Failure Detection

Phi Accrual Failure Detection is an adaptive Failure Detection algorithm that provides a building block to implementing failure detectors in any distributed system. A generic Accrual Failure Detector, instead of providing output as a boolean (system being up or down), outputs the suspicion information (level) on a continuous scale such that higher the suspicion value, the higher are the chances that the system is down.

## Detailing φ

We define φ as the suspicion level output by this failure detector and as the algorithm is adaptive, the value will be dynamic and will reflect the current network conditions and system behavior. As we established earlier - lower are the chances of receiving the heartbeat, higher are the chances that the system crashed hence higher should be the value of φ; the details around expressing φ mathematically are as illustrated below.

![Phi Accrual Failure Detection](https://user-images.githubusercontent.com/4745789/87240784-469c0a00-c43a-11ea-8689-9dc41eb1ccf1.png)

The illustration above mathematically expresses our establishments and shows how we can use `-log10(x)` function applied to the probability to get a gradual negative slope indicating a decline in the value of φ. We observe how, as the probability of receiving heartbeat increases, the value of φ decreases and approaches `0`, and when the probability of receiving heartbeat decreases and approaches `0`, the value of φ tends to infinity ∞.

The φ value computed using `-log10(x)` also suggests our likeliness of making mistakes decreases exponentially as the value of φ increases. So if we say a system is down if φ crosses a certain threshold `X` where `X` is `1`, it implies that our decision will be contradicted in the future by the reception of a late heartbeat is about `10%`. For `X = 2`, the likelihood of the mistake will be `1%`, for `X = 3` it will be `0.1%`, and so on.

## Estimating the probability of receiving another heartbeat

Now that we have defined what φ is, we need a way to compute the probability of receiving another heartbeat given we have seen some heartbeats before. This probability is proportional to the probability that the heartbeat will arrive more than `t` units after the previous one i.e. longer the wait lesser are the chances of receiving the heartbeat.

In order to implement this, we keep a sampled Sliding Window holding arrival times of past heartbeats. Whenever a new heartbeat arrives, its arrival time is stored into the window, and the data regarding the oldest heartbeat is deleted.

We observe that the arrival intervals follow a [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution) indicating most of the heartbeats arrive within a specific range while there are a few that arrive late due to various network or system conditions. From the information stored in the window, we can easily compute the arrival intervals, mean, and variance which we require to estimate the probability.

Since arrival intervals follow a Normal Distribution, we can integrate the [Probability Density Function](https://en.wikipedia.org/wiki/Probability_density) over the interval `(t, ∞)` to get the probability of receiving heartbeat after `t` units of time. Thus the expression for deriving this can be illustrated below.

![Estimating probability of receiving another heartbeat](https://user-images.githubusercontent.com/4745789/87231591-fbe8a680-c3d5-11ea-9427-d4cd66e8e717.png)

We observe that if the process actually crashes, the value is guaranteed to accrue (accumulate) over time and will tend to infinity ∞. Since the accrual failure detectors output value in a continuous range, we need to explicitly define thresholds crossing which we say that the system crashed.

# Benefits of using Accrual Failure Detectors

We can define multiple thresholds, crossing which we can take precautionary measures defined for it. As the threshold becomes steeper the action could become more drastic. Another major benefit of using this system is that it favors a nearly complete decoupling between application requirements and monitoring as it leaves the applications to define threshold according to their QoS requirements.

# References

- [The φ Accrual Failure Detector](https://pdfs.semanticscholar.org/11ae/4c0c0d0c36dc177c1fff5eb84fa49aa3e1a8.pdf)
- [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/10x-engineer

Traits of a 10x engineer

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Becoming Better](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Becoming Better](/knowledge-base/career-growth)

# Traits of a 10x engineer

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

10x engineers are not 10x because they built a complex system; they are 10x because they almost always tend to find simple, stable, and manageable solutions to complex problems. To become one,

1. operate with extreme ownership
2. be curious with a high bias for action
3. be a problem solver and not a complainer
4. do not remain confined to some syllabus or curriculum
5. have an innate passion for the domain you are operating in
6. and most importantly understand business >> product >> engineering

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/decipher-repeated-key-xor

Deciphering Repeated-key XOR Ciphertext

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Cryptography](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Cryptography](/knowledge-base/algorithms-and-explorations)

# Deciphering Repeated-key XOR Ciphertext

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Encryption is a process of encoding messages such that it can only be read and understood by the intended parties. The process of extracting the original message from an encrypted one is called Decryption. Encryption usually scrambles the original message using a key, called the encryption key, that the involved parties agree on.

In the previous essay, we went through the [Single-byte XOR cipher](https://arpitbhayani.me/blogs/decipher-single-xor) and found a way to decipher it without having any knowledge of the encryption key. In this essay, we find how to break a [Repeating-key XOR cipher](https://en.wikipedia.org/wiki/XOR_cipher) with variable key length. The problem statement, defined above, is based on [Cryptopals Set 1 Challenge 6](https://cryptopals.com/sets/1/challenges/6).

# Repeating-key XOR Cipher

The Repeating-key XOR cipher algorithm works with an encryption key with no constraint on its length, which makes it much stronger than a Single-byte XOR Cipher, where the encryption key length was restricted to a single byte.

## Encryption

A plain text is encrypted using an encryption key by performing a bitwise [XOR](https://en.wikipedia.org/wiki/Exclusive_or) operation on every character. The encryption key is repeated until it XORs every single character of the plain text and the resultant stream of bytes is again translated back as characters and sent to the other party. These encrypted bytes need not be among the usual printable characters and should ideally be interpreted as a stream of bytes. Following is the python-based implementation of this encryption process.

```
def repeating_key_xor(text: bytes, key: bytes) -> bytes:
    """Given a plain text `text` as bytes and an encryption key `key`
    as bytes, the function encrypts the text by performing
    XOR of all the bytes and the `key` (in repeated manner) and returns
    the resultant XORed byte stream.
    """

    # we update the encryption key by repeating it such that it
    # matches the length of the text to be processed.
    repetitions = 1 + (len(text) // len(key))
    key = key * repetitions
    key = key[:len(text)]

    # XOR text and key generated above and return the raw bytes
    return bytes([b ^ k for b, k in zip(text, key)])
```

As an example, we encrypt the plain text - `secretattack` - with encryption key `$^!` and as per the algorithm, we first repeat the encryption key until it matches the length of the plain text and then XOR it against the plain text. The illustration below shows the entire encryption process.

![https://user-images.githubusercontent.com/4745789/85919742-d1520600-b88b-11ea-8d71-aa36c58dc48a.png](https://user-images.githubusercontent.com/4745789/85919742-d1520600-b88b-11ea-8d71-aa36c58dc48a.png)

For the first character in plain text - `s` - the byte i.e. ASCII value is `115` which when XORed with `$` results in `87` whose character equivalent is `W`, similarly for the second character `e` the encrypted byte is `;`, for `c` it is `B`, for the fourth character `r`, since the key repeats, the XOR is taken with `$` to get `V` and the process continues. The resultant encrypted text using repeated-key XOR on the plain text `secretattack` with key `$^!` is `W;BV;UE*UE=J`.

## Decryption

Decryption is a process of extracting the original message from the encrypted ciphertext given the encryption key. XOR has a [property](https://brainly.in/question/3038497) - if `a = b ^ c` then `b = a ^ c`, hence the decryption process is exactly the same as the encryption i.e. we first repeat the encryption key till it matches the length and then perform bitwise XOR with the ciphertext - the resultant bytes stream will be the original message.

Since encryption and decryption both have an exact same implementation - we pass the ciphertext to the function `repeating_key_xor`, defined above, to get the original message back.

```
>>> repeating_key_xor(b'W;BV;UE*UE=J', b'$^!')
b'secretattack'
```

# Deciphering without the encryption key

Things become really interesting when, given the encryption algorithm, we have to recover the original message from the ciphertext with no knowledge of the encryption key. Just like solving any other problem, the crux of deciphering the message encrypted using repeated-key XOR cipher is to break it down into manageable sub-problems and tackle them independently. We break this deciphering problem into the following two sub-problems:

- Finding the length of the Encryption Key
- Bruteforce with all possible keys and finding the “most English” plain text

## Finding the length of the Encryption Key

In order to recover the original text from the cipher, we first find the length of the encryption key used and then apply brute force with all possible keys of the estimated length and deduce the plain text. Finding the length of the Encryption key makes the deciphering process quicker as it eliminates a lot of false keys and thus reducing the overall effort required during the brute force. In order to find the length of the Encryption Key, we need to have a better understanding of a seemingly unrelated topic - Hamming Distance.

### Hamming Distance

Hamming distance between two bytes is the number of positions at which the corresponding bits differ. For a stream of bytes, of equal lengths, it is the sum of Hamming Distances between the corresponding bytes. Finding differences between bits can be efficiently done using bitwise XOR operation as the operation yields `0` when both the bits are the same and `1` when they differ. So for computing Hamming Distance between two bytes we XOR the bytes and count the number of `1` in its binary representation.

```
def hamming_distance_bytes(text1: bytes, text2: bytes) -> int:
    """Given two stream of bytes, the function returns the Hamming Distance
    between the two.
    Note: If the two texts have unequal lengths, the hamming distance is
    computed only till one of the text exhausts, other bytes are not iterated.
    """
    dist = 0
    for byte1, byte2 in zip(text1, text2):
        dist += bin(byte1 ^ byte2).count('1')
    return dist

>>> hamming_distance_bytes(b'ab', b'zb')
4
```

In the example above, we find that the hamming distance between two bytestreams `ab` and `zb` is `4`, which implies that the byte streams `ab` and `zb` differ at `4` different bits in their binary representations.

### Hamming Score

Hamming distance is an absolute measure, hence in order to compare hamming distance across byte streams of varying lengths, it has to be normalized with the number of pairs of bits compared. We name this measure - Hamming Score - which thus is defined as the Hamming Distance per unit bit-pair. In python, Hamming Score is implemented as:

```
def hamming_score_bytes(text1: bytes, text2: bytes) -> float:
    """Given two streams of bytes, the function computes a normalized Hamming
    Score based on the Hamming distance.
    Normalization is done by dividing the Hamming Distance by the number of bits
    present in the shorter text.
    """
    return hamming_distance_bytes(text1, text2) / (8 * min(len(text1), len(text2)))

>>> hamming_score_bytes(b'ab', b'zb')
0.25
```

### What can we infer through Hamming Distance?

Hamming Distance is an interesting measure; it effectively tells us the minimum number of bit flips required to convert one bytestream into another. It also implies that (on average) if the numerical values of two bytestreams are closer then their Hamming Distance and Hamming Score will be lower i.e it would take fewer bit flips to convert one into another.

This is evident from the fact that the average Hamming distance between any two bytes `[0-256)` picked at random is `3.9999` while that of any two lowercased English characters `[97, 122]` is just `2.45`. Similar ratios are observed for Hamming Score where `0.4999` is of the former while `0.3072` is of the later.

This inference comes in handy when we want to find out the length of Encryption Key in Repeating-key XOR Cipher as illustrated in the section below.

### Formal definition of encryption and decryption processes

Say if `p` denotes the plaintext, `k` denotes the encryption key which is repeated to match the length of the plain text, and `c` denotes the ciphertext, we could define encryption and decryption processes as

```
encryption: c[i] = p[i] XOR k[i]   for i in [0, len(c))
decryption: p[i] = c[i] XOR k[i]   for i in [0, len(p))
```

The above definitions, along with the rules of XOR operations, implying that if we XOR two bytes of the ciphertext, encrypted (XORed) using the same byte of the encryption key, we are effectively XORing the corresponding bytes of the plain text. If `k'` is the byte of the encryption key `k` which was used to encrypt (XOR) the bytes `p[i]` and `p[j]` of the plain text to generate `c[i]` and `c[j]` of the ciphertext, we could derive the following relation

```
# k' is the common byte of the key i.e. k' = k[i] = k[j]

c[i] XOR c[j] = (p[i] XOR k') XOR (p[j] XOR k')
              = p[i] XOR k' XOR k' XOR p[j]
              = p[i] XOR 0 XOR p[j]
              = p[i] XOR p[j]
```

The above relation, `c[i] XOR c[j]` equal to `p[i] XOR p[j]`, holds true only because both the bytes were XORed with the same byte `k'` of the encryption key; which in fact helped reduce the expression. If the byte from the encryption key which was used to XOR the pain texts were different then the relation was irreducible and we could not have possibly setup this relation.

### Chunking of ciphertext

Chunking is the process where the ciphertext is split into smaller chunks (segments) of almost equal lengths. For example, chunking the ciphertext `W;BV;UE*UE=J` for chunk length `4` would create `3` chunks `W;BV`, `;UE*` and `UE=J`. The illustration below shows the chunks that would be formed for `W;BV;UE*UE=J` with chunks lengths varying from 2 to 6.

![https://user-images.githubusercontent.com/4745789/86434084-24a7d680-bd1a-11ea-8346-aad7b42bab1c.png](https://user-images.githubusercontent.com/4745789/86434084-24a7d680-bd1a-11ea-8346-aad7b42bab1c.png)

### XOR of the chunks

Something very interesting happens when we compute the Average Hamming Score for all possible chunk lengths. If we consider the ciphertext `b'W;BV;UE*UE=J` and we chunk it with lengths varying from 2 to 6, we get the following distribution for the Average Hamming Score for each of the chunk length.

![https://user-images.githubusercontent.com/4745789/86473899-6149f100-bd5f-11ea-908a-d4adabff1cf0.png](https://user-images.githubusercontent.com/4745789/86473899-6149f100-bd5f-11ea-908a-d4adabff1cf0.png)

From the distribution above it is evident that the score was minimum at chunk length equalling `3`, which actually was the length of the Encryption Key used on the plain text. Is this mere coincidence or are we onto something?

When chunk length is equal to the length of the encryption key, the XOR operation on any two chunks will reduce the expression to XOR of the corresponding plain texts (as seen above), because there will be a perfect alignment of bytes from ciphertext and bytes from the keys i.e every `i`th byte from both the chunks would have been XORed with `i`th byte from the encryption key.

We have established that for chunk length equal to the length of the encryption key `c[i] XOR c[j]` is effectively `p[i] XOR p[j]`. Since we have assumed that the plain text is a lowercased English sentence the XOR is happening between bytes residing numerically closer to each other and hence has a lower Average Hamming Score between them; because of which we see a minimum at this particular chunk length. The Hamming Score will be much higher for lengths other than the length of Encryption Key because during XOR operation the expression stays irreducible and hence hamming distance is computed panning the entire range of bytes `[0, 256)`.

### Something far more interesting

This minimum does not only hold true for chunk length equal to the length of the encryption key, but it also holds true when the length of the chunk is a multiple of the length of the encryption key. This happens because for repeated keys when the chunk length is a multiple of Encryption Key there will be a perfect alignment of bytes such that every `i`th byte of chunks is XORed with `i`th byte of the encryption key; which sets up the relation `c[i] XOR c[j]` equalling `p[i] XOR p[j]`.

![https://user-images.githubusercontent.com/4745789/86473953-7cb4fc00-bd5f-11ea-83af-f22413e1ecf9.png](https://user-images.githubusercontent.com/4745789/86473953-7cb4fc00-bd5f-11ea-83af-f22413e1ecf9.png)

The above distribution shows a lot of sharp drops of Average Hamming Score for chunk lengths that are multiples of `7` - the length of the encryption key used.

## Computing Encryption Key Length

Now that we understand the theory and concept behind the process of finding the length of the Encryption Key, we can compile the logic into a function that accepts `text` and bytes and returns the length of the Encryption Key as illustrated below

```
def compute_key_length(text: bytes) -> int:
    """The function returns the length of the encryption key
    by chunking and minimizing the Average Hamming Score
    """
    min_score, key_len = None, None

    # We check for chunk lengths from 2 till the half the length of the
    # plain text. Here we assume that the Encryption Key had to be
    # repeated at least twice to match the length of the plaintext
    for klen in range(2, math.ceil(len(text)/2)):

        # We create chunks such that length of each chunk if `klen`
        chunks = [
            text[i: i+klen]
            for i in range(0, len(text), klen)
        ]

        # To gain better accuracy we get rid of the last chunk that had
        # length smaller than klen/2
        if len(chunks) >= 2 and len(chunks[-1]) <= len(chunks[-2])/2:
            chunks.pop()

        # For each chunk length, for every pair of chunks we compute the
        # Hamming Score and keep piling it in a list.
        _scores = []
        for i in range(0, len(chunks) - 1, 1):
            for j in range(i+1, len(chunks), 1):
                score = hamming_score_bytes(chunks[i], chunks[j])
                _scores.append(score)

        # The Hamming Score for a chunk length is the average
        # hamming score computed over all possible pairs of chunks
        score = sum(_scores) / len(_scores)

        # Keep track of the minimum score we have seen and the key length
        # corresponding to it.
        if min_score is None or score < min_score:
            min_score, key_len = score, klen

    # return the key length corresponding to the minimum score
    return key_len
```

## Bruteforce to recover the original text

The function `compute_key_length` returns the length of the Encryption Key used to encrypt the plain text. Once we know the length, we can apply Bruteforce with all possible keys of that length and try to decipher the ciphertext. The approach of deciphering will be very similar to how it was done to [Decipher single-byte XOR Ciphertext](https://arpitbhayani.me/blogs/decipher-single-xor) i.e. by using [Letter Frequency Distribution](https://en.wikipedia.org/wiki/Letter_frequency) and Fitting Quotient to find which key leads to the plain text that is closest to a genuine English sentence.

A test was run on 100 random English sentences with random Encryption keys of varying lengths and it was found that this deciphering technique worked with an accuracy of 99%. Even though the approach is not fool-proof, it does pretty well in eliminating keys that would definitely not result in a correct plain text.

# Conclusion

Deciphering a repeated-key XOR Cipher could also be done using [Kasiski examination](https://en.wikipedia.org/wiki/Kasiski_examination); the method we saw in this essay was Friedman Test using Hamming Distance and Frequency Analysis. The main purpose of this essay was to showcase how seemingly unrelated concepts work together to solve an interesting problem efficiently.

# References

- [Vigenère Cipher](https://en.wikipedia.org/wiki/Vigen%C3%A8re_cipher)
- [Repeating-key XOR Cipher](https://en.wikipedia.org/wiki/XOR_cipher)
- [Cryptopals Set 1 Challenge 6](https://cryptopals.com/sets/1/challenges/6)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/decipher-single-xor

Deciphering Single-byte XOR Ciphertext

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Cryptography](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Cryptography](/knowledge-base/algorithms-and-explorations)

# Deciphering Single-byte XOR Ciphertext

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Encryption is a process of encoding messages such that it can only be read and understood by the intended parties. The process of extracting the original message from an encrypted one is called Decryption. Encryption usually scrambles the original message using a key, called encryption key, that the involved parties agree on.

The strength of an encryption algorithm is determined by how hard it would be to extract the original message without knowing the encryption key. Usually, this depends on the number of bits in the key - bigger the key, the longer it takes to decrypt the enciphered data.

In this essay, we will work with a very simple cipher (encryption algorithm) that uses an encryption key with a size of one byte, and try to decipher the ciphered text and retrieve the original message without knowing the encryption key. The problem statement, defined above, is based on [Cryptopals Set 1 Challenge 3](https://cryptopals.com/sets/1/challenges/3).

# Single-byte XOR cipher

The Single-byte XOR cipher algorithm works with an encryption key of size 1 byte - which means the encryption key could be one of the possible 256 values of a byte. Now we take a detailed look at how the encryption and decryption processes look like for this cipher.

## Encryption

As part of the encryption process, the original message is iterated bytewise and every single byte `b` is XORed with the encryption key `key` and the resultant stream of bytes is again translated back as characters and sent to the other party. These encrypted bytes need not be among the usual printable characters and should ideally be interpreted as a stream of bytes. Following is the python-based implementation of the encryption process.

```
def single_byte_xor(text: bytes, key: int) -> bytes:
    """Given a plain text `text` as bytes and an encryption key `key` as a byte
    in range [0, 256) the function encrypts the text by performing
    XOR of all the bytes and the `key` and returns the resultant.
    """
    return bytes([b ^ key for b in text])
```

As an example, we can try to encrypt the plain text - `abcd` - with encryption key `69` and as per the algorithm, we perform XOR bytewise on the given plain text. For character `a`, the byte i.e. ASCII value is `97` which when XORed with `69` results in `36` whose character equivalent is `$`, similarly for `b` the encrypted byte is `'`, for `c` it is `&` and for `d` it is `!`. Hence when `abcd` is encrypted using single-byte XOR cipher and encryption key `69`, the resultant ciphertext i.e. the encrypted message is `$'&!`.

![https://user-images.githubusercontent.com/4745789/85209379-0b377f80-b355-11ea-8206-54ad558b4a6f.png](https://user-images.githubusercontent.com/4745789/85209379-0b377f80-b355-11ea-8206-54ad558b4a6f.png)

## Decryption

Decryption is the process of extracting the original message from the encrypted ciphertext given the encryption key. XOR has a [property](https://brainly.in/question/3038497) - if `a = b ^ c` then `b = a ^ c`, hence the decryption process is exactly the same as the encryption i.e. we iterate through the encrypted message bytewise and XOR each byte with the encryption key - the resultant will be the original message.

Since encryption and decryption both have an exact same implementation - we pass the ciphertext to the function `single_byte_xor`, defined above, to get the original message back.

```
>>> single_byte_xor(b"$'&!", 69)
b'abcd'
```

# Deciphering without the encryption key

Things become really interesting when we have to recover the original message given the ciphertext and having no knowledge of the encryption key; although we do know the encryption algorithm.

As a sample plain text, we take the last couple of messages, sent across on their German military radio network during World War II. These messages were intercepted and decrypted by the British troops. During wartime, the messages were encrypted using [Enigma Machine](https://en.wikipedia.org/wiki/Enigma_machine) and [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing) famously [cracked the Enigma Code](https://www.iwm.org.uk/history/how-alan-turing-cracked-the-enigma-code) (similar to encryption key) that was used to encipher German messages.

![https://user-images.githubusercontent.com/4745789/85209533-72096880-b356-11ea-8a84-97f2feb86b44.png](https://user-images.githubusercontent.com/4745789/85209533-72096880-b356-11ea-8a84-97f2feb86b44.png)

> In this essay, instead of encrypting the message using the Enigma Code, we are going to use Single-byte XOR cipher and try to recover the original message back without any knowledge of the encryption key.

Here, we assume that the original message, to be encrypted, is a genuine English lowercased sentence. The ciphertext that we would try to decipher can be obtained as

```
>>> key = 82
>>> plain_text = b'british troops entered cuxhaven at 1400 on 6 may - from now on all radio traffic will cease - wishing you all the best. lt kunkel.'
>>> single_byte_xor(plain_text, key)
b'0 ;&;!:r& =="!r7<&7 76r1\'*:3$7<r3&rcfbbr=<rdr?3+r\x7fr4 =?r<=%r=<r3>>r 36;=r& 344;1r%;>>r173!7r\x7fr%;!:;<5r+=\'r3>>r&:7r07!&|r>&r9\'<97>|'
```

## Bruteforce

There are a very limited number of possible encryption keys - 256 to be exact - we can, very conveniently, go for the Bruteforce approach and try to decrypt the ciphered text with every single one of it. So we start iterating over all keys in the range `[0, 256)` and decrypt the ciphertext and see which one resembles the original message the most.

![https://user-images.githubusercontent.com/4745789/85209704-ad586700-b357-11ea-8b7c-4d4616af609a.png](https://user-images.githubusercontent.com/4745789/85209704-ad586700-b357-11ea-8b7c-4d4616af609a.png)

In the illustration above, we see that the message decrypted through key `82` is, in fact, our original message, while the other retrieved plain texts look scrambled and garbage. Doing this visually is very easy; we, as humans, are able to comprehend familiarity but how will a computer recognize this?

We need a way to quantify the closeness of a text to a genuine English sentence. Closer the decrypted text is to be a genuine English sentence, the closer it would be to our original plain text.

> We can do this only because of our assumption - that the original plain text is a genuine English sentence.

## ETAOIN SHRDLU

Letter Frequency is the number of times letters of an alphabet appear on average in written language. In the English language the letter frequency of letter `a` is `8.239%`, for `b` it is `1.505%` which means out of 100 letters written in English, the letter `a`, on an average, will show up `8.239%` of times while `b` shows up `1.505%` of times. Letter frequency (in percentage) for other letters is as shown below.

```
occurance_english = {
    'a': 8.2389258,    'b': 1.5051398,    'c': 2.8065007,    'd': 4.2904556,
    'e': 12.813865,    'f': 2.2476217,    'g': 2.0327458,    'h': 6.1476691,
    'i': 6.1476691,    'j': 0.1543474,    'k': 0.7787989,    'l': 4.0604477,
    'm': 2.4271893,    'n': 6.8084376,    'o': 7.5731132,    'p': 1.9459884,
    'q': 0.0958366,    'r': 6.0397268,    's': 6.3827211,    't': 9.1357551,
    'u': 2.7822893,    'v': 0.9866131,    'w': 2.3807842,    'x': 0.1513210,
    'y': 1.9913847,    'z': 0.0746517
}
```

This Letter Frequency analysis is a rudimentary way for language identification in which we see if the current letter frequency distribution of a text matches the average letter frequency distribution of the English language. [ETAOIN SHRDLU](https://en.wikipedia.org/wiki/Etaoin_shrdlu) is the approximate order of frequency of the 12 most commonly used letters in the English language.

The following chart shows Letter Frequency analysis for decrypted plain texts with encryption keys from `79` to `84`.

![https://user-images.githubusercontent.com/4745789/85209804-5a32e400-b358-11ea-8e1b-2b6bb3e22868.png](https://user-images.githubusercontent.com/4745789/85209804-5a32e400-b358-11ea-8e1b-2b6bb3e22868.png)

In the illustration above, we could clearly see how well the Letter Frequency distribution for encryption key `82` fits the distribution of the English language. Now that our hypothesis holds true, we need a way to quantify this measure and we call if the Fitting Quotient.

## Fitting Quotient

Fitting Quotient is the measure that suggests how well the two Letter Frequency Distributions match. Heuristically, we define the Fitting Quotient as the average of the absolute difference between the frequencies (in percentage) of letters in `text` and the corresponding letter in the English Language. Thus having a smaller value of Fitting Quotient implies the text is closer to the English Language.

![https://user-images.githubusercontent.com/4745789/85219888-f2ff4900-b3c4-11ea-933a-96e26580a3fb.png](https://user-images.githubusercontent.com/4745789/85219888-f2ff4900-b3c4-11ea-933a-96e26580a3fb.png)

Python-based implementation of the, above defined, Fitting Quotient is as shown below. The function first computes the relative frequency for each letter in `text` and then takes an average of the absolute difference between the two distributions.

```
dist_english = list(occurance_english.values())

def compute_fitting_quotient(text: bytes) -> float:
    """Given the stream of bytes `text` the function computes the fitting
    quotient of the letter frequency distribution for `text` with the
    letter frequency distribution of the English language.

    The function returns the average of the absolute difference between the
    frequencies (in percentage) of letters in `text` and the corresponding
    letter in the English Language.
    """
    counter = Counter(text)
    dist_text = [
        (counter.get(ord(ch), 0) * 100) / len(text)
        for ch in occurance_english
    ]
    return sum([abs(a - b) for a, b in zip(dist_english, dist_text)]) / len(dist_text)
```

## Deciphering

Now that we have everything we require to directly get the plain text out of the given ciphertext we wrap it in a function that iterates over all possible encryption keys in the range `[0, 256)`, decrypts the ciphertext, computes the fitting quotient for the plain text and returns the one that minimizes the quotient as the original message. Python-based implementation of this deciphering logic is as illustrated below.

```
def decipher(text: bytes) -> Tuple[bytes, int]:
    """The function deciphers an encrypted text using Single Byte XOR and returns
    the original plain text message and the encryption key.
    """
    original_text, encryption_key, min_fq = None, None, None
    for k in range(256):
        # we generate the plain text using encryption key `k`
        _text = single_byte_xor(text, k)

        # we compute the fitting quotient for this decrypted plain text
        _fq = compute_fitting_quotient(_text)

        # if the fitting quotient of this generated plain text is lesser
        # than the minimum seen till now `min_fq` we update.
        if min_fq is None or _fq < min_fq:
            encryption_key, original_text, min_fq = k, _text, _fq

    # return the text and key that has the minimum fitting quotient
    return original_text, encryption_key
```

This approach was also tested against 100 random English sentences with random Encryption keys and it was found that this deciphering technique fared well for all the samples. The approach would fail if the sentence is very short or contains a lot of symbols. The source code for this entire deciphering process is available in a Jupyter notebook at [arpitbhayani.me/decipher-single-byte-xor](https://github.com/arpitbbhayani/decipher-single-byte-xor/blob/master/decipher-single-byte-xor.ipynb).

# References

- [Etaoin shrdlu](https://en.wikipedia.org/wiki/Etaoin_shrdlu)
- [English Letter Frequency](https://en.wikipedia.org/wiki/Letter_frequency)
- [Single-byte XOR encryption](https://wiki.bi0s.in/crypto/xor/#single-byte-xor-cipher)
- [Cryptopals Challenge - Set 1 Challenge 3](https://cryptopals.com/sets/1/challenges/3)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/python-iterable-integers

Making Python integers iterable - a deep dive into CPython

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Language Internals](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

- [KB](/knowledge-base)
- [Lang...](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

# Making Python integers iterable - a deep dive into CPython

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Iterables in Python are objects and containers that could be stepped through one item at a time, usually using a `for ... in` loop. Not all objects can be iterated, for example - we cannot iterate an integer, it is a singular value. The best we can do here is iterate on a range of integers using the `range` type which helps us iterate through all integers in the range `[0, n)`.

Since integers, individualistically, are not iterable, when we try to do a `for x in 7`, it raises an exception stating `TypeError: 'int' object is not iterable`. So what if, we change the Python’s source code and make integers iterable, say every time we do a `for x in 7`, instead of raising an exception it actually iterates through the values `[0, 7)`. In this essay, we would be going through exactly that, and the entire agenda being:

- What is a Python iterable?
- What is an iterator protocol?
- Changing Python’s source code and make integers iterable, and
- Why it might be a bad idea to do so?

# Python Iterables

Any object that could be iterated is an Iterable in Python. The list has to be the most popular iterable out there and it finds its usage in almost every single Python application - directly or indirectly. Even before the first user command is executed, the Python interpreter, while booting up, has already created `406` lists, for its internal usage.

In the example below, we see how a list `a` is iterated through using a `for ... in` loop and each element can be accessed via variable `x`.

```
>>> a = [2, 3, 5, 7, 11, 13]
>>> for x in a: print(x, end=" ")
2 3 5 7 11 13
```

Similar to `list`, `range` is a python type that allows us to iterate on integer values starting with the value `start` and going till `end` while stepping over `step` values at each time. `range` is most commonly used for implementing a C-like `for` loop in Python. In the example below, the `for` loop iterates over a `range` that starts from `0`, goes till `7` with a step of `1` - producing the sequence `[0, 7)`.

```
# The range(0, 7, 1) will iterate through values 0 to 6 and every time
# it will increment the current value by 1 i.e. the step.
>>> for x in range(0, 7, 1): print(x, end=" ")
0 1 2 3 4 5 6
```

Apart from `list` and `range` other [iterables](https://docs.python.org/3/library/stdtypes.html#sequence-types-list-tuple-range) are - `tuple`, `set`, `frozenset`, `str`, `bytes`, `bytearray`, `memoryview`, and `dict`. Python also allows us to create custom iterables by making objects and types follow the [Iterator Protocol](https://docs.python.org/3/c-api/iter.html).

# Iterators and Iterator Protocol

Python, keeping things simple, defines iterable as any object that follows the [Iterator Protocol](https://docs.python.org/3/c-api/iter.html); which means the object or a container implements the following functions

- `__iter__` should return an iterator object having implemented the `__next__` method
- `__next__` should return the next item of the iteration and if items are exhausted then raise a `StopIteration` exception.

So, in a gist, `__iter__` is something that makes any python object iterable; hence to make integers iterable we need to have `__iter__` function set for integers.

# Iterable in CPython

The most famous and widely used implementation of Python is [CPython](https://github.com/python/cpython/) where the core is implemented in pure C. Since we need to make changes to one of the core datatypes of Python, we will be modifying CPython, add `__iter__` function to an Integer type, and rebuild the binary. But before jumping into the implementation, it is important to understand a few fundamentals.

## The `PyTypeObject`

Every object in Python is associated with a type and each [type](https://docs.python.org/3/c-api/typeobj.html#type-objects) is an instance of a struct named [PyTypeObject](https://docs.python.org/3/c-api/typeobj.html). A new instance of this structure is effectively a new type in python. This structure holds a few meta information and a bunch of C function pointers - each implementing a small segment of the type’s functionality. Most of these “slots” in the structure are optional which could be filled by putting appropriate function pointers and driving the corresponding functionality.

## The `tp_iter` slot

Among all the slots available, the slot that interests us is the `tp_iter` slot which can hold a pointer to a function that returns an iterator object. This slot corresponds to the `__iter__` function which effectively makes the object iterable. A non `NULL` value of this slot indicates iterability. The `tp_iter` holds the function with the following signature

```
PyObject * tp_iter(PyObject *);
```

Integers in Python do not have a fixed size; rather the size of integer depends on the value it holds. [How Python implements super long integers](https://arpitbhayani.me/blogs/super-long-integers) is a story on its own but the core implementation can be found at [longobject.c](https://github.com/python/cpython/blob/master/Objects/longobject.c). The instance of `PyTypeObject` that defines integer/long type is `PyLong_Type` and has its `tp_iter` slot set to `0` i.e. `NULL` which asserts the fact that Integers in python are not iterable.

```
PyTypeObject PyLong_Type = {
    ...

    "int",                                      /* tp_name */
    offsetof(PyLongObject, ob_digit),           /* tp_basicsize */
    sizeof(digit),                              /* tp_itemsize */
    ...
    0,                                          /* tp_iter */
    ...
};
```

This `NULL` value for `tp_iter` makes `int` object not iterable and hence if this slot was occupied by an appropriate function pointer with the aforementioned signature, this could well make any integer iterable.

# Implementing `long_iter`

Now we implement the `tp_iter` function on integer type, naming it `long_iter`, that returns an iterator object, as required by the convention. The core functionality we are looking to implement here is - when an integer `n` is iterated, it should iterate through the sequence `[0, n)` with step `1`. This behavior is very close to the pre-defined `range` type, that iterates over a range of integer values, more specifically a `range` that starts at `0`, goes till `n` with a step of `1`.

We define a utility function in `rangeobject.c` that, given a python integer, returns an instance of `longrangeiterobject` as per our specifications. This utility function will instantiate the `longrangeiterobject` with start as `0`, ending at the long value given in the argument, and step as `1`. The utility function is as illustrated below.

```
/*
 *  PyLongRangeIter_ZeroToN creates and returns a range iterator on long
 *  iterating on values in the range [0, n).
 *
 *  The function creates and returns a range iterator from 0 till the
 *  provided long value.
 */
PyObject *
PyLongRangeIter_ZeroToN(PyObject *long_obj)
{
    // creating a new instance of longrangeiterobject
    longrangeiterobject *it;
    it = PyObject_New(longrangeiterobject, &PyLongRangeIter_Type);

    // if unable to allocate memoty to it, return NULL.
    if (it == NULL)
        return NULL;

    // we set the start to 0
    it->start = _PyLong_Zero;

    // we set the step to 1
    it->step = _PyLong_One;

    // we set the index to 0, since we want to always start from the first
    // element of the iteration
    it->index = _PyLong_Zero;

    // we set the total length of iteration to be equal to the provided value
    it->len = long_obj;

    // we increment the reference count for each of the values referenced
    Py_INCREF(it->start);
    Py_INCREF(it->step);
    Py_INCREF(it->len);
    Py_INCREF(it->index);

    // downcast the iterator instance to PyObject and return
    return (PyObject *)it;
}
```

The utility function `PyLongRangeIter_ZeroToN` is defined in `rangeobject.c` and will be declared in `rangeobject.h` so that it can be used across the CPython. Declaration of function in `rangeobject.h` using standard Python macros goes like this

```
PyAPI_FUNC(PyObject *)   PyLongRangeIter_ZeroToN(PyObject *);
```

The function occupying the `tp_iter` slot will receive the `self` object as the input argument and is expected to return the iterator instance. Hence, the `long_iter` function will receive the python integer object (self) that is being iterated as an input argument and it should return the iterator instance. Here we would use the utility function `PyLongRangeIter_ZeroToN`, we just defined, which is returning us an instance of range iterator. The entire `long_iter` function could be defined as

```
/*
 *  long_iter creates an instance of range iterator using PyLongRangeIter_ZeroToN
 *  and returns the iterator instance.
 *
 *  The argument to the `tp_iter` is the `self` object and since we are trying to
 *  iterate an integer here, the input argument to `long_iter` will be the
 *  PyObject of type PyLong_Type, holding the integer value.
 */
static PyObject * long_iter(PyObject *long_obj)
{
    return PyLongRangeIter_ZeroToN(long_obj);
}
```

Now that we have `long_iter` defined, we can place the function on the `tp_iter` slot of `PyLong_Type` that enables the required iterability on integers.

```
PyTypeObject PyLong_Type = {
    ...

    "int",                                      /* tp_name */
    offsetof(PyLongObject, ob_digit),           /* tp_basicsize */
    sizeof(digit),                              /* tp_itemsize */
    ...
    long_iter,                                  /* tp_iter */
    ...
};
```

## Consolidated flow

Once we have everything in place, the entire flow goes like this -

Every time an integer is iterated, using any iteration method - for example `for ... in`, it would check the `tp_iter` of the `PyLongType` and since now it holds the function pointer `long_iter`, the function will be invoked. This invocation will return an iterator object of type `longrangeiterobject` with a fixed start, index, and step values - which in pythonic terms is effectively a `range(0, n, 1)`. Hence the `for x in 7` is inherently evaluated as `for x in range(0, 7, 1)` allowing us to iterate integers.

> These changes are also hosted on a remote branch [cpython@02-long-iter](https://github.com/arpitbbhayani/cpython/tree/02-long-iter) and Pull Request holding the `diff` can be found [here](https://github.com/arpitbbhayani/cpython/pull/7).

# Integer iteration in action

Once we build a new python binary with the aforementioned changes, we can see iterable integers in actions. Now when we do `for x in 7`, instead of raising an exception, it actually iterates through values `[0, 7)`.

```
>>> for i in 7: print(i, end=" ");
0 1 2 3 4 5 6

# Since integers are now iterable, we can create a list of [0, 7) using `list`
# Internally `list` tries to iterate on the given object i.e. `7`
# now that the iteration is defined as [0, 7) we get the list from
# from iteration, instead of an exception
>>> list(7)
[0, 1, 2, 3, 4, 5, 6]
```

# Why it is not a good idea

Although it seems fun, and somewhat useful, to have iterable integers, it is really not a great idea. The core reason for this is that it makes unpacking unpredictable. Unpacking is when you unpack an iterable and assign it to multiple variables. For example: `a, b = 3, 4` will assign 3 to a and 4 to b. So assigning `a, b = 7` should be an error because there is just one value on the right side and multiple on the left.

Unpacking treats right-hand size as iterable and tries to iterate on it; and now since Integers are iterable the right-hand side, post iteration yields 7 values which the left-hand side has mere 2 variables; Hence it raises an exception `ValueError: too many values to unpack (expected 2)`.

Things would work just fine if we do `a, b = 2` as now the right-hand side, post iteration, has two values, and the left-hand side has two variables. Thus two very similar statements result in two very different outcomes, making unpacking unpredictable.

```
>>> a, b = 7
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: too many values to unpack (expected 2)

>>> a, b = 2
>>> a, b
0, 1
```

# Conclusion

In this essay, we modified the Python’s source code and made integers iterable. Even though it is not a good idea to do so, but it is fun to play around with the code and make changes in our favorite programming language. It helps us get a detailed idea about core python implementation and may pave the way for us to become a Python core developer. This is one of many articles in Python Internals series - [How python implements super long integers?](https://arpitbhayani.me/blogs/super-long-integers) and [Python Caches Integers](https://arpitbhayani.me/blogs/python-caches-integers).

# References

- [PyTypeObject](https://docs.python.org/3/c-api/type.html#c.PyTypeObject)
- [Python Type Objects](https://docs.python.org/3/c-api/typeobj.html)
- [Python Iterator Protocol](https://docs.python.org/3/c-api/iter.html)
- [CPython with long_iter](https://github.com/arpitbbhayani/cpython/pull/7)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/inheritance-c

Stucture composition in C - implementing inheritence in C

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

# Stucture composition in C - implementing inheritence in C

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

C language does not support inheritance however it does support Structure Compositions which can be tweaked to serve use-cases requiring parent-child relationships. In this article, we find out how Structure Compositions help us emulate inheritance in C and keep our code extensible. We will also find how it powers two of the most important things to have ever been invented in the field of computer science.

# What is structure composition?

Structure Composition is when we put one structure within another, not through its pointer but as a native member - something like this

```
// this structure defines a node of a linked list and
// it only holds the pointers to the next and the previous
// nodes in the linked list.
struct list_head {
	struct list_head *next; // pointer to the node next to the current one
	struct list_head *prev; // pointer to the node previous to the current one
};

// list_int holds an list_head and an integer data member
struct list_int {
	struct list_head list;  // common next and prev pointers
	int value;              // specific member as per implementation
};

// list_int holds an list_head and an char * data member
struct list_str {
	struct list_head list;  // common next and prev pointers
	char * str;             // specific member as per implementation
};
```

In the example above, we define a node of a linked list using structure composition. Usually, a linked list node has 3 members - two pointers to adjacent nodes (next and previous) and a third one could either be the data or a pointer to it. The defining factor of a linked list is the two pointers that logically form a chain of nodes. To keep things abstract we create a struct named `list_head` which holds these two pointers `next` and `prev` and omits the specifics i.e. data.

Using `list_head` structure, if we were to define a node of a linked list holding an integer value we could create another struct, named `list_int` that holds a member of type `list_head` and an integer value `value`. The next and previous pointers are brought into this struct through `list_head list` and could be referred to as `list.next` and `list.prev`.

> There is a very genuine reason for picking such weird names for a linked list node and members of structures; the reason to do so will be cleared in the later sections of this essay.

Because of the above structure definition, building a linked list node holding of any type becomes a breeze. For example, a node holding string could be quickly defined as a struct `list_str` having `list_head` and a `char *`. This ability to extend `list_head` and build a node holding data of any type and any specifics make low-level code simple, uniform, and extensible.

## Memory Representation of `list_int`

Structures in C are not padded and they do not even hold any meta information, not even for the member names; hence during allocation, they are allocated the space just enough to hold the actual data.

![https://user-images.githubusercontent.com/4745789/83953834-694a6a00-a861-11ea-8ff7-fa69af6af7d6.png](https://user-images.githubusercontent.com/4745789/83953834-694a6a00-a861-11ea-8ff7-fa69af6af7d6.png)

In the illustration above we see how members of `list_int` are mapped on the allocated space - required by its individual members. It is allocated a contiguous space of 12 bytes - 4 bytes for each of the two pointers and another 4 bytes for the integer value. The contiguity of space allocation and order of members during allocation could be verified by printing out their addresses as shown below.

```
void print_addrs() {
    // creating a node of the list_int holding value 41434
    struct list_int *ll = new_list_int(41434);

    // printing the address of individual members
    printf("%p: head\n",             head);
    printf("%p: head->list.next\n",  &((head->list).next));
    printf("%p: head->list.prev\n",  &((head->list).prev));
    printf("%p: head->value\n",      &(head->value));
}

~ $ make && ./a.out
0x4058f0: head
0x4058f0: head->list.next
0x4058f4: head->list.prev
0x4058f8: head->value
```

We clearly see all the 3 members, occupying 12 bytes contiguous memory segments in order of their definition within the struct.

> The above code was executed on a machine where the size of integer and pointers were 4 bytes each. The results might differ depending on the machine and CPU architecture.

## Casting pointers pointing to struct

In C language, when a pointer to a struct is cast to a pointer to another struct, the engine maps the individual members of a target struct type, depending on their order and offsets, on to the slice of memory of the source struct instance.

When we cast `list_int *` into `list_head *`, the engine maps the space required by target type i.e. `list_head` on space occupied by `list_int`. This means it maps the 8 bytes required by `list_head` on the first 8 bytes occupied by `list_int` instance. Going by the memory representation discussed above, we find that the first 8 bytes of `list_int` are in fact `list_head`, and hence casting `list_int *` to `list_head *` is effectively just referencing the `list_head` member of `list_int` through a new variable.

![https://user-images.githubusercontent.com/4745789/83943610-2e254800-a81b-11ea-8b25-056e1b1df85e.png](https://user-images.githubusercontent.com/4745789/83943610-2e254800-a81b-11ea-8b25-056e1b1df85e.png)

This effectively builds a parent-child relationship between the two structs where we can safely typecast a child `list_int` to its parent `list_head`.

> It is important to note here that the parent-child relationship is established only because the first member of `list_int` is of type `list_head`. it would not have worked if we change the order of members in `list_int`.

# How does this drive inheritance?

As established above, by putting one struct within another as its first element we are effectively creating a parent-child relationship between the two. Since this gives us an ability to safely typecast child to its parent we can define functions that accept a pointer to parent struct as an argument and perform operations that do not really require to deal with specifics. This allows us to **NOT** rewrite the functional logic for every child extensions and thus avoid redundant code.

From the context we have set up, say we want to write a function that adds a node between the two in a linked list. The core logic to perform this operation does not really need to deal with any specifics all it takes is a few pointer manipulations of `next` and `prev`. Hence, we could just define the function accepting arguments of type `list_head *` and write the function as

```
/*
 * Insert a new entry between two known consecutive entries.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
static void __list_add(struct list_head *new,
                       struct list_head *prev,
                       struct list_head *next)
{
    next->prev = new;
    new->next = next;
    new->prev = prev;
    prev->next = new;
}
```

Since we can safely typecase `list_int *` and `list_str *` to `list_head *` we can pass any of the specific implementations the function `__list_add` and it would still add the node between the other two seamlessly.

Since the core operations on linked lists only require pointer manipulations, we can define these operations as functions accepting `list_head *` instead of specific types like `list_int *`. Thus we need not write similar functions for specifics. A function to delete a node could be written as

```
/*
 * Delete a list entry by making the prev/next entries
 * point to each other.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
static inline void __list_del(struct list_head * prev, struct list_head * next)
{
    next->prev = prev;
    prev->next = next;
}
```

Other linked list utilities like _adding a node to tail_, _swapping nodes_, _splicing the list_, _rotating the list_, etc only require manipulations of `next` and `prev` pointers. Hence they could also be written in a very similar way i.e accepting `list_head *` and thus eliminating the need to reimplement function logic for every single child implementation.

This behavior is very similar to how inheritance in modern OOP languages, like Python and Java, work where the child is allowed to invoke any parent function.

# Who uses structure compositions?

There are a ton of practical usage of using Structure Compositions but the most famous ones are

## Linux Kernel

In order to keep things abstract and extensible, Linux Kernel uses Structure Composition at several places. One of the most important places where it uses composition is for managing and maintaining Linked Lists, exactly how we saw things above. The struct definitions and code snippets are taken as-is from the [Kernel’s source code](https://elixir.bootlin.com/linux/latest/source/include/linux/list.h), and hence the structure and variable names look different than usual.

## Python Type and Object Hierarchy

Python, one of the most important languages in today’s world, uses Structure Composition to build Type Hierarchy. Python defines a root structure called `PyObject` which holds reference count, defining the number of places from which the object is referenced - and object type - determining the type of the object i.e. `int`, `str`, `list`, `dict`, etc.

```
typedef struct _object {
    Py_ssize_t     ob_refcnt;  // holds reference count of the object
    PyTypeObject   *ob_type;   // holds the type of the object
} PyObject;
```

Since Python wants these fields to be present in every single object that is created during runtime, it uses structure composition to ensure that objects like integers, floats, string, etc put `PyObject` as their first element and thus establishing a parent-child relationship. A Float object in Python is defined as

```
#define PyObject_HEAD PyObject ob_base;

typedef struct {
    PyObject_HEAD
    double ob_fval;    // holds the actual float value
} PyFloatObject;
```

Now writing utility functions that increments and decrements references count on every access of any object could be written as just a single function accepting `PyObject *` as shown below

```
static inline void _Py_INCREF(PyObject *op) {
    op->ob_refcnt++;
}
```

Thus we eradicate a need of rewriting `INCREF` for every single object type and just write it once for `PyObject` and it will work for every single Python object type that is extended through `PyObject`.

# References

- [LinkedList in Linux Source Code](https://elixir.bootlin.com/linux/latest/source/include/linux/list.h)
- [PyObject - Python Internals Documentation](https://docs.python.org/3/c-api/structures.html#c.PyObject)
- [PyFloatObject - Python Internals Documentation](https://docs.python.org/3/c-api/float.html)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/rum

The RUM Conjecture

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

# The RUM Conjecture

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

The RUM Conjecture states that we cannot design an access method for a storage system that is optimal in all the following three aspects - Reads, Updates, and, Memory. The conjecture puts forth that we always have to trade one to make the other two optimal and this makes the three constitutes a competing triangle, very similar to the famous [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem).

![https://user-images.githubusercontent.com/4745789/83323578-6eb21e00-a27d-11ea-941b-43e875169c97.png](https://user-images.githubusercontent.com/4745789/83323578-6eb21e00-a27d-11ea-941b-43e875169c97.png)

# Access Method

Data access refers to an ability to access and retrieve data stored within a storage system driven by an optional storage engine. Usually, a storage system is designed to be optimal for serving a niche use case and achieve that by carefully and judiciously deciding the memory and disk storage requirements, defining well-structured access and retrieval pattern, designing data structures for primary and auxiliary data and picking additional techniques like compression, encryption, etc. These decisions define, and to some extent restricts, the possible ways the storage engine can read and update the data in the system.

# RUM Overheads

An ideal storage system would be the one that has an access method that provides lowest Read Overhead, minimal Update Cost, and does not require any extra Memory or Storage space, over the main data. In the real-world, achieving this is near impossible and that is something that is dictated by this conjecture.

### Read Overhead

Read Overhead occur when the storage engine performs reads on auxiliary data to fetch the required intended main data. This usually happens when we use an auxiliary data structure like a Secondary Index to speed up reads. The reads happening on this auxiliary structure constitutes read overheads.

Read Overhead is measured through Read Amplification and it is defined as the ratio between the total amount of data read (main + auxiliary) and the amount of main data intended to be read.

### Update Overhead

Update Overhead occur when the storage engine performs writes on auxiliary data or on some unmodified main data along with intended updates on the main data. A typical example of Update Overheads is the writes that happen on an auxiliary structure like Secondary Index alongside the write happening on intended main data.

Update Overhead is measured through Write Amplification and it is defined as the ratio between the total amount of data written (main + auxiliary) and the amount of main data intended to be updated.

### Memory Overhead

Memory overhead occurs when the storage system uses an auxiliary data structure to speed up reads, writes, or to serve common access patterns. This storage is in addition to the storage needs of the main data.

Memory Overhead is measured through Space Amplification and it is defined as the ratio between the space utilized for auxiliary and main data and space utilized by the main data.

# The Conjecture

The RUM Conjecture, in a formal way, states that

> An access method that can set an upper bound for two out of the read, update, and memory overheads, also sets a lower bound for the third overhead.

This is not a hard rule that is followed and hence it is not a theorem but a conjecture - widely observed but not proven. But we can safely keep this in mind while designing the next big storage system serving a use case.

# Categorizing Storage Systems

Now that we have seen RUM overheads and the RUM Conjecture we take a look at examples of Storage Systems that classify into one of the three types.

## Read Optimised

Read Optimised storage systems offer very low read overhead but require some extra auxiliary space to gain necessary performance that again comes at a cost of updates required to keep auxiliary data in sync with main data which adds to update overheads. When the updates, on main data, become frequent the performance of a read optimized storage system takes a dip.

A fine example of a read optimized storage system is the one that supports Point Indexes, also called Hash-based indexes, offering constant time access. The systems that provide logarithmic time access, like [B-Trees](https://en.wikipedia.org/wiki/B-tree) and [Skiplists](https://en.wikipedia.org/wiki/Skip_list), also fall into this category.

## Update Optimised

Update Optimised storage systems offer very low Update Overhead by usually using an auxiliary space holding differential data (delta) and flushing them over main data in a bulk operation. The need of having an auxiliary data to keep track of delta to perform a bulk update adds to Memory Overhead.

A few examples of Update Optimised systems are [LSM Trees](https://en.wikipedia.org/wiki/Log-structured_merge-tree), [Partitioned B Trees](http://cs.emis.de/LNI/Proceedings/Proceedings26/GI-Proceedings.26-47.pdf), and [FD Tree](http://pages.cs.wisc.edu/~yinan/fdtree.html). These structures offer very good performance for an update-heavy system but suffer from an increased read and space overheads. While reading data from LSM Tree, the engine needs to perform read on all the tiers and then perform a conflict resolution, and maintaining tiers of data itself is a huge Space Overhead.

## Memory Optimised

Memory Optimised storage systems are designed to minimize auxiliary memory required for access and updates on the main data. To be memory-optimized the systems usually use compress the main data and auxiliary storages, or allow some error rate, like false positives.

A few examples of Memory Optimises systems are lossy index structures like [Bloom Filters](https://en.wikipedia.org/wiki/Bloom_filter), [Count-min sketches](https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch), Lossy encodings, and Sparse Indexes. Keeping either main or auxiliary data compressed, to be memory efficient, the system takes a toll on writes and reads as they now have additionally performed compression and decompressions adding to the Update and Read overheads.

![https://user-images.githubusercontent.com/4745789/83323560-55a96d00-a27d-11ea-9d33-4001c672b920.png](https://user-images.githubusercontent.com/4745789/83323560-55a96d00-a27d-11ea-9d33-4001c672b920.png)

Storage System examples for RUM Conjecture

# Block-based Clustered Indexing

Block-based Clustered Indexing, sits comfortably between these three optimized systems types. It is not read Read efficient but also efficient on Updates and Memory. It builds a very short tree for its auxiliary data, by storing a few pointers to pages and since the data is clustered i.e. the main data itself is stored in the index, the system does not go to fetch the main data from the main storage and hence provides a minimal Read overhead.

# Being RUM Adaptive

Storage systems have always been rigid with respect to the kind of use cases it aims to solve. the application, the workload, and the hardware should dictate how we access our data, and not the constraints of our systems. Storage systems could be designed to be RUM Adaptive and they should possess an ability to be tuned to reduce the RUM overheads depending on the data access pattern and computation knowledge. RUM Adaptive storage systems are part of the discussion for some other day.

# Conclusion

There will always be trade-offs, between Read, Update, and Memory, while either choosing one storage system over others; the RUM conjecture facilitates and to some extent formalizes the entire process. Although this is just a conjecture, it still helps us disambiguate and make an informed, better and viable decision that will go a long way.

This essay was heavily based on the original research paper introducing The RUM Conjecture.

# References

- [Designing Access Methods: The RUM Conjecture](https://stratos.seas.harvard.edu/files/stratos/files/rum.pdf)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/consistent-hashing

Consistent Hashing - explanation and implementation

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [System Design](/knowledge-base/system-design)
- [Algorithms](/knowledge-base/system-design)

- [KB](/knowledge-base)
- [Syst...](/knowledge-base/system-design)
- [Algorithms](/knowledge-base/system-design)

# Consistent Hashing - explanation and implementation

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Consistent hashing is a hashing technique that performs really well when operated in a dynamic environment where the distributed system scales up and scales down frequently. The core concept of Consistent Hashing was introduced in the paper [Consistent Hashing and RandomTrees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web](https://www.akamai.com/us/en/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf) but it gained popularity after the famous paper introducing DynamoDB - [Dynamo: Amazon’s Highly Available Key-value Store](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf). Since then the consistent hashing gained traction and found a ton of use cases in designing and scaling distributed systems efficiently. The two famous examples that exhaustively use this technique are Bit Torrent, for their peer-to-peer networks and Akamai, for their web caches. In this article we dive deep into the need of Consistent Hashing, the internals of it, and more importantly along the way implement it using arrays and [Binary Search](https://en.wikipedia.org/wiki/Binary_search_algorithm).

# Hash Functions

Before we jump into the core Consistent Hashing technique we first get a few things cleared up, one of which is Hash Functions. Hash Functions are any functions that map value from an arbitrarily sized domain to another fixed-sized domain, usually called the Hash Space. For example, mapping URLs to 32-bit integers or web pages’ HTML content to a 256-byte string. The values generated as an output of these hash functions are typically used as keys to enable efficient lookups of the original entity.

An example of a simple hash function is a function that maps a 32-bit integer into an 8-bit integer hash space. The function could be implemented using the arithmetic operator `modulo` and we can achieve this by taking a `modulo 256` which yields numbers in the range `[0, 255]` taking up 8-bits for its representation. A hash function, that maps keys to such integer domain, more often than not applies the `modulo N` so as to restrict the values, or the hash space, to a range `[0, N-1]`.

A good hash function has the following properties

- The function is computationally efficient and the values generated are easy for lookups
- The function, for most general use cases, behaves like a pseudorandom generator that spreads data out evenly without any noticeable correlation

Now that we have seen what a hash function is, we take a look into how we could use them and build a somewhat scalable distributed system.

# Building a distributed storage system

Say we are building a distributed storage system in which users can upload files and access them on demand. The service exposes the following APIs to the users

- `upload` to upload the file
- `fetch` to fetch the file and return its content

Behind the scenes the system has Storage Nodes on which the files are stored and accessed. These nodes expose the functions `put_file` and `fetch_file` that puts and gets the file content to/from the disk and sends the response to the main API server which in turn sends it back to the user.

To sustain the initial load, the system has 5 Stogare Nodes which stores the uploaded files in a distributed manner. Having multiple nodes ensures that the system, as a whole, is not overwhelmed, and the storage is distributed almost evenly across.

When the user invokes `upload` function with the path of the file, the system first needs to identify the storage node that will be responsible for holding the file and we do this by applying a hash function to the path and in turn getting the storage node index. Once we get the storage node, we read the content of the file and put that file on the node by invoking the `put_file` function of the node.

```
# storage_nodes holding instances of actual storage node objects
storage_nodes = [
    StorageNode(name='A', host='10.131.213.12'),
    StorageNode(name='B', host='10.131.217.11'),
    StorageNode(name='C', host='10.131.142.46'),
    StorageNode(name='D', host='10.131.114.17'),
    StorageNode(name='E', host='10.131.189.18'),
]


def hash_fn(key):
    """The function sums the bytes present in the `key` and then
    take a mod with 5. This hash function thus generates output
    in the range [0, 4].
    """
    return sum(bytearray(key.encode('utf-8'))) % 5


def upload(path):
    # we use the hash function to get the index of the storage node
    # that would hold the file
    index = hash_fn(path)

    # we get the StorageNode instance
    node = storage_nodes[index]

    # we put the file on the node and return
    return node.put_file(path)


def fetch(path):
    # we use the hash function to get the index of the storage node
    # that would hold the file
    index = hash_fn(path)

    # we get the StorageNode instance
    node = storage_nodes[index]

    # we fetch the file from the node and return
    return node.fetch_file(path)
```

The hash function used over here simply sums the bytes and takes the modulo by `5` (since there are 5 storage nodes in the system) and thus generating the output in the hash space `[0, 4]`. This output value now represents the index of the storage engine that will be responsible for holding the file.

Say we have 5 files ‘f1.txt’, ‘f2.txt’, ‘f3.txt’, ‘f4.txt’, ‘f5.txt’ if we apply the hash function to these we find that they are stored on storage nodes E, A, B, C, and D respectively.

Things become interesting when the system gains some traction and it needs to be scaled to 7 nodes, which means now the hash function should do `mod 7` instead of a `mod 5`. Changing the hash function implies changing the mapping and association of files with storage nodes. We first need to administer the new associations and see which files required to be moved from one node to another.

With the new hash function the same 5 files ‘f1.txt’, ‘f2.txt’, ‘f3.txt’, ‘f4.txt’, ‘f5.txt’ will now be associated with storage nodes D, E, F, G, A. Here we see that changing the hash function requires us to move every single one of the 5 files to a different node.

![File association changed](https://user-images.githubusercontent.com/4745789/82746677-16c47480-9db0-11ea-8dea-7b5a3cb73e91.png)

If we have to change the hash function every time we scale up or down and if this requires us to move not all but even half of the data, the process becomes super expensive and in longer run infeasible. So we need a way to minimize the data movement required during scale-ups or scale-downs, and this is where Consistent Hashing fits in and minimizes the required data transfer.

# Consistent Hashing

The major pain point of the above system is that it is prone to events like scale-ups and scale-downs as it requires a lot of alterations in associations. These associations are purely driven by the underlying Hash Function and hence if we could somehow make this hash function independent of the number of the storage nodes in the system, we address this flaw.

Consistent Hashing addresses this situation by keeping the Hash Space huge and constant, somewhere in the order of `[0, 2^128 - 1]` and the storage node and objects both map to one of the slots in this huge Hash Space. Unlike in the traditional system where the file was associated with storage node at index where it got hashed to, in this system the chances of a collision between a file and a storage node are infinitesimally small and hence we need a different way to define this association.

Instead of using a collision-based approach we define the association as - the file will be associated with the storage node which is present to the immediate right of its hashed location. Defining association in this way helps us

- keep the hash function independent of the number of storage nodes
- keep associations relative and not driven by absolute collisions

![Associations in Consistent Hashing](https://user-images.githubusercontent.com/4745789/82748149-4d54bc00-9dbd-11ea-8f06-6710a5c98f20.png)

> Consistent Hashing on an average requires only k/n units of data to be migrated during scale up and down; where k is the total number of keys and n is the number of nodes in the system.

A very naive way to implement this is by allocating an array of size equal to the Hash Space and putting files and storage node literally in the array on the hashed location. In order to get association we iterate from the item’s hashed location towards the right and find the first Storage Node. If we reach the end of the array and do not find any Storage Node we circle back to index 0 and continue the search. The approach is very easy to implement but suffers from the following limitations

- requires huge memory to hold such a large array
- finding association by iterating every time to the right is `O(hash_space)`

A better way of implementing this is by using two arrays: one to hold the Storage Nodes, called `nodes` and another one to hold the positions of the Storage Nodes in the hash space, called `keys`. There is a one-to-one correspondence between the two arrays - the Storage Node `nodes[i]` is present at position `keys[i]` in the hash space. Both the arrays are kept sorted as per the `keys` array.

## Hash Function in Consistent Hashing

We define `total_slots` as the size of this entire hash space, typically of the order `2^256` and the hash function could be implemented by taking [SHA-256](https://en.wikipedia.org/wiki/SHA-2) followed by a `mod total_slots`. Since the `total_slots` is huge and a constant the following hash function implementation is independent of the actual number of Storage Nodes present in the system and hence remains unaffected by events like scale-ups and scale-downs.

```
def hash_fn(key: str, total_slots: int) -> int:
    """hash_fn creates an integer equivalent of a SHA256 hash and
    takes a modulo with the total number of slots in hash space.
    """
    hsh = hashlib.sha256()

    # converting data into bytes and passing it to hash function
    hsh.update(bytes(key.encode('utf-8')))

    # converting the HEX digest into equivalent integer value
    return int(hsh.hexdigest(), 16) % total_slots
```

## Adding a new node in the system

When there is a need to scale up and add a new node in the system, in our case a new Storage Node, we

- find the position of the node where it resides in the Hash Space
- populate the new node with data it is supposed to serve
- add the node in the Hash Space

When a new node is added in the system it only affects the files that hash at the location to the left and associated with the node to the right, of the position the new node will fit in. All other files and associations remain unaffected, thus minimizing the amount of data to be migrated and mapping required to be changed.

![Adding a new node in the system - Consistent Hashing](https://user-images.githubusercontent.com/4745789/82751279-c959fe80-9dd3-11ea-86de-62d162519262.png)

From the illustration above, we see when a new node K is added between nodes B and E, we change the associations of files present in the segment B-K and assign them to node K. The data belonging to the segment B-K could be found at node E to which they were previously associated with. Thus the only files affected and that needs migration are in the segment B-K; and their association changes from node E to node K.

In order to implement this at a low-level using `nodes` and `keys` array, we first get the position of the new node in the Hash Space using the hash function. We then find the index of the smallest key greater than the position in the sorted `keys` array using binary search. This index will be where the key and the new Storage node will be placed in `keys` and `nodes` array respectively.

```
def add_node(self, node: StorageNode) -> int:
    """add_node function adds a new node in the system and returns the key
    from the hash space where it was placed
    """

    # handling error when hash space is full.
    if len(self._keys) == self.total_slots:
        raise Exception("hash space is full")

    key = hash_fn(node.host, self.total_slots)

    # find the index where the key should be inserted in the keys array
    # this will be the index where the Storage Node will be added in the
    # nodes array.
    index = bisect(self._keys, key)

    # if we have already seen the key i.e. node already is present
    # for the same key, we raise Collision Exception
    if index > 0 and self._keys[index - 1] == key:
        raise Exception("collision occurred")

    # Perform data migration

    # insert the node_id and the key at the same `index` location.
    # this insertion will keep nodes and keys sorted w.r.t keys.
    self.nodes.insert(index, node)
    self._keys.insert(index, key)

    return key
```

## Removing a new node from the system

When there is a need to scale down and remove an existing node from the system, we

- find the position of the node to be removed from the Hash Space
- populate the node to the right with data that was associated with the node to be removed
- remove the node from the Hash Space

When a node is removed from the system it only affects the files associated with the node itself. All other files and associations remain unaffected, thus minimizing the amount of data to be migrated and mapping required to be changed.

![Removing a new node from the system - Consistent Hashing](https://user-images.githubusercontent.com/4745789/82751261-b0e9e400-9dd3-11ea-81ee-3fd3f0187857.png)

From the illustration above, we see when node K is removed from the system, we change the associations of files associated with node K to the node that lies to its immediate right i.e. node E. Thus the only files affected and needs migration are the ones associated with node K.

In order to implement this at a low-level using `nodes` and `keys` array, we get the index where the node K lies in the `keys` array using binary search. Once we have the index we remove the key from the `keys` array and Storage Node from the `nodes` array present on that index.

```
def remove_node(self, node: StorageNode) -> int:
    """remove_node removes the node and returns the key
    from the hash space on which the node was placed.
    """

    # handling error when space is empty
    if len(self._keys) == 0:
        raise Exception("hash space is empty")

    key = hash_fn(node.host, self.total_slots)

    # we find the index where the key would reside in the keys
    index = bisect_left(self._keys, key)

    # if key does not exist in the array we raise Exception
    if index >= len(self._keys) or self._keys[index] != key:
        raise Exception("node does not exist")

    # Perform data migration

    # now that all sanity checks are done we popping the
    # keys and nodes at the index and thus removing the presence of the node.
    self._keys.pop(index)
    self.nodes.pop(index)

    return key
```

## Associating an item to a node

Now that we have seen how consistent hashing helps in keeping data migration, during scale-ups and scale-downs, to a bare minimum; it is time we see how to efficiently we can find the “node to the right” for a given item. The operation to find the association has to be super fast and efficient as it is something that will be invoked for every single read and write that happens on the system.

To implement this at low-level we again take leverage of binary search and perform this operation in `O(log(n))`. We first pass the item to the hash function and fetch the position where the item is hashed in the hash space. This position is then binary searched in the `keys` array to obtain the index of the first key which is greater than the position (obtained from the hash function). if there are no keys greater than the position, in the `keys` array, we circle back and return the 0th index. The index thus obtained will be the index of the storage node in the `nodes` array associated with the item.

```
def assign(self, item: str) -> str:
    """Given an item, the function returns the node it is associated with.
    """
    key = hash_fn(item, self.total_slots)

    # we find the first node to the right of this key
    # if bisect_right returns index which is out of bounds then
    # we circle back to the first in the array in a circular fashion.
    index = bisect_right(self._keys, key) % len(self._keys)

    # return the node present at the index
    return self.nodes[index]
```

The source code with the implementation of Consistent Hashing in Python could be found at [github.com/arpitbbhayani/consistent-hashing](https://github.com/arpitbbhayani/consistent-hashing/blob/master/consistent-hashing.ipynb).

# Conclusion

Consistent Hashing is one of the most important algorithms to help us horizontally scale and manage any distributed system. The algorithm does not only work in sharded systems but also finds its application in load balancing, data partitioning, managing server-based sticky sessions, routing algorithms, and many more. A lot of databases owe their scale, performance, and ability to handle the humongous load to Consistent Hashing.

# References

- [Hash Functions - Wikipedia](https://en.wikipedia.org/wiki/Hash_function)
- [Consistent Hashing - Wikipedia](https://en.wikipedia.org/wiki/Consistent_hashing)
- [Consistent Hashing - Stanford](https://web.stanford.edu/class/cs168/l/l1.pdf)
- [Consistent Hashing and RandomTrees](https://www.akamai.com/us/en/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf)
- [Dynamo: Amazon’s Highly Available Key-value Store](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/python-caches-integers

Python caches smaller integers - a deep dive into CPython

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Language Internals](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

- [KB](/knowledge-base)
- [Lang...](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

# Python caches smaller integers - a deep dive into CPython

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

An integer in Python is not a traditional 2, 4, or 8-byte implementation but rather it is implemented as an array of digits in base 230 which enables Python to support [super long integers](https://arpitbhayani.me/blogs/super-long-integers). Since there is no explicit limit on the size, working with integers in Python is extremely convenient as we can carry out operations on very long numbers without worrying about integer overflows. This convenience comes at a cost of allocation being expensive and trivial operations like addition, multiplication, division being inefficient.

Each integer in python is implemented as a C structure illustrated below.

```
struct _longobject {
    ...
    Py_ssize_t    ob_refcnt;      // <--- holds reference count
    ...
    Py_ssize_t    ob_size;        // <--- holds number of digits
    digit         ob_digit[1];    // <--- holds the digits in base 2^30
};
```

It is observed that smaller integers in the range -5 to 256, are used very frequently as compared to other longer integers and hence to gain performance benefit Python preallocates this range of integers during initialization and makes them singleton and hence every time a smaller integer value is referenced instead of allocating a new integer it passes the reference of the corresponding singleton.

Here is what [Python’s official documentation](https://docs.python.org/3/c-api/long.html#c.PyLong_FromLong) says about this preallocation

> The current implementation keeps an array of integer objects for all integers between -5 and 256 when you create an int in that range you actually just get back a reference to the existing object.

In the CPython’s [source code](https://github.com/python/cpython/) this optimization can be traced in the macro `IS_SMALL_INT` and the function [`get_small_int`](https://github.com/python/cpython/blob/master/Objects/longobject.c#L40) in [longobject.c](https://github.com/python/cpython/blob/master/Objects/longobject.c). This way python saves a lot of space and computation for commonly used integers.

# Verifying smaller integers are indeed a singleton

For a CPython implementation, the in-built [`id` function](https://docs.python.org/3/library/functions.html#id) returns the address of the object in memory. This means if the smaller integers are indeed singleton then the `id` function should return the same memory address for two instances of the same value while multiple instances of larger values should return different ones, and this is indeed what we observe

```
>>> x, y = 36, 36
>>> id(x) == id(y)
True


>>> x, y = 257, 257
>>> id(x) == id(y)
False
```

The singletons can also be seen in action during computations. In the example below, we reach the same target value `6` by performing two operations on three different numbers, 2, 4, and 10, and we see the `id` function returning the same memory reference in both the cases.

```
>>> a, b, c = 2, 4, 10
>>> x = a + b
>>> y = c - b
>>> id(x) == id(y)
True
```

# Verifying if these integers are indeed referenced often

We have established that Python indeed is consuming smaller integers through their corresponding singleton instances, without reallocating them every time. Now we verify the hypothesis that Python indeed saves a bunch of allocations during its initialization through these singletons. We do this by checking the reference counts of each of the integer values.

## Reference Counts

Reference count holds the number of different places there are that have a reference to the object. Every time an object is referenced the `ob_refcnt`, in its structure, is increased by `1`, and when dereferenced the count is decreased by `1`. When the reference count becomes `0` the object is garbage collected.

In order to get the current reference count of an object, we use the function `getrefcount` from the `sys` module.

```
>>> ref_count = sys.getrefcount(50)
11
```

When we do this for all the integers in range -5 to 300 we get the following distribution

![Reference counts of interger values](https://user-images.githubusercontent.com/4745789/82141240-1e38ca80-9852-11ea-8133-fd8e1b26fc01.png)

The above graph suggests that the reference count of smaller integer values is high indicating heavy usage and it decreases as the value increases which asserts the fact that there are many objects referencing smaller integer values as compared to larger ones during python initialization.

The value `0` is referenced the most - `359` times while along the long tail we see spikes in reference counts at powers of 2 i.e. 32, 64, 128, and 256. Python during its initialization itself requires small integer values and hence by creating singletons it saves about `1993` allocations.

The reference counts were computed on a freshly spun python which means during initialization it requires some integers for computations and these are facilitated by creating singleton instances of smaller values.

In usual programming, the smaller integer values are accessed much more frequently than larger ones, having singleton instances of these saves python a bunch of computation and allocations.

# References

- [Python Object Types and Reference Counts](https://docs.python.org/3/c-api/intro.html#objects-types-and-reference-counts)
- [How python implements super-long integers](https://arpitbhayani.me/blogs/super-long-integers)
- [Why Python is Slow: Looking Under the Hood](http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/fractional-cascading

Fractional Cascading - a way to speed up binary searches

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Fractional Cascading - a way to speed up binary searches

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Binary Search is an algorithm that finds the position of a target value in a sorted list. The algorithm exploits the fact that the list is sorted, and is devised such that is does not have to even look at all the `n` elements, to decide if a value is present or not. In the worst case, the algorithm checks the `log(n)` number of elements to make the decision.

Binary Search could be tweaked to output the position of the target value, or return the position of the smallest number greater than the target value i.e. position where the target value should have been present in the list.

Things become more interesting when we have to perform an iterative binary search on `k` lists in which we find the target value in each of the `k` lists independently. The problem statement could be formally defined as

> Given `k` lists of `n` sorted integers each, and a target value `x`, return the position of the smallest value greater than or equal to `x` in each of the `k` lists. Preprocessing of the list is allowed before answering the queries.

# The naive approach - k binary searches

The expected output of this iterative search is the position of the smallest value greater than or equal to `x` in each of the `k` lists. This is a classical Binary Search problem and hence in this approach, we fire `k` binary searches on `k` lists for the target value `x` and collect the positions.

![k-binary searches](https://user-images.githubusercontent.com/4745789/81492614-dbf21500-92b6-11ea-9f75-29eb3522186f.png)

Python has an in-built module called `bisect` which has the function `bisect_left` which outputs the smallest value greater than or equal to `x` in a list which is exactly what we need to output and hence python-based solution using this k-binary searches approach could be

```
import bisect

arr = [
    [21, 54, 64, 79, 93],
    [27, 35, 46, 47, 72],
    [11, 44, 62, 66, 94],
    [10, 35, 46, 79, 83],
]

def get_positions_k_bin_search(x):
    return [bisect.bisect_left(l, x) for l in arr]

>>> get_positions_k_bin_search(60)
[2, 4, 2, 3]
```

## Time and Space Complexity

Each of the `k` lists have size `n` and we know the time complexity of performing a binary search in one list of `n` elements is `O(log(n))`. Hence we deduce that the time complexity of this k-binary searches approach is `O(klog(n))`.

This approach does not really require any additional space and hence the space complexity is `O(1)`.

The k-binary searches approach is thus super-efficient on space but not so much on time. Hence by trading some space, we could reap some benefits on time, and on this exact principle, the unified binary search approach is based.

# Unified binary search

This approach uses some extra space, preprocessing and computations to reduce search time. The preprocessing actually involves precomputing the positions of all elements in all the `k` lists. This precomputation enables us to perform just one binary search and get the required precalculated positions in one go.

## Preprocess

The preprocessing is done in two phases; in the first phase, we compute a position tuple for each element and associate it with the same. In phase two of preprocessing, we create an auxiliary list containing all the elements of all the lists, on which we then perform a binary search for the given target value.

### Computing position tuple for each element

Position tuple is a `k` item tuple where every `i`th item denotes the position of the associated element in the `i`th list. We compute this tuple by performing a binary search on all the `k` lists treating the element as the target value.

From the example above, the position tuple of 4th element in the 4th list i.e 79 will be `[3, 5, 4, 3]` which denotes its position in all 4 lists. In list 1, 79 is at index `3`, in list 2, 79 is actually out of bounds but would be inserted at index `5` hence the output `5`, we could also have returned a value marking out of bounds, like `-2`, in list 3, 79 is not present but the smallest number greater than 79 is 94 and which is at index `4` and in list 4, 79 is present at index `3`. This makes the position tuple for 79 to be `[3, 5, 4, 3]`.

Given a 2-dimensional array `arr` we compute the position tuple for an element `(i, j)` by performing a binary search on all `k` lists as shown in python code below

```
for i, l in enumerate(arr):
    for j, e in enumerate(l):
        for k, m in enumerate(arr):
            positions[i][j][k] = int(bisect.bisect_left(m, e))
```

### Creating a huge list

Once we have all the position tuples and they are well associated with the corresponding elements, we create an auxiliary list of size `k * n` that holds all the elements from all the `k` lists. This auxiliary list is again kept sorted so that we could perform a binary search on it.

## Working

Given a target value, we perform a binary search in the above auxiliary list and get the smallest element greater than or equal to this target value. Once we get the element, we now get the associated position tuple. This position tuple is precisely the position of the target element in all the `k` lists. Thus by performing one binary search in this huge list, we are able to get the required positions.

![unified binary search](https://user-images.githubusercontent.com/4745789/81492609-ca107200-92b6-11ea-8fdf-999852f4d9b1.png)

## Complexity

We are performing binary search just once on the list of size `k * n` hence, the time complexity of this approach is `O(log(kn))` which is a huge improvement over the k-binary searches approach where it was `O(klog(n))`.

This approach, unlike k-binary searches, requires an additional space of `O(k.kn)` since each element holds `k` item position tuple and there are in all `k * n` elements.

Fractional cascading is something that gives us the best of both worlds by creating bridges between the lists and narrowing the scope of binary searches on subsequent iterations. Let’s find out how.

# Fractional Cascading

Fractional cascading is a technique through which we speed up the iterative binary searches by creating bridges between the lists. The main idea behind this approach is to dampen the need to perform binary searches on subsequent lists after performing the search on one.

In the k-binary searches approach, we solved the problem by performing `k` binary searches on `k` lists. If, after the binary search on the first list, we would have known a range within which the target value was present in the 2nd list, we would have limited our search within that subset which helps us save a bunch of computation time. The bridges, defined above, provides us with a shortcut to reach the subset of the other list where that target value would be present.

![Fractional Cascading the Idea](https://user-images.githubusercontent.com/4745789/81495324-241c3200-92cd-11ea-9d7d-9c9b0911071b.png)

Fractional cascading is just an idea through which we could speed up binary searches, implementations vary with respect to the underlying data. The bridges could be implemented using pointers, graphs, or array indexes.

## Preprocess

Preprocessing is a super-critical step in fractional cascading because it is responsible for speeding up the iterative binary searches. Preprocessing actually sets up all the bridges from all the elements from one list to the range of items in the lower list where the element could be found. These bridges then cascade to all the lists on the lower levels.

### Create Auxiliary Lists

The first step in pre-processing is to create `k` auxiliary lists from `k` original lists. These lists are created bottom-up which means lists on the lower levels are created first - `M(i+1)` is created before `M(i)`. An auxiliary list `M(i)` is created as a sorted list of elements of the original list `L(i)` and half of the previously created auxiliary list `M(i+1)`. The half elements of auxiliary lists are chosen by picking every other element from it.

![Create Auxiliary Lists](https://user-images.githubusercontent.com/4745789/81494077-8112ea80-92c3-11ea-9416-bb2422334744.png)

By picking every other element from lower-level lists, we fill the gaps in value ranges in the original list `L(i)`, giving us a uniform spread of values across all auxiliary lists. Another advantage of picking every other element is that we eradicate the need for performing binary searches on subsequent lists altogether. Now we only need to perform a binary search for list `M(0)` and for every other list, we only need to check the element we reach via the bridge and an element before that - a constant time comparison.

### Position tuples

A position tuple for Fractional Cascading is a 2 item tuple, associated with each element of the auxiliary list, where the first item is the position of the element in the original list on the same level - serving as the required position - and the second element is the position of the element in the auxiliary list on the lower level - serving as the bridge from one level to another.

![Create position pointerss](https://user-images.githubusercontent.com/4745789/89712282-83adda80-d9ad-11ea-888f-2b20d839252f.png)

The position tuple for each element in the auxiliary array can be created by doing a binary search on the original list and the auxiliary list on the lower level. Given a 2-dimensional array `arr` and auxiliary lists `m_arr` we compute the position tuples for element `(i, j)` by performing a binary search on all `k` original and auxiliary lists as shown in python code below

```
for i, l in enumerate(m_arr):
    for j, m in enumerate(m_arr[i]):
        pointers[i][j] = [
            bisect.bisect_left(arr[i], m_arr[i][j]),
            bisect.bisect_left(m_arr[i+1], m_arr[i][j]),
        ]
```

## Fractional Cascading in action

We start by performing a binary search on the first auxiliary list `M(0)` from which we get the element corresponding to the target value. The position tuple for this element contains the position corresponding to the original list `L(0)` and bridge that will take us to the list `M(1)`. Now when we move to the list `M(1)` through the bridge and have reached the index `b`.

Since auxiliary lists have uniform range spread, because of every other element being promoted, we are sure that the target value should be checked again at the index `b` and `b - 1`; because if the value was any lower it would have been promoted and bridged to other value and hence the trail we trace would be different from what we are tracing now.

Once we know which of the `b` and `b-1` index to pick (depending on the values at the index and the target value) we add the first item of the position tuple to the solution set and move the auxiliary list on the lower level and the entire process continues.

Once we reach the last auxiliary list and process the position tuple there and pick the element, our solution set contains the required positions and we can stop the iteration.

```
def get_locations_fractional_cascading(x):
    locations = []

    # the first and only binary search on the auxiliary list M[0]
    index = bisect.bisect_left(m_arr[0], x)

    # loc always holds the required location from the original list on same level
    # next_loc holds the bridge index on the lower level
    loc, next_loc = pointers[0][index]

    # adding loc to the solution
    locations.append(loc)

    for i in range(1, len(m_arr)):
        # we check for the element we reach through the bridge
        # and the one before it and make the decision to go with one
        # depending on the target value.
        if x <= m_arr[i][next_loc-1]:
            loc, next_loc = pointers[i][next_loc-1]
        else:
            loc, next_loc = pointers[i][next_loc]

        # adding loc to the solution
        locations.append(loc)

    # returning the required locations
    return locations
```

The entire working code could be found here [github.com/arpitbbhayani/fractional-cascading](https://github.com/arpitbbhayani/fractional-cascading/blob/master/fractional-cascading.ipynb)

## Time and space complexity

In Fractional Cascading, we perform binary search once on the auxiliary list `M(0)` and then make `k` constant comparisons for each of the subsequent levels; hence the time complexity is `O(k + log(n))`.

The auxiliary lists could at most contain all the elements from the original list plus `1/2 |L(n)| + 1/4 |L(n-1)| + 1/8 |L(n-2)| + ...` which is less than all elements of the original list combined. Thus the total size of the auxiliary list cannot exceed twice the original lists. The position tuple for each of the elements is also a constant 2 item tuple thus the space complexity of Fractional Cascading is `O(kn)`.

Thus Fractional Cascading has time complexity very close to the k-binary searches approach with a very low space complexity as compared to the unified binary searches approach; thus giving us the best of both worlds.

## Fractional Cascading in real world

Fractional Cascading is used in [FD-Trees](http://pages.cs.wisc.edu/~yinan/fdtree.html) which are used in databases to address the asymmetry of read-write speeds in tree indexing on the flash disk. Fractional cascading is typically used in [range search](https://en.wikipedia.org/wiki/Range_searching) data structures like [Segment Trees](https://en.wikipedia.org/wiki/Segment_tree) to speed up lookups and filters.

# References

- [Fractional Cascading - Wikipedia](https://en.wikipedia.org/wiki/Fractional_cascading)
- [Fractional Cascading - Original Paper by Bernard Chazelle and Leonidas Guibas](https://www.cs.princeton.edu/~chazelle/pubs/FractionalCascading1.pdf)
- [Fractional Cascading Revisited](http://www.cse.iitd.ernet.in/~ssen/journals/frac.pdf)
- [Fractional Cascading - Brown University](http://cs.brown.edu/courses/cs252/misc/resources/lectures/pdf/notes08.pdf)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/copy-on-write

What is copy-on-write and why it matters?

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Concepts - Single-node Databases](/knowledge-base/database-engineering)

# What is copy-on-write and why it matters?

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Copy-On-Write, abbreviately referred to as CoW suggests deferring the copy process until the first modification. A resource is usually copied when we do not want the changes made in either to be visible to the other. A resource here could be anything - an in-memory page, a database disk block, an item in a structure, or even the entire data structure.

CoW suggests that we first copy by reference and let both instances share the same resource and just before the first modification we clone the original resource and then apply the updates.

# Deep copying

The process of creating a pure clone of the resource is called [Deep Copying](https://en.wikipedia.org/wiki/Object_copying#Deep_copy) and it copies not only the immediate content but also all the remote resources that are referenced within it. Thus if we were to deep copy a [Linked List](https://en.wikipedia.org/wiki/Linked_list) we do not just copy the head pointer, rather we clone all the nodes of the list and create an entirely new list from the original one. A C++ function for deep copying a Linked List is as illustrated below

```
struct node* copy(struct node *head) {
    if (!head) {
        return NULL;
    }

    struct node *nhead = (struct node *) calloc(sizeof(struct node))
    nhead->val = head->val;

    struct node *p = head;
    struct node *q = nhead;

    while(p -> next) {
        q -> next = (struct node *) calloc(sizeof(struct node));
        q -> next -> val = p -> next -> val;
        p = p -> next;
        q = q -> next;
    }

    return nhead;
}
```

![deep copying a linked list](https://user-images.githubusercontent.com/4745789/80907205-76d87580-8d32-11ea-88a8-153a94d92d72.png)

Going by the details, we understand that deep copying is a very memory-intensive operation, and hence we try to not do it very often.

# Why Copy-on-Write

Copy-on-Write, as established earlier, suggests we defer the copy operation until the first modification is requested. The approach suits the best when the traversal and access operations vastly outnumber the mutations. CoW has a number of advantages, some of them are

## Perceived performance gain

By having a CoW, the process need not wait for the deep copy to happen, instead, it could directly proceed by just doing a copy-by-reference, where the resource is shared between the two, which is much faster than a deep copy and thus gaining a performance boost. Although we cannot totally get rid of deep copy because once some modification is requested the deep copy has to be triggered.

A particular example where we gain a significant performance boost is during the `fork` system call.

`fork` system call creates a child process that is a spitting copy of its parent. During this call, if the parent’s program space is huge and we trigger a deep copy, the time taken to create the child process will shoot up. But if we just do copy-by-reference the child process could be spun super fast. Once the child decides to make some modifications to its program space, then we trigger the deep copy.

## Better resource management

CoW gives us an optimistic way to manage memory. One peculiar property that CoW exploits are how, before any modifications to the copied instance, both the original and the copied resources are exactly the same. The readers, thus, cannot distinguish if they are reading from the original resource or the copied one.

Things change when the first modification is made to the copied instance and that’s where readers of the corresponding resource would expect to see things differently. But what if the copied instance is never modified?

Since there are no modifications, in CoW, the deep copy would never happen and hence the only operation that ever happened was a super-fast copy-by-reference of the original resource; and thus we just saved an expensive deep copy operation.

One very common pattern in OS is called [fork-exec](https://en.wikipedia.org/wiki/Fork%E2%80%93exec) in which a child process is forked as a spitting copy of its parent but it immediately executes another program, using `exec` family of functions, replacing its entire program space. Since the child does not intend to modify its program space ever, inherited from the parent, and just wants to replace it with the new program, deep copy plays no part and is a waste. So if we defer the deep copy operation until modification, the deep copy would never happen and we thus save a bunch of memory and CPU cycles.

```
#include <stdio.h>

int main( void ) {
    char * argv[2] = {".", NULL};

    // fork spins the child process and both child and the parent
    // continues to co-exist from this point with the same
    // program space.
    int pid = fork();

    if ( pid == 0 ) {
        // The entire child program space is replace by the
        // execvp function call.
        // The child continues to execute the `ls` command.
        execvp("ls", argv);
    }

    // Child process will never reach here.
    // hence all memory that was copied from its parent's
    // program space is of no use.

    // The parent will continue its execution and print the
    // following message.
    printf("parent finishes...\n");
    return 0;
}
```

## Updating without locks

Locks are required when we have in-place updates. Multiple writers try to modify the same instance of the resource and hence we need to define a [critical section](https://en.wikipedia.org/wiki/Critical_section) where the updations happen. This critical section is bounded by locks and any writer who wishes to modify would have to acquire the lock. This streamlines the writers and ensures only one writer could enter the critical section at any point in time, creating a chokepoint.

If we follow CoW aggressively, which suggests we copy before we write, there will be no in-place updates. All variables during every single write will create a clone, apply updates to it and then in one atomic [compare-and-swap](https://en.wikipedia.org/wiki/Compare-and-swap) operation switch and start pointing to this newer version; thus eradicating the need for locking entirely. Garbage collection on unused items, with old values, could happen from time to time.

![Updating variables without locks](https://user-images.githubusercontent.com/4745789/80912595-9fc13080-8d5b-11ea-9b73-599b673e6715.png)

## Versioning and point in time snapshots

If we aggressively follow CoW then on every write we create a clone of the original resource and apply updates to it. If we do not garbage collect the older unused instances, what we get is the history of the resource that shows us how it has been changing with time (every write operation).

Each update creates a new version of the resource and thus we get resource versioning; enabling us to take point-in-time snapshots. This particular behavior is used by all collaborative document tools, like [Google Docs](https://en.wikipedia.org/wiki/Google_Docs), to provide document versioning. Point-in-time snapshots are also used in the databases to take timely backups allowing us to have a rollback and recovery plan in case of some data loss or worse a database failure.

# Implementing CoW

CoW is just a technique and it tells us what and not how. The implementation is all in the hands of the system and depending on the type of resource being CoW’ed the implementation details differ.

The naive way to perform copy operation is by doing a deep copy which, as established before, is a super inefficient way. We can do a lot better than this by understanding the nuances of the underlying resource. To gain a deeper understanding we see how efficiently we could make CoW Binary Tree [Binary Tree](https://en.wikipedia.org/wiki/Binary_tree).

## Efficient Copy-on-write on a Binary Tree

Given a Binary Tree `A` we create a copy `B` such that any modifications by `A` are not visible to `B` and any modifications on `B` are not visible to `A`. The simplest way to achieve this is by cloning all the nodes of the tree, their pointer references, and create a second tree which is then pointed by `B` - as illustrated in the diagram below. Any modifications made to either tree will not be visible to the other because their entire space is mutually exclusive.

![Deep Copying a Binary Tree](https://user-images.githubusercontent.com/4745789/80859895-b3986400-8c81-11ea-9ebe-829540df77d5.png)

Copy-on-Write semantics suggest an optimistic approach where `B` instead of pointing to the cloned `A`, shares the same reference as `A` which means it also points to the exact same tree as `A`. Now say, we modify the node `2` in tree `B` and change its value to `9`.

Observing closely we find that a lot of pointers could be reused and hence a better approach would be to only copy the path from the updating node till the root, keeping all other pointers references same, and let `B` point to this new root, as shown in the illustration.

![Copy-on-Write a Binary Tree](https://user-images.githubusercontent.com/4745789/80869877-7606fb80-8cc0-11ea-8a9b-2b7312a59f11.png)

Thus instead of maintaining two separate mutually exclusive trees, we make space partially exclusive depending on which node is updated and in the process make things efficient with respect to memory and time. This behavior is core to a family of data structures called [Persistent Data Structures](https://en.wikipedia.org/wiki/Persistent_data_structure).

> Fun fact: You can model Time Travel using Copy-on-Write semantics.

# Why shouldn’t we Copy-on-Write

CoW is an expensive process if done aggressively. If on every single write, we create a copy then in a system that is write-heavy, things could go out of hand very soon. A lot of CPU cycles will be occupied for doing garbage collections and thus stalling the core processes. Picking which battles to win is important while choosing something as critical as Copy-on-Write.

# References

- [Copy on Write](https://en.wikipedia.org/wiki/Copy-on-write)
- [Persistent Data Structures](https://en.wikipedia.org/wiki/Persistent_data_structure)
- [Fork Exec Pattern](https://en.wikipedia.org/wiki/Fork%E2%80%93exec)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/midpoint-insertion-caching-strategy

A midpoint insertion caching strategy to avoid cache bust

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Algorithms](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Algorithms](/knowledge-base/database-engineering)

# A midpoint insertion caching strategy to avoid cache bust

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Disk reads are 4x (for SSD) to 80x (for magnetic disk) [slower](https://gist.github.com/hellerbarde/2843375) as compared to main memory (RAM) reads and hence it becomes extremely important for a database to utilize main memory as much as it can, and be super-performant while keeping its latencies to a bare minimum. Engines cannot simply replace disks with RAM because of volatility and cost, hence it needs to strike a balance between the two - maximize main-memory utilization and minimize the disk access.

The database engine virtually splits the data files into pages. A page is a unit which represents how much data the engine transfers at any one time between the disk (the data files) and the main memory. It is usually a few kilobytes 4KB, 8KB, 16KB, 32KB, etc. and is configurable via engine parameters. Because of its bulky size, a page can hold one or multiple rows of a table depending on how much data is in each row i.e. the length of the row.

# Locality of reference

Database systems exhibit a strong and predictable behaviour called [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference) which suggests the access pattern of a page and its neighbours.

## Spatial Locality of Reference

The spatial locality of reference suggests if a row is accessed, there is a high probability that the neighbouring rows will be accessed in the near future.

Having a larger page size addresses this situation to some extent. As one page could fit multiple rows, this means when that page is cached in main memory, the engine saves a disk read if the neighbouring rows residing on the same page are accessed.

Another way to address this situation is to [read-ahead](https://dev.mysql.com/doc/refman/8.0/en/innodb-disk-io.html) pages that are very likely to be accessed in the future and keep them available in the main memory. This way if the read-ahead pages are referenced, the engine needs to go to the disk to fetch the page, rather it will find the page residing in the main memory and thus saving a bunch of disk reads.

## Temporal Locality of Reference

The temporal locality of reference suggests that if a page is recently accessed, it is very likely that the same page will be accessed again in the near future.

Caching exploits this behaviour by putting every single page accessed from the disk into main-memory (cache). Hence the next time a page which is available in the cache is referenced, the engine need not make a disk read to get the page, rather it could reference it from the cache directly, again saving a disk read.

![Disk cache-control flow](https://user-images.githubusercontent.com/4745789/80286313-4e57e680-8748-11ea-88c2-dcb67f6ac566.png)

Since the cache is very costly, it is in magnitude smaller in capacity than the disk. It can only hold some fixed number of pages which means the cache suffers from the problem of getting full very quickly. Once the cache gets full, the engine needs to evict an old page so that the new page, which according to the temporal locality of reference is going to be accessed in the near future, could get a place in the cache.

The most common strategy that decides the page that will be evicted from the cache is the [Least Recently Used cache eviction strategy](<https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)>). This strategy uses Temporal Locality of Reference to the core and hence evicts the page which was not accessed the longest, thus maximizing the time the most-recently accessed pages are held in the cache.

# LRU Cache

The LRU cache holds the items in the order of its last access, allowing us to identify which item is not being used the longest. When the cache is full and a newer item needs to make an entry in the cache, the item which is not accessed the longest is evicted and hence the name Least Recently Used.

The one end (head) of the list holds the most-recently referenced page while the fag end (tail) of the list holds the least-recently referenced one. A new page, being most-recently accessed, is always added at the head of the list while the eviction happens at the tail. If a page from the cache is referenced again, it is moved to the head of the list as it is now the most-recently referenced.

## Implementation

LRU cache is often implemented by pairing a [doubly-linked list](https://en.wikipedia.org/wiki/Doubly_linked_list) with a [hash map](https://en.wikipedia.org/wiki/Hash_table). The cache is thus just a linked list of pages and the hashmap maps the `page_id` to the node in the linked list, enabling `O(1)` lookups.

![LRU Cache](https://user-images.githubusercontent.com/4745789/80288324-d7751a80-8754-11ea-96ab-6a8e25730bff.png)

## InnoDB’s Buffer Pool

MySQL InnoDB’s cache is called [Buffer Pool](https://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html) which does exactly what has been established earlier. Pseudocode implementation of `get_page` function, using which the engine gets the page for further processing, could be summarized as

```
def get_page(page_id:int) -> Page:
    # Check if the page is available in the cache
    page = cache.get_page(page_id)

    # if the page is retrieved from the main memory
    # return the page.
    if page:
        return page

    # retrieve the page from the disk
    page = disk.get_page(page_id)

    # put the page in the cache,
    # if the cache is full, evict a page which is
    # least recently used.
    if cache.is_full():
        cache.evict_page()

    # put the page in the cache
    cache.put_page(page)

    # return the pages
    return page
```

## A notorious problem with Sequential Scans

Above caching strategy works wonders and helps the engine to be super-performant. [Cache hit ratio](https://www.stix.id.au/wiki/Cache_Hit_Ratio) is usually more than 80% for mid-sized production-level traffic, which means 80% of the times the pages were served from the main memory (cache) and the engine did not require to make the disk read.

What would happen if an entire table is scanned? say, while talking a [DB dump](<(https://dev.mysql.com/doc/refman/8.0/en/mysqldump.html)>), or running a `SELECT` without `WHERE` to perform some statistical computations.

Going by the MySQL’s aforementioned behaviour, the engine iterates on all the pages and since each page which is accessed now is the most recent one, it puts it at the head of the cache while evicting one from the tail.

If the table is bigger than the cache, this process will wipe out the entire cache and fill it with the pages from just one table. If these pages are not referenced again, this is a total loss and performance of the database takes a hit. The performance will pickup once these pages are evicted from the cache and other pages make an entry.

# Midpoint Insertion Strategy

MySQL InnoDB Engine ploys an extremely smart solution to solve the notorious problem with Sequential Scans. Instead of keeping its Buffer Pool a strict LRU, it tweaks it a little bit.

Instead of treating the Buffer Pool as a single doubly-linked list, it treats it as a combination of two smaller sublists - usually 5/8th and 3/8th of the total size. One sublist holds the younger data while the other one holds the older data. The head of the Young sublist holds the most recent pages and the recency decreases as we reach the tail of the Old sublist.

![MySQL InnoDB Midpoint Insertion Strategy](https://user-images.githubusercontent.com/4745789/80299447-138a9880-87b2-11ea-9b0a-888e0ccf4b49.png)

## Eviction

The tail of the Old Sublist holds the Least Recently Used page and the eviction thus happens as per the LRU Strategy i.e. at the tail of the Old Sublist.

## Insertion

This is where this strategy differs from Strict LRU. The insertion, instead of happening at “newest” end of the list i.e. head of Young sublist, happens at the head of Old sublist i.e. in the “middle” of the list. This position of the list where the tail of the Young sublist meets the head of the Old sublist is referred to as the “midpoint”, and hence the name of the strategy is Midpoint Insertion Strategy.

> By inserting in the middle, the pages that are only read once, such as during a full table scan, can be aged out of the Buffer Pool sooner than with a strict LRU algorithm.

## Moving page from Old to the Young sublist

In this strategy, like in Strict LRU implementation, whenever the page is accessed it moves to the newest end of the list i.e. the head of the Young sublist. During the first access, the pages make an entry in the cache in the “middle” position.

If the page is referenced the second time it is moved to the head of Young sublist and hence stays in the cache for a longer time. If the page, after being inserted in the middle, is never referenced again (during full scans), it is evicted sooner because the Old sublist is usually shorter than the Young sublist.

The Young sublist thus remains unaffected by table scans bringing in new blocks that might or might not be accessed afterwards. The engine thus remains performant as more frequently accessed pages continue to remain in the cache (Young sublist).

## MySQL parameter to tune the midpoint

InnoDB allows us to tune the midpoint of the buffer pool through the parameter `innodb_old_blocks_pct`. This parameter controls the percentage of Old sublist to Buffer Pool. The default value is 37 which corresponds to the ratio 3/8.

In order to get greater insights about Buffer Pool we can invoke the following command as

```
$ SHOW ENGINE INNODB STATUS

----------------------
BUFFER POOL AND MEMORY
----------------------
Total memory allocated 137363456; in additional pool allocated 0
Dictionary memory allocated 159646
Buffer pool size   8191
Free buffers       7741
Database pages     449
Old database pages 0

...

Pages made young 12, not young 0
43.00 youngs/s, 27.00 non-youngs/s

...

Buffer pool hit rate 997 / 1000, young-making rate 0 / 1000 not 0 / 1000
Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s

...
```

The command `SHOW ENGINE INNODB STATUS` outputs a lot of interesting metrics but the most interesting and critical ones, w.r.t Memory and Buffer Pool, are

- number of pages that were made young
- rate of eviction without access
- cache hit ratio
- read ahead rate

# Conclusion

We see how by changing just one aspect of LRU cache, MySQL InnoDB makes itself Scan Resistant. Sequential scanning was a critical issue for the cache but it was addressed in a very elegant way.

# References

- [Latency numbers](https://gist.github.com/hellerbarde/2843375)
- [Locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference)
- [InnoDB: Making Buffer Cache Scan Resistant](https://serge.frezefond.com/2009/12/innodb-making-buffer-cache-scan-resistant/)
- [MySQL Dev - Buffer Pool](https://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html)
- [MySQL Dev - Making the Buffer Pool Scan Resistant](https://dev.mysql.com/doc/refman/8.0/en/innodb-performance-midpoint_insertion.html)
- [MySQL Dev - InnoDB Disk I/O](https://dev.mysql.com/doc/refman/8.0/en/innodb-disk-io.html)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/fsm-python

Building Finite State Machines with Python Coroutines

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

# Building Finite State Machines with Python Coroutines

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Finite State Machine is a mathematical model of computation that models a sequential logic. FSM consists of a finite number of states, transition functions, input alphabets, a start state and end state(s). In the field of computer science, the FSMs are used in designing Compilers, Linguistics Processing, Step workflows, Game Design, Protocols Procedures (like TCP/IP), Event-driven programming, Conversational AI and many more.

To understand what a finite machine is, we take a look at Traffic Signal. Finite State Machine for a Traffic Signal is designed and rendered below. `Green` is the start/initial state, which upon receiving a trigger moves to `Yellow`, which, in turn, upon receiving a trigger, transitions to `Red`. The `Red` then circles back to `Green` and the loop continues.

![traffic signal fsm](https://user-images.githubusercontent.com/4745789/79678813-d572ff00-821c-11ea-8437-b4a3b7fd1a60.png)

An FSM must be in exactly one of the finite states at any given point in time and then in response to an input, it receives, the machine transitions to another state. In the example above, the traffic signal is exactly in one of the 3 states - `Green`, `Yellow` or `Red`. The transition rules are defined for each state which defines what sequential logic will be played out upon input.

Implementing an FSM is crucial to solving some of the most interesting problems in Computer Science and in this article, we dive deep into modeling a Finite State Machine using Python coroutines.

# Python Coroutines

Before diving into the implementation we take a detour and look at what Generators and Coroutines are, how they keep implementation intuitive and fits into the scheme of things.

## Generators

Generators are **resumable functions** that yield values as long as someone, by calling `next` function, keeps asking it. If there are no more values to yield, the generator raises a `StopIteration` exception.

```
def fib():
    a, b = 0, 1
    while True:
        yield a
        a, b = b, a+b
```

The `yield` statement is where the magic happens. Upon reaching the `yield` statement, the generator function execution is paused and the yielded value is returned to the caller and the caller continues its execution. The flow returns back to the generator when the caller function asks from the next value. Once the next value is requested by calling `next` (explicitly or implicitly), the generator function resumes from where it left off i.e. `yield` statement.

```
>>> fgen = fib()
>>> [next(fgen) for _ in range(10)]
[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]
```

Using a Fibonacci generator is memory-efficient as now we need not compute a lot of Fibonacci numbers and hold them in memory, in a list, rather the requesting process could ask for as many values as it needs and the generator would keep on yielding values one by one.

## Coroutines

Coroutines, just like generators, are resumable functions but instead of generating values, they consume values on the fly. The working of it is very similar to the generator and again the `yield` statement is where the magic happens. When a coroutine is paused at the `yield` statement, we could send the value it using `send` function and the value could be used using the assignment operator `=` on `yield` as shown below

```
def grep(substr):
    while True:
        line = yield
        if substr in line:
            print(f"found {substr}")
```

In the example above, we wrote a simple `grep` utility that checks for a substring in a given stream of text. When the coroutine `grep` is paused at the `yield` statement, using the `send` function, we send the text to it, and it will be referenced by the variable `line`. The coroutine then continues its execution to check if `substr` is in `line` or not. Once the flow reaches the `yield` statement again, the coroutine pauses and waits for the caller to `send` it a new value.

Note that, this is not a thread that keeps on running and hogging the CPU. It is just a function whose execution is paused at the `yield` statement waiting for the value; the state is persisted and the control is passed back to the caller. When resumed the coroutine starts from the same state where it left off.

> Before sending the value to a coroutine we need to “prime” it so that the flow reaches the yield statement and the execution is paused while waiting for the value to be sent.

```
>>> g = grep("users/created")
>>> next(g)  # priming the generator
>>>
>>> g.send("users/get api took 1 ms.")
>>> g.send("users/created api took 3 ms.")
found users/created
>>> g.send("users/get api took 1 ms.")
>>> g.send("users/created api took 4 ms.")
found users/created
>>> g.send("users/get api took 1 ms.")
```

In the function invocations above we see how we could keep on sending the text to the coroutine and it continues to spit out if it found the given substring `users/created` in the text. This ability of coroutine to pause the execution and accept input on the fly helps us model FSM in a very intuitive way.

# Building a Finite State Machine

While building FSMs, the most important thing is how we decide to model and implement states and transition functions. States could be modeled as Python Coroutines that run an infinite loop within which they accept the input, decides the transition and updates the current state of the FSM. The transition function could be as simple as a bunch of `if` and `elif` statements and in a more complex system it could be a decision function.

To dive into low-level details, we build an FSM for the regular expression `ab*c`, which means if the given string matches the regex then the machine should end at the end state, only then we say that the string matches the regex.

![fsm for ab*c](https://user-images.githubusercontent.com/4745789/79634655-84fe9180-8189-11ea-9b94-f9ee563394bf.png)

## State

From the FSM above we model the state `q2` as

```
def _create_q2():
    while True:
        # Wait till the input is received.
        # once received store the input in `char`
        char = yield

        # depending on what we received as the input
        # change the current state of the fsm
        if char == 'b':
            # on receiving `b` the state moves to `q2`
            current_state = q2
        elif char == 'c':
            # on receiving `c` the state moves to `q3`
            current_state = q3
        else:
            # on receiving any other input, break the loop
            # so that next time when someone sends any input to
            # the coroutine it raises StopIteration
            break
```

The coroutine runs as an infinite loop in which it waits for the input token at the `yield` statement. Upon receiving the input, say `b` it changes the current state of FSM to `q2` and on receiving `c` changes the state to `q3` and this precisely what we see in the FSM diagram.

## FSM Class

To keep things encapsulated we will define a class for FSM which holds all the states and maintains the current state of the machine. It will also have a method called `send` which reroutes the received input to the current state. The current state upon receiving this input makes a decision and updates the `current_state` of the FSM as shown above.

Depending on the use-case the FSM could also have a function that answers the core problem statement, for example, does the given line matches the regular expression? or is the number divisible by 3?

The FSM class for the regular expression `ab*c` could be modeled as

```
class FSM:
    def __init__(self):
        # initializing states
        self.start = self._create_start()
        self.q1 = self._create_q1()
        self.q2 = self._create_q2()
        self.q3 = self._create_q3()

        # setting current state of the system
        self.current_state = self.start

        # stopped flag to denote that iteration is stopped due to bad
        # input against which transition was not defined.
        self.stopped = False

    def send(self, char):
        """The function sends the curretn input to the current state
        It captures the StopIteration exception and marks the stopped flag.
        """
        try:
            self.current_state.send(char)
        except StopIteration:
            self.stopped = True

    def does_match(self):
        """The function at any point in time returns if till the current input
        the string matches the given regular expression.

        It does so by comparing the current state with the end state `q3`.
        It also checks for `stopped` flag which sees that due to bad input the iteration of FSM had to be stopped.
        """
        if self.stopped:
            return False
        return self.current_state == self.q3

    ...

    @prime
    def _create_q2(self):
        while True:
            # Wait till the input is received.
            # once received store the input in `char`
            char = yield

            # depending on what we received as the input
            # change the current state of the fsm
            if char == 'b':
                # on receiving `b` the state moves to `q2`
                self.current_state = self.q2
            elif char == 'c':
                # on receiving `c` the state moves to `q3`
                self.current_state = self.q3
            else:
                # on receiving any other input, break the loop
                # so that next time when someone sends any input to
                # the coroutine it raises StopIteration
                break
    ...

```

Similar to how we have defined the function `_create_q2` we could define functions for the other three states `start`, `q1` and `q3`. You can find the complete FSM modeled at [arpitbbhayani/fsm/regex-1](https://github.com/arpitbbhayani/fsm/blob/master/regex-1.ipynb)

## Driver function

The motive of this problem statement is to define a function called `grep_regex` which tests a given `text` against the regex `ab*c`. The function will internally create an instance of `FSM` and will pass the stream of characters to it. Once all the characters are exhausted, we invoke `does_match` function on the FSM which suggests if the given `text` matches the regex `ab*c` or not.

```
def grep_regex(text):
    evaluator = FSM()
    for ch in text:
        evaluator.send(ch)
    return evaluator.does_match()

>>> grep_regex("abc")
True

>>> grep_regex("aba")
False
```

> The entire execution is purely running sequential - and that’s because of Coroutines. All states seem to run in parallel but they that are all executing in one thread concurrently. The coroutine of the current state is executing while all others are suspended on their corresponding `yield` statements. When a new input is sent to the coroutine it is unblocked completes its execution, changes the current state of FSM and pauses itself on its `yield` statement again.

# More FSMs

We have seen how intuitive it is to build Regular expression FSMs using Python Coroutines, but if our hypothesis is true things should equally intuitive when we are implementing FSMs for other use cases and here we take a look at two examples and see how a state is implemented in each

## Divisibility by 3

Here we build an FSM that tells if a given stream of digits of a number is divisible by 3 or not. The state machine is as shown below.

![div3](https://user-images.githubusercontent.com/4745789/79641628-564ae000-81b6-11ea-9c84-147cae3a30a6.png)

We can implement the state `q1` as a coroutine as

```
def _create_q1(self):
    while True:
        digit = yield
        if  digit in [0, 3, 6, 9]:
            self.current_state = self.q1
        elif  digit in [1, 4, 7]:
            self.current_state = self.q2
        elif  digit in [2, 5, 8]:
            self.current_state = self.q0
```

We can see the similarity between the coroutine implementation and the transition function for a state. The entire implementation of this FSM can be found at [arpitbbhayani/fsm/divisibility-by-3](https://github.com/arpitbbhayani/fsm/blob/master/divisibility-by-3.ipynb).

## SQL Query Validator

Here we build an FSM for a SQL Query Validator, which for a given a SQL query tells if it is a valid SQL query or not. The FSM for the validator that covers all the SQL queries will be massive, hence we just deal with the subset of it where we support the following SQL queries

```
SELECT * from TABLE_NAME;
SELECT column, [...columns] from TABLE_NAME;
```

![fsm for sql query validator](https://user-images.githubusercontent.com/4745789/79635523-1c1a1800-818f-11ea-8afe-fe8065b55791.png)

We can implement the state `explicit_cols` as a coroutine as

```
def _create_explicit_cols(self):
    while True:
        token = yield
        if token == 'from':
            self.current_state = self.from_clause
        elif token == ',':
            self.current_state = self.more_cols
        else:
            break
```

Again the coroutine through which the state is implemented is very similar to the transition function of the state keeping things intuitive. The entire implementation of this FSM can be found at [arpitbbhayani/fsm/sql-query-validator](https://github.com/arpitbbhayani/fsm/blob/master/sql-query-validator.ipynb).

# Conclusion

Even though this may not be the most efficient way to implement and build FSM but it is the most intuitive way indeed. The edges and state transitions, translate well into `if` and `elif` statements or the decision functions, while each state is being modeled as an independent coroutine and we still do things in a sequential manner. The entire execution is like a relay race where the baton of execution is being passed from one coroutine to another.

# References and Readings

- [Finite State Machines - Wikipedia](https://en.wikipedia.org/wiki/Finite-state_machine)
- [Finite State Machines - Brilliant.org](https://brilliant.org/wiki/finite-state-machines/)
- [FSM Applications](https://web.cs.ucdavis.edu/~rogaway/classes/120/spring13/eric-applications.pdf)
- [What Are Python Coroutines?](https://realpython.com/lessons/what-are-python-coroutines/)
- [How to Use Generators and yield in Python](https://realpython.com/introduction-to-python-generators/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/bayesian-average

Bayesian average to compute average rating, properly

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

# Bayesian average to compute average rating, properly

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Almost every single website, app or platform on the internet has some sort of rating system in place. Whenever you purchase a product or use a service, you are asked to rate it on a scale, say 1 to 5. The platform then uses this data to generate a score and build a ranking system around it. The score is the measure of quality for each product or service. By surfacing the most quality content on top of the list, the platform tries to up their sales and ensure better engagement with their users.

Coming up with an aggregated score is not an easy thing - we need to crunch a million ratings and then see that the score is, in fact, the true measure of quality. If it isn’t then it would directly affect the business. Today we discuss how we should define this score in a rating based system; spoiler alert! the measure is called [Bayesian Average](https://en.wikipedia.org/wiki/Bayesian_average).

To keep things simple we define the problem statement as

> Given the ratings, on a scale of 1 to 5, that users give to a movie, we generate a score that is a measure of how good a movie is which then helps us get the top 10 movies of all time.

We will use the [MovieLens Dataset](https://grouplens.org/datasets/movielens/) to explore various scoring functions in this article. In the dataset, we get user ratings for each movie and the ratings are made on a scale of 1 to 5.

# Generating the score

The score we generate for each item should be proportional to the quality quotient which means higher the score, superior is the item. Hence we say that the score of an item is the function of all the `m` ratings that it received.

![score function](https://user-images.githubusercontent.com/4745789/79067003-cf8b9400-7cd9-11ea-9b16-c1875933725a.png)

## Arithmetic Mean

The simplest and the most common strategy to compute this aggregated score for an item is by taking an [Arithmetic Mean (average)](https://en.wikipedia.org/wiki/Arithmetic_mean) of all the ratings it received. Hence for each item we sum all the ratings that it received and divide it by its cardinality, giving us the average value.

![arithmetic mean](https://user-images.githubusercontent.com/4745789/79049349-b387e400-7c40-11ea-9adf-b40aa377778f.png)

### Issues with arithmetic mean

The arithmetic mean falls apart pretty quickly. Let’s say there is an item with just one rating of 5 on 5, the item would soar high on the leaderboard ranking. But does it deserve that place? probably not. Because of low cardinality (number of ratings), the score (and hence the rank) of the item will fluctuate more and will not give a true measure of quality.

With the movie dataset, we are analyzing here are the top 10 movies ranked using Arithmetic Mean.

![top 10 movies arithmetic mean](https://user-images.githubusercontent.com/4745789/79049814-58a3bc00-7c43-11ea-980e-a12ae10379f7.png)

Through this measure, all of the top 10 movies have a score of 5 (out of 5) and all of them have just 1 rating. Are these really the top 10 movies of all time? Probably not. Looks like we need to do a lot better than the Arithmetic Mean.

## Cumulative Rating

To remedy the issue with Arithmetic Mean, we come up with an approach of using Cumulative Rating as the scoring function hence instead of taking the average we only consider the sum of all the ratings as the final score.

![cumulative rating as scoring function](https://user-images.githubusercontent.com/4745789/79050470-e1245b80-7c47-11ea-824b-ecd5cbb40912.png)

Cumulative Rating actually does a pretty decent job, it makes popular items with a large number of ratings bubble up to the top of the leaderboard. When we rank the movies in our dataset using Cumulative Ratings we get the following as the top 10.

![top 10 movies through cunulative rating](https://user-images.githubusercontent.com/4745789/79050520-2d6f9b80-7c48-11ea-8e48-1c12fbbc0a88.png)

The top 10 movies now feature Shawshank Redemption, Forrest Gump, Pulp Fiction, etc. which are in fact considered as the top movies of all times. But is Cumulative Rating fool-proof?

### Issues with cumulative rating

Cumulative Rating favors high cardinality. Let’s say there is an extremely poor yet popular item `A` that got 10000 ratings of 1 on 5, and there is another item `B` which is very good but it got 1000 rating of 5 on 5 Cumulative Rating thus gives a score of 10000 \_ 1 = 10000 to item `A` and 1000 \_ 5 = 5000 to item `B`, but `B` clearly is far superior of an item than `A`.

Another issue with Cumulative Rating is the fact that it generates an unbounded score. Ideally, any ranking system expects a normalized bounded score so that the system becomes predictable and consistent.

We established that Cumulative Rating is better than Arithmetic Mean but it is not fool-proof and that’s where the Bayesian Average comes to the rescue.

# The Bayesian Average

Bayesian Average computes the mean of a population by not only using the data residing in the population but also considering some outside information, like a pre-existing belief - a derived property from the dataset, for example, prior mean.

## The intuition

The major problem with Arithmetic Mean as the scoring function was how unreliable it was when we had a low number of data points (cardinality) to compute the score. Bayesian Average plays a part here by introducing pre-belief into the scheme of things.

We start by defining the requirements of our scoring function

- for an item with a fewer than average number of ratings - the score should be around the system’s arithmetic mean
- for an item with a substantial number of ratings - the score should be the item’s arithmetic mean
- as the number of ratings that an item receives increases, the score should gradually move from system’s mean to item’s mean

By ensuring the above we neither prematurely promote nor demote an item in the leaderboard. An item is given a fair number of chances before its score falls to its own Arithmetic mean. This way we use the prior-belief - System’s Arithmetic mean, to make the scoring function more robust and fair to all items.

## The formula

Given the intuition and scoring rules, we come up with the following formula

![bayesian average formula for rating system](https://user-images.githubusercontent.com/4745789/79066315-ab798400-7cd4-11ea-804b-e5e8479824b2.png)

In the above formula, `w` indicates the weight that needs to be given the item’s Arithmetic Mean `A` while `S` represents the System’s Arithmetic Mean. If `A` and `S` are bounded then the final score `s` will also be bounded in the same range, thus solving the problem with Cumulative Rating.

Suppose the number of ratings that an item `i` receives is denoted by `m` and the average number of ratings that any item in the system receives is denoted by `m_avg`, we define the requirements of weight `w` as follows

- `w` is bounded in the range [0, 1]
- `w` should be monotonically increasing
- `w` should be close to 0 when `m` is close to 0
- `w` should reach 0.5 when number `m` reaches `m_avg`
- `w` tries to get closer to 1 as `m` increases

From the above requirements, it is clear that `w` is acting like a knob which decides in what proportions we should consider an item’s mean versus the system’s mean. As `w` increases we tilt more towards item’s mean. We define the `w` as

![weight function for bayesian average](https://user-images.githubusercontent.com/4745789/79066802-4162de00-7cd8-11ea-8068-467ce3305810.png)

When we combine all of the above we get the final scoring function as

![scoring function for bayesian average rating system](https://user-images.githubusercontent.com/4745789/79066769-111b3f80-7cd8-11ea-979e-6437334ccbba.png)

One of the most important properties of Bayesian Average is the fact that the pre-existing belief acts as support which oversees that the score does not fluctuate too abruptly and it smoothens with more number of ratings.

## Applying Bayesian Average to movies dataset

After applying the above mentioned Bayesian Average scoring function to our Movie dataset, we get the following movies as top 10

![top 10 movies by Basysian Average](https://user-images.githubusercontent.com/4745789/79066961-686ddf80-7cd9-11ea-87d7-7e7e582ab9ac.png)

Pretty impressive list! The list contains almost all the famous movies that we all think make the cut. Bayesian average thus provides a bounded score that is a measure of the quality of the item, by using prior-belief i.e. system’s mean.

## Analyzing how Bayesian Average changes the rank

Now that we have seen that the Bayesian Average is, in fact, an excellent way to rank items in a rating system, we find how the rank of an item changes as it receives more ratings. Below we plot the change in the percentile rank of the movies: [Kingsman](https://en.wikipedia.org/wiki/Kingsman:_The_Secret_Service), [Logan](<https://en.wikipedia.org/wiki/Logan_(film)>) and [The Scorpion King](https://en.wikipedia.org/wiki/The_Scorpion_King).

![Kingsman position with ratings](https://user-images.githubusercontent.com/4745789/79068414-53e31480-7ce4-11ea-884a-90e7aee326d8.png)

![Logan rankings](https://user-images.githubusercontent.com/4745789/79068443-7f65ff00-7ce4-11ea-9623-6f03451235de.png)

![Scorpion King](https://user-images.githubusercontent.com/4745789/79068524-35c9e400-7ce5-11ea-8726-d1836a6b9c23.png)

We observe that the fluctuations in percentile rank are more in the case of Arithmetic Mean. Sometimes even after receiving a good number of reviews, the rank fluctuates sharply. In the case of Bayesian Average after an initial set of aberrations, the rank smoothens and converges.

# A note on Bayesian Average

Bayesian Average is not a fixed formula that we have seen above, but it is a concept where we make the scoring function “smoother” by using a pre-existing belief as support. Hence we can tweak the formula as per our needs, or use multiple prior beliefs and still it would classify as a Bayesian Average.

# References

- [Bayesian Average](https://en.wikipedia.org/wiki/Bayesian_average)
- [How not to sort by Average Rating](https://evanmiller.org/how-not-to-sort-by-average-rating.html)
- [How to Rank (Restaurants)](http://www.ebc.cat/2015/01/05/how-to-rank-restaurants/)
- [Of Bayesian average and star ratings](https://fulmicoton.com/posts/bayesian_rating/)
- [Code to compute Bayesian Average](https://github.com/arpitbbhayani/ranking-on-ratings/blob/master/movie-lens.ipynb)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/sliding-window-ratelimiter

Designing and implementing a Sliding Window based Rate Limiter

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [System Design](/knowledge-base/system-design)
- [Algorithms](/knowledge-base/system-design)

- [KB](/knowledge-base)
- [Syst...](/knowledge-base/system-design)
- [Algorithms](/knowledge-base/system-design)

# Designing and implementing a Sliding Window based Rate Limiter

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

A rate limiter restricts the intended or unintended excessive usage of a system by regulating the number of requests made to/from it by discarding the surplus ones. In this article, we dive deep into an intuitive and heuristic approach for rate-limiting that uses a sliding window. The other algorithms and approaches include [Leaky Bucket](https://en.wikipedia.org/wiki/Leaky_bucket), [Token Bucket](https://en.wikipedia.org/wiki/Token_bucket) and Fixed Window.

Rate limiting is usually applied per access token or per user or per region/IP. For a generic rate-limiting system that we intend to design here, this is abstracted by a configuration key `key` on which the capacity (limit) will be configured; the key could hold any of the aforementioned value or its combinations. The limit is defined as the number of requests `number_of_requests` allowed within a time window `time_window_sec` (defined in seconds).

# The algorithm

The algorithm is pretty intuitive and could be summarized as follow

> If the number of requests served on configuration key `key` in the last `time_window_sec` seconds is more than `number_of_requests` configured for it then discard, else the request goes through while we update the counter.

Although the above description of the algorithm looks very close to the core definition of any rate limiter, it becomes important to visualize what is happening here and implement it in an extremely efficient and resourceful manner.

## Visualizing sliding window

Every time we get a request, we make a decision to either serve it or not; hence we check the `number_of_requests` made in last `time_window_sec` seconds. So this process of checking for a fixed window of `time_window_sec` seconds on every request, makes this approach a sliding window where the fixed window of size `time_window_sec` seconds is moving forward with each request. The entire approach could be visualized as follows

![Sliding window visualization](https://user-images.githubusercontent.com/4745789/78364339-eac01a80-75da-11ea-8f65-633fd779afac.png)

## The pseudocode

The core of the algorithm could be summarized in the following Python pseudocode. It is not recommended to put this or similar code in production as it has a lot of limitations (discussed later), but the idea here is to design the rate limiter ground up including low-level data models, schema, data structures, and a rough algorithm.

```
def is_allowed(key:str) -> Bool:
"""The function decides is the current request should be served or not.
It accepts the configuration key `key` and checks the number of requests made against it
as per the configuration.

The function returns True if the request goes through and False otherwise.
"""
    current_time = int(time.time())

    # Fetch the configuration for the given key
    # the configuration holds the number of requests allowed in a time window.
    config = get_ratelimit_config(key)

    # Fetch the current window for the key
    # The window returned, holds the number of requests served since the start_time
    # provided as the argument.
    start_time = current_time - config.time_window_sec
    window = get_current_window(key, start_time)

    if window.number_of_requests > config.capacity:
        return False

    # Since the request goes through, register it.
    register_request(key, current_time)
    return True
```

A naive implementation of the above pseudocode is trivial but the true challenge lies in making the implementation horizontally scalable, with low memory footprint, low CPU utilization, and low time complexity.

# Design

Designing a rate limiter has to be super-efficient because the rate limiter decision engine will be invoked on every single request and if the engine takes a long time to decide this, it will add some overhead in the overall response time of the request. A better design will not only help us keep the response time to a bare minimum, but it also ensures that the system is extensible with respect to future requirement changes.

## Components of the Rate limiter

The Rate limiter has the following components

- Configuration Store - to keep all the rate limit configurations
- Request Store - to keep all the requests made against one configuration key
- Decision Engine - it uses data from the Configuration Store and Request Store and makes the decision

## Deciding the datastores

Picking the right data store for the use case is extremely important. The kind of datastore we choose determines the core performance of a system like this.

### Configuration Store

The primary role of the Configuration Store would be to

- efficiently store configuration for a key
- efficiently retrieve the configuration for a key

In case of machine failure, we would not want to lose the configurations created, hence we choose a disk-backed data store that has an efficient `get` and `put` operation for a key. Since there would be billions of entries in this Configuration Store, using a SQL DB to hold these entries will lead to a performance bottleneck and hence we go with a simple key-value NoSQL database like [MongoDB](https://mongodb.com) or [DynamoDB](https://aws.amazon.com/dynamodb/) for this use case.

### Request Store

Request Store will hold the count of requests served against each key per unit time. The most frequent operations on this store will be

- registering (storing and updating) requests count served against each key - _write heavy_
- summing all the requests served in a given time window - _read and compute heavy_
- cleaning up the obsolete requests count - _write heavy_

Since the operations are both read and write-heavy and will be made very frequently (on every request call), we chose an in-memory store for persisting it. A good choice for such operation will be a datastore like [Redis](https://redis.io) but since we would be diving deep with the core implementation, we would store everything using the common data structures available.

## Data models and data structures

Now we take a look at data models and data structures we would use to build this generic rate limiter.

### Configuration Store

As decided before we would be using a NoSQL key-value store to hold the configuration data. In this store, the key would be the configuration key (discussed above) which would identify the user/IP/token or any combination of it; while the value will be a tuple/JSON document that holds `time_window_sec` and `capacity` (limit).

```
{
  "user:241531": {
    "time_window_sec": 1,
    "capacity": 5
  }
}
```

The above configuration defines that the user with id `241531` would be allowed to make `5` requests in `1` second.

## Request Store

Request Store is a nested dictionary where the outer dictionary maps the configuration key `key` to an inner dictionary, and the inner dictionary maps the epoch second to the request counter. The inner dictionary is actually holding the number of requests served during the corresponding epoch second. This way we keep on aggregating the requests per second and then sum them all during aggregation to compute the number of requests served in the required time window.

![Request Store for sliding window rate limiter](https://user-images.githubusercontent.com/4745789/78384914-b0657600-75f8-11ea-8158-981ac3ecd46d.png)

## Implementation

Now that we have defined and designed the data stores and structures, it is time that we implement all the helper functions we saw in the pseudocode.

### Getting the rate limit configuration

Getting the rate limit configuration is a simple get on the Configuration Store by `key`. Since the information does not change often and making a disk read every time is expensive, we cache the results in memory for faster access.

```
def get_ratelimit_config(key):
    value = cache.get(key)

    if not value:
        value = config_store.get(key)
        cache.put(key, value)

    return value
```

### Getting requests in the current window

Now that we have the configuration for the given key, we first compute the `start_time` from which we want to count the requests that have been served by the system for the `key`. For this, we iterate through the data from the inner dictionary second by second and keep on summing the requests count for the epoch seconds greater than the `start_time`. This way we get the total requests served from start_time till now.

In order to reduce the memory footprint, we could delete the items from the inner dictionary against the time older than the `start_time` because we are sure that the requests for a timestamp older than `start_time` would never come in the future.

```
def get_current_window(key, start_time):
    ts_data = requests_store.get(key)
    if not key:
        return 0

    total_requests = 0
    for ts, count in ts_data.items():
        if ts > start_time:
            total_requests += count
        else:
            del ts_data[ts]

    return total_requests
```

### Registering the request

Once we have validated that the request is good to go through, it is time to register it in the store and the defined function `register_request` does exactly that.

```
def register_request(key, ts):
    store[key][ts] += 1
```

## Potential issues and performance bottlenecks

Although the above code elaborates on the overall low-level implementation details of the algorithm, it is not something that we would want to put in production as there are lots of improvements to be made.

### Atomic updates

While we register a request in the Request Store we increment the request counter by 1. When the code runs in a multi-threaded environment, all the threads executing the function for the same key `key`, all will try to increment the same counter. Thus there will be a classical problem where multiple writers read the same old value and updates. To fix this we need to ensure that the increment is done atomically and to do this we could use one of the following approaches

- optimistic locking (compare and swap)
- pessimistic locks (always taking lock before incrementing)
- utilize atomic hardware instructions (fetch-and-add instruction)

### Accurately computing total requests

Since we are deleting the keys from the inner dictionary that refers to older timestamps (older than the `start_time`), it is possible that a request with older `start_time` is executing while a request with newer `start_time` deleted the entry and lead to incorrect `total_request` calculation. To remedy this we could either

- delete entries from the inner dictionary with a buffer (say older than 10 seconds before the start_time),
- take locks while reading and block the deletions

### Non-static sliding window

There would be cases where the `time_window_sec` is large - an hour or even a day, suppose it is an hour, so if in the Request Store we hold the requests count against the epoch seconds there will be 3600 entries for that key and on every request, we will be iterating over at least 3600 keys and computing the sum. A faster way to do this is, instead of keeping granularity at seconds we could do it at the minute-level and thus we sub-aggregate the requests count at per minute and now we only need to iterate over about 60 entries to get the total number of requests and our window slides not per second but per minute.

The granularity configuration could be persisted in the configuration as a new attribute which would help us take this call.

### Other improvements

The solution described above is not the most optimal solution but it aims to prove a rough idea on how we could implement a sliding window rate limiting algorithm. Apart from the improvements mentioned above there some approaches that would further improve the performance

- use a data structure that is optimized for range sum, like segment tree
- use a running aggregation algorithm that would prevent from recomputing redundant sums

## Scaling the solution

### Scaling the Decision engine

The decision engine is the one making the call to each store to fetch the data and taking the call to either accept or discard the request. Since decision engine is a typical service engine we would put it behind a load balancer that takes care of distributing requests to decision engine instances in a round-robin fashion ensuring it scales horizontally.

The scaling policy of the decision engine will be kept on following metrics

- number of requests received per second
- time taken to make a decision (response time)
- memory consumption
- CPU utilization

### Scaling the Request Store

Since the Request Store is doing all the heavy lifting and storing a lot of data in memory, this would not scale if kept on a single instance. We would need to horizontally scale this system and for that, we shard the store using configuration key key and use consistent hashing to find the machine that holds the data for the key.

To facilitate sharding and making things seamless for the decision engine we will have a Request Store proxy which will act as the entry point to access Request Store data. It will abstract out all the complexities of distributed data, replication, and failures.

### Scaling the Configuration Store

The number of configurations would be high but it would be relatively simple to scale since we are using a NoSQL solution, sharding on configuration key `key` would help us achieve horizontal scalability.

Similar to Request Store proxy we will have a proxy for Configuration Store that would be an abstraction over the distributed Configuration Stores.

## High-level design

The overall high-level design of the entire system looks something like this

![Rate limiter high-level design diagram](https://user-images.githubusercontent.com/4745789/78460031-1cb8a600-76db-11ea-94f4-b821244993b3.png)

## Deploying in production

While deploying it to production we could use a memory store like Redis whose features, like Key expiration, transaction, locks, sorted, would come in handy. The language we chose for explaining and pseudocode was Python but in production to make things super-fast and concurrent we would prefer a language like Java or Golang. Picking this stack will keep our server cost down and would also help us make optimum utilization of the resources.

# References

- [Rate Limiting - Wikipedia](https://en.wikipedia.org/wiki/Rate_limiting)
- [Rate-limiting strategies and techniques](https://cloud.google.com/solutions/rate-limiting-strategies-techniques)
- [An alternative approach to rate limiting](https://www.figma.com/blog/an-alternative-approach-to-rate-limiting/)
- [Building a sliding window rate limiter with Redis](https://engagor.github.io/blog/2017/05/02/sliding-window-rate-limiter-redis/)
- [Everything You Need To Know About API Rate Limiting](https://nordicapis.com/everything-you-need-to-know-about-api-rate-limiting/)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/idf

The math behind Inverse Document Frequency - the IDF of TFIDF

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# The math behind Inverse Document Frequency - the IDF of TFIDF

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is one of the most popular measures that quantify document relevance for a given term. It is extensively used in [Information Retrieval](https://en.wikipedia.org/wiki/Information_retrieval) (ex: Search Engines), Text Mining and even for text-heavy Machine Learning use cases like Document Classification and Clustering. Today we explore the better half of TF-IDF and see its connection with Probability, the role it plays in TF-IDF and even the intuition behind it.

Inverse Document Frequency (IDF) is a measure of term rarity which means it quantifies how rare the term, in the corpus, really is (document collection); higher the IDF, rarer the term. A rare term helps in discriminating, distinguishing and ranking documents and it contributes more information to the corpus than what a more frequent term (like `a`, `and` and `the`) does.

The IDF was heuristically proposed in the paper “[A statistical interpretation of term specificity and its application in retrieval](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.8343&rep=rep1&type=pdf)” (Spärck Jones, 1972) and was originally called Term Specificity.

# The intuition behind IDF

In order to quantify the term rarity, the heuristic says we need to give higher weight to the term that occurs in fewer documents and lesser weights to the frequent ones. Thus this measure (weight) `w` of the term is **inversely proportional** to the number of documents in which it is present (called Document Frequency) - and hence the measure is called Inverse Document Frequency.

![IDF Inversely proportional to Document Frequency](https://user-images.githubusercontent.com/4745789/76211536-85237d00-622c-11ea-82f5-c0b655634839.png)

Any function that adheres to the requirement of being inversely proportional to the document frequency i.e. a decreasing function, would do the job; it may not yield optimality but could be used as an IDF for some use cases. Some decreasing functions that could be used as an IDF for some use cases are shown below

![Decreasing functions](https://user-images.githubusercontent.com/4745789/76213296-63c49000-6230-11ea-9d24-94ce048732bc.png)

The more frequent words, like `a`, `and` and `the` will lie on the far right of the plot and will have a smaller value of IDF.

# The most common IDF

A widely adapted IDF measure that performs better in most use cases is defined below

![common idf function](https://user-images.githubusercontent.com/4745789/76239930-633fef80-6258-11ea-823a-2011c04a1e97.png)

where

- `N` is the number of documents in the corpus
- `df(t)` is the number of documents that has an occurrence of the term `t`

If we plot the above IDF function against the document frequency we get a nice smooth decreasing function as shown below. For lower values of X i.e. Document Frequency we see the IDF is very high as it suggests a good discriminator and as the Document Frequency increases the plot smoothly descends and reaches 0 for `df(t) = N`.

![IDF Graph](https://user-images.githubusercontent.com/4745789/76215908-ae94d680-6235-11ea-8e50-498aae029ea2.png)

# IDF and Probability

What would be the probability that a random document picked from a corpus of `N` documents contains the term `t`? The answer to this question is the fraction of documents, out of N, that contains the term `t` and, as seen above, this is its Document Frequency.

![Probability](https://user-images.githubusercontent.com/4745789/76229411-29ff8380-6248-11ea-9518-6cbc4c6947da.png)

The fraction inside the logarithm in the IDF function is oddly similar to the above probability, in fact, it is the inverse of probability defined above. Hence we could redefine IDF using this probability as

![IDF as probability](https://user-images.githubusercontent.com/4745789/76229704-a09c8100-6248-11ea-9960-0cfd5f45dcce.png)

By defining IDF as a probability, we could now estimate the true IDF of a term by observing a random sample instead and computing IDF on this sampled data.

# IDF of conjunction

Computing IDF for a single term is fine but what happens when we have multiple terms? How would that fare out? This is a very common use case in Information Retrieval where we need to rank documents for a given search query, and the search query more often than not contains multiple terms.

For finding IDF of multiple terms in conjunction we make an assumption - the occurrences of terms are statistically independent and because of this the equation below holds true

![Probability of conjunction](https://user-images.githubusercontent.com/4745789/76239792-2d9b0680-6258-11ea-8da2-56899540cab0.png)

Given this, we could derive the IDF of two terms in conjunction as follows

![IDF derivation](https://user-images.githubusercontent.com/4745789/76232475-c2980280-624c-11ea-8a3a-37d17704a221.png)

From the derivation above we see that the IDF of conjunction is just the summation of IDF of individual terms. Extending this to search engines we could say that the score of a document for a given search query is the summation of scores that document gets for individual terms of the query.

> Note: IDF on conjunction could be made much more complex by not assuming statistical independence.

# Other measures of IDF

The decreasing functions we see in the first section of this article were just some examples of possible IDF functions. But there are IDF functions that are not just examples but are also used in some specific use cases and some of them are:

![Other IDF Measures](https://user-images.githubusercontent.com/4745789/76232678-0db21580-624d-11ea-864c-1094559e0790.png)

Most of the IDF functions only differ in the bounds they produce for a given range of document frequency. The plots of 3 IDF functions namely - Common IDF, Smooth IDF, and Probabilistic IDF, are shown below:

![Plot IDF Functions](https://user-images.githubusercontent.com/4745789/76232756-2de1d480-624d-11ea-81cb-8d29109bd594.png)

By observing the plots of 3 different IDF functions it becomes clear that we should use Probabilistic IDF function when we want to penalize a term that occurs in more than 50% of document by giving it a negative weight; and use a Smooth IDF when we do not want a bounded IDF value and not `undefined` (for `DF(t) = 0`) and `0` (for `DF(t) = N`) as such values ruins a function where IDF is multiplied with some other scalar (like Term Frequency).

Similarly, we could define our own IDF function by deciding when and how the penalty to be applied and defining the parameters accordingly.

# Role of IDF in TF-IDF

TF-IDF suggests how important a word is to a document in a collection (corpus). It helps search engines identify what it is that makes a given document special for a given query. It is defined as the product of Term Frequency (number of occurrences of the term in the document) and Inverse Document Frequency.

For the document to have a high TF-IDF score (high relevance) it needs to have high term frequency and a high inverse document frequency (i.e. low document frequency) of the term. Thus IDF primarily downscales the frequent occurring of common words and boosts the infrequent words with high term frequency.

# References

This article is mostly based on the wonderful paper [Understanding Inverse Document Frequency: On theoretical arguments for IDF](https://pdfs.semanticscholar.org/8397/ab573dd6c97a39ff4feb9c2d9b3c1e16c705.pdf) by Stephen Robertson.

Other references:

- [TF-IDF - Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
- [Inverse Document Frequency and the Importance of Uniqueness](https://moz.com/blog/inverse-document-frequency-and-the-importance-of-uniqueness)

Images used in other measures of IDF are taken from [Wikipedia page of TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/better-programmer

Eight Rituals to be a Better Programmer

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Career Growth](/knowledge-base/career-growth)
- [Becoming Better](/knowledge-base/career-growth)

- [KB](/knowledge-base)
- [Care...](/knowledge-base/career-growth)
- [Becoming Better](/knowledge-base/career-growth)

# Eight Rituals to be a Better Programmer

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

“How to get better at programming?” is the question I had been asked quite a few times, and today I lay down the 8 rituals I have been following, and action items for each, to be good and get better at programming.

# Code exhaustively

Doing something repeatedly always helps and writing a lot of code will develop our ability to

- write code while we think
- think faster, think better
- foresee requirement changes and possible logic extensions

### Action Items

- One significant contribution to a project every two weeks
- Solve at least two programming questions (from [Codechef](https://www.codechef.com/), [Spoj](https://www.spoj.com/) or [HackerRank](https://www.hackerrank.com/)) every week, till we solve at least 300 questions

# Code consistently

If we don’t do something repeatedly, it becomes extremely hard to get good at it. Writing code consistently helps us

- define the programmatic and algorithmic flow quickly
- build a habit of programming and thinking analytically

### Action Items

- make one small contribution to anyone project every three days

# Once a while build a complex system

Solving programming questions is about developing logic but things become a little trickier when we build a complex system, as it requires us to take our programming skills to go up a notch. Some examples of complex systems are - a Library management system, a [Twitter](https://twitter.com) clone, an [Instagram](https://www.instagram.com/) clone, etc. Building a complex system

- widens our tech stack
- makes us keep our code flexible, extensible and reusable
- helps us understand how to split our code into independent segments that work in harmony

### Action Items

- build one complex system every 4 months

# Once a while build something inspired by the real world

After we spend some time writing programs and solving problems, things become monotonous and do not seem to challenge us anymore, so to spice things up a bit we should model something from the real world, like

- [projectile motion](https://en.wikipedia.org/wiki/Projectile_motion)
- [double pendulum](https://en.wikipedia.org/wiki/Double_pendulum)
- [solar system simulation](https://en.wikipedia.org/wiki/Numerical_model_of_the_Solar_System)

There are lots of libraries and framework like [p5.js](https://p5js.org) that makes visual programming simple.

### Action Items

- once every 6 months model a physical phenomenon

# Read super exhaustively

It is not only writing code that improves our programming skills but it is reading some quality code written by expert programmers that make the difference. Reading code written by experts improve our programming vocabulary and by doing this we

- learn the best programming practices
- discover the new programming paradigms
- find ways to properly structure our code for extensibility

The best way to start doing it is by picking up an open-source project and start skimming the code. It is okay to not understand it in the first go but it is important to skim it a few times and get acquainted. After a few skim, everything will fall in place, the code becomes familiar and we start to understand the flow and business logic.

### Action Items

- pick an open-source project every 6 months and skim its code once every two months
- pick a tiny open-source utility, from an experienced developer, every month and skim it

# Collaborate with a stranger

There is always someone sitting on the other side of the globe, who knows a thing or two more than us. Look for them and collaborate on a project. The developer community is filled with super smart and super enthusiastic developers who love to share and collaborate. Use websites like [Dev.to](https://dev.to/), [Hashnode](https://hashnode.com/) and [Twitter](https://twitter.com/) to find and interact with like-minded people.

### Action Items

- collaborate on a project once a year
- be active on platforms like [Dev.to](https://dev.to/), [Hashnode](https://hashnode.com/) and [Twitter](https://twitter.com/)

# Fundamentals go a long way

A programming language is just a tool to express business logic. While learning a programming language we should try to understand the constructs and paradigms used - for example: [Functional programming](https://en.wikipedia.org/wiki/Functional_programming), [Polymorphism](<https://en.wikipedia.org/wiki/Polymorphism_(computer_science)>), [Event driven programming](https://en.wikipedia.org/wiki/Event-driven_programming), [Actor model](https://en.wikipedia.org/wiki/Actor_model), etc. It is important to do so because we could pick constructs from one language and use it in another to solve our problem. For example: picking Functional programming (Callbacks) from Javascript and using it in Python to create generic action functions.

### Action Items

- learn one design pattern every month and build a simulation around it
- pick a language construct and implement it in some other language

# We think before we code

Writing code before putting in some thought is degraded the code more often than not. The code written like this lacks simplicity, reusability, and extensibility. Spending some time thinking about problem statement or task at hand and having a rough execution plan always helps.

### Action Items

- always define the scope of implementation, create an execution plan and then code

# Conclusion

These rituals have helped me get better at programming with time and in parallel, I pick at max 3 and act on the action items. Programming is simple but being better than most is difficult. Doing it consistently makes one get better by the day.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/python-prompts

Personalize your Python Prompt

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

# Personalize your Python Prompt

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

The `>>>` we see when the Python interactive shell starts, is called the Prompt String. Usually, the prompt string suggests that the interactive shell is now ready to take new commands.

```
Python 2.7.10 (default, Feb 22 2019, 21:55:15)
[GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.37.14)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>>
```

Python has 2 prompt strings, one primary `>>>` and one secondary `...` which we usually see when an execution unit (statement) spans multiline, for example: while defining a function

```
>>> def foo(a, b):
...     return a + b
...
>>>
```

# Personalizing the prompt strings

The prompt strings are defined in the [sys](https://docs.python.org/3/library/sys.html) module as [ps1](https://docs.python.org/3/library/sys.html#sys.ps1) and [ps2](https://docs.python.org/3/library/sys.html#sys.ps2) and just like any other attribute we can change the values of `sys.ps1` and `sys.ps2` and the changes take effect immediately and as a result, the prompt we see in the shell changes to the new value.

```
>>> import sys
>>> sys.ps1 = '::: '
:::
```

From the example above we see that changing the value of `sys.ps1` to `:::` changes the prompt to `:::` .

As the interactive shell runs in a terminal, we can color and format it using [bash color format](https://misc.flogisoft.com/bash/tip_colors_and_formatting) as shown below

```
import sys
sys.ps1 = "\033[1;33m>>>\033[0m "
sys.ps2 = "\033[1;34m...\033[0m "
```

The code snippet above makes our primary prompt string yellow and secondary prompt string blue. Here’s how it looks

![Python colored prompt](https://user-images.githubusercontent.com/4745789/74897098-03be9480-53bc-11ea-8395-7b3bbb1814dd.png)

## Dynamic prompt strings

The [documentation](https://docs.python.org/3/library/sys.html#sys.ps2) states that if we assign a non-string object to `ps1` or `ps2` then Python prompts by calling `str()` on the object every time a prompt is shown. Now we create some stateful and dynamic prompt by defining a class and overriding the `__str__` method.

Below we implement [IPython](https://ipython.org/) like prompt where execution statement number is stored in member `line` of the class and is incremented every time the primary prompt renders.

```
# -*- coding: utf-8 -*-
import sys

class IPythonPromptPS1(object):
  def __init__(self):
    self.line = 0

  def __str__(self):
    self.line += 1
    return "\033[92mIn [%d]:\033[0m " % (self.line)

sys.ps1 = IPythonPromptPS1()
sys.ps2 = "    \033[91m...\033[0m "
```

The above code snippet makes prompt look like this

![ipython prompt](https://user-images.githubusercontent.com/4745789/74897125-18029180-53bc-11ea-86e6-9d0ca6753fb9.png)

# Setting new prompt strings every time the shell starts

We would not want to run this code snippet every time we start the shell and hence we use an environment variable [PYTHONSTARTUP](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONSTARTUP) which holds the path of a readable file and is executed before the first prompt is displayed in interactive mode.

So we dump the code snippet in a file, say `ipython.py` and export `PYTHONSTARTUP` as

```
export PYTHONSTARTUP="$HOME/ipython.py"
```

Now every time, we start our Python interactive shell, it will execute the file `ipython.py` and set the required prompt strings.

# Conclusion

Combining everything mentioned above I have created a utility called [py-prompts](https://github.com/arpitbbhayani/py-prompts). Here is a glimpse of the themes that the package holds.

![Pretty Python Prompts GIF](https://user-images.githubusercontent.com/4745789/74897216-539d5b80-53bc-11ea-8cdd-91177b6553b5.gif)

I hope you found this piece interesting. Python being an exhaustively extensible language made it super-easy for us to change the prompt strings and be creative with it. If you have a theme idea or have already personalized your prompt, share it with me [@arpit_bhayani](https://twitter.com/arpit_bhayani), I will be thrilled to learn more about it.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/rule-30-cellular-automata

Pseudorandom Number Generation using Cellular Automata - Rule 30

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Pseudorandom Number Generation using Cellular Automata - Rule 30

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

A pseudorandom number generator produces numbers deterministically but they seem aperiodic (random) most of the time for most use-cases. The generator accepts a seed value (ideally a true random number) and starts producing the sequence as a function of this seed and/or a previous number of the sequence. These are Pseudorandom (not truly random) because if seed value is known they can be determined algorithmically. True random numbers are hardware generated or generated from blood volume pulse, atmospheric pressure, thermal noise, quantum phenomenon, etc.

There are lots of [techniques](<https://en.wikipedia.org/wiki/List_of_random_number_generators#Pseudorandom_number_generators_(PRNGs)>) to generate Pseudorandom numbers, namely: [Blum Blum Shub algorithm](https://en.wikipedia.org/wiki/Blum_Blum_Shub), [Middle-square method](https://en.wikipedia.org/wiki/Middle-square_method), [Lagged Fibonacci generator](https://en.wikipedia.org/wiki/Lagged_Fibonacci_generator), etc. Today we dive deep into [Rule 30](https://en.wikipedia.org/wiki/Rule_30) that uses a controversial science called [Cellular Automaton](https://en.wikipedia.org/wiki/Cellular_automaton). This method passes many standard tests for randomness and was used in [Mathematica](https://www.wolfram.com/mathematica/online/) for generating random integers.

# Cellular Automaton

Before we dive into Rule 30, we will spend some time understanding [Cellular Automaton](https://en.wikipedia.org/wiki/Cellular_automaton). A Cellular Automaton is a discrete model consisting of a regular grid, of any dimension, with each cell of the grid having a finite number of states and a neighborhood definition. There are rules that determine how these cells interact and transition into the next generation (state). The rules are mostly mathematical/programmable functions that depend on the current state of the cell and its neighborhood.

![Cellular Automata](https://user-images.githubusercontent.com/4745789/74360178-9bcfe300-4dea-11ea-8c87-91005e89c881.png)

In the above Cellular Automaton, each cell has 2 finite states `0` (shown in red), `1` (shown in black). Each cell transitions into the next generation by XORing the state values of its 8 neighbors. The first generation (initial state) of the grid is allocated at random and the state transitions, of the entire grid, is as below

![Cellular Automata Demo](https://media.giphy.com/media/J27aUn6QIWZFnVWzEB/giphy.gif)

Cellular Automata was originally conceptualized in the 1940s by [Stanislaw Ulam](https://en.wikipedia.org/wiki/Stanislaw_Ulam) and [John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann); it finds its application in computer science, mathematics, physics, complexity science, theoretical biology and microstructure modeling. In the 1980s, [Stephen Wolfram](https://en.wikipedia.org/wiki/Stephen_Wolfram) did a systematic study of one-dimensional cellular automata (also called elementary cellular automata) on which Rule 30 is based.

# Rule 30

Rule 30 is an elementary (one-dimensional) cellular automaton where each cell has two possible states `0` (shown in red) and `1` (shown in black). The neighborhood of a cell is its two immediate neighbors, one on its left and other on right. The next state (generation) of the cell depends on its current state and the state of its neighbors; the transition rules are as illustrated below

![Rule 30](https://user-images.githubusercontent.com/4745789/74396927-78805480-4e39-11ea-8349-b6774d05a600.png)

The above transition rules could be simplified as `left XOR (central OR right)`.

We visualize Rule 30 in a 2-dimensional grid where each row represents one generation (state). The next generation (state) of the cells is computed and populated in the row below. Each row contains a finite number of cells which “wraps around” at the end.

![Rule 30 in action](https://media.giphy.com/media/d9YuURGwsOD8qVt8uE/giphy.gif)

The above pattern emerges from an initial state (row 0) in a single cell with state 1 (shown as black) surrounded by cells with state 0 (red). The next generation (as seen in row 1) is computed using the rule chart mentioned above. The vertical axis represents time and any horizontal cross-section of the image represents the state of all the cells in the array at a specific point in the pattern’s evolution.

![Chaos in Rule 30](https://user-images.githubusercontent.com/4745789/74433188-f1a59900-4e85-11ea-970d-c60af22568ea.png)

As the pattern evolves, frequent red triangles of varying sizes pop up but the structure as a whole has no recognizable pattern. The above snapshot of the grid was taken at a random point of time and we could observe chaos and aperiodicity. This property is exploited to generate pseudorandom numbers.

## Pseudorandom Number Generation

As established earlier, Rule 30 is exhibits aperiodic and chaotic behavior and hence it produces complex, seemingly random patterns from simple, well-defined rules. To generate random numbers from using Rule 30 we use the center column and pick a batch of `n` random bits and form the required `n` bit random number from it. The next random number is built using the next `n` bits from the column.

![Pseudorandom Number Rule 30](https://user-images.githubusercontent.com/4745789/74435575-c2455b00-4e8a-11ea-835b-ca5f722dae9e.png)

If we always start from the first row, the sequence of the numbers we generate will always be predictable - which is not what we want. To make things pseudorandom, we take a random seed value (ex: current timestamp) and skip that number of bits and then pick batches of `n` and build random numbers.

> The pseudorandom numbers generated using Rule 30 are not cryptographically secure but are suitable for simulation as long as we do not use bad seed like `0`.

One major advantage of using Rule 30 to generate pseudorandom numbers is that we could generate multiple random numbers in parallel by picking multiple columns to batch `n` bits each at random. A sample 8-bit random integer sequence generated using this method with seed `0` is `220`, `197`, `147`, `174`, `117`, `97`, `149`, `171`, `240`, `241`, etc.

The seed value could also be used as the initial state (row 0) for Rule 30 and random numbers are then simply the `n` bits batches picked from the center column starting from row 0. This approach is more efficient but is heavily dependent on the quality of seed value, as a bad seed value could make things extremely predictable. A demonstration of this approach could be found on [Wolfram Cloud Demonstration Page](https://demonstrations.wolfram.com/UsingRule30ToGeneratePseudorandomRealNumbers/).

## Rule 30 in the real world

Rule 30 is also seen in nature, on the shell of code snail species [Conus textile](https://en.wikipedia.org/wiki/Conus_textile). The [Cambridge North railway station](https://en.wikipedia.org/wiki/Cambridge_North_railway_station#Facilities) is decorated with architectural panels displaying the evolution of Rule 30.

# Conclusion

If you found Rule 30 interesting I urge you to write your own simulation of using [p5 library](https://p5js.org/); you could keep it generic enough to so that the program could generate patterns for different rules like 90, 110, 117, etc. The patterns generated using these rules are quite interesting. If you want, you could things to the next level and extend rule to work in 3 dimensions and see how patterns evolve. I believe programming is fun when it is visual.

It is exciting when two seemingly unrelated fields, Cellular Automata and Cryptography, come together and create something wonderful. Although this algorithm is not widely used anymore, because of more efficient algorithms, it urges us to be creative in using Cellular Automata in more ways than one. This article is first in the series of Cellular Automata, so stay tuned and watch this space for more.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/function-overloading

Implementing functional overloading in Python

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Prototypes](/knowledge-base/algorithms-and-explorations)

# Implementing functional overloading in Python

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Function overloading is the ability to have multiple functions with the same name but with different signatures/implementations. When an overloaded function `fn` is called, the runtime first evaluates the arguments/parameters passed to the function call and judging by this invokes the corresponding implementation.

```
int area(int length, int breadth) {
  return length * breadth;
}

float area(int radius) {
  return 3.14 * radius * radius;
}
```

In the above example (written in C++), the function `area` is overloaded with two implementations; one accepts two arguments (both integers) representing the length and the breadth of a rectangle and returns the area; while the other function accepts an integer radius of a circle. When we call the function `area` like `area(7)` it invokes the second function while `area(3, 4)` invokes the first.

### Why no Function Overloading in Python?

Python does not support function overloading. When we define multiple functions with the same name, the later one always overrides the prior and thus, in the namespace, there will always be a single entry against each function name. We see what exists in Python namespaces by invoking functions `locals()` and `globals()`, which returns local and global namespace respectively.

```
def area(radius):
  return 3.14 * radius ** 2

>>> locals()
{
  ...
  'area': <function area at 0x10476a440>,
  ...
}
```

Calling the function `locals()` after defining a function we see that it returns a dictionary of all variables defined in the local namespace. The key of the dictionary is the name of the variable and value is the reference/value of that variable. When the runtime encounters another function with the same name it updates the entry in the local namespace and thus removes the possibility of two functions co-existing. Hence python does not support Function overloading. It was the design decision made while creating language but this does not stop us from implementing it, so let’s overload some functions.

# Implementing Function Overloading in Python

We know how Python manages namespaces and if we would want to implement function overloading, we would need to

- manage the function definitions in a maintained virtual namespace
- find a way to invoke the appropriate function as per the arguments passed to it

To keep things simple, we will implement function overloading where the functions with the same name are distinguished by the **number of arguments** it accepts.

## Wrapping the function

We create a class called `Function` that wraps any function and makes it callable through an overridden `__call__` method and also exposes a method called `key` that returns a tuple which makes this function unique in entire codebase.

```
from inspect import getfullargspec

class Function(object):
  """Function is a wrap over standard python function.
  """
  def __init__(self, fn):
    self.fn = fn

  def __call__(self, *args, **kwargs):
    """when invoked like a function it internally invokes
    the wrapped function and returns the returned value.
    """
    return self.fn(*args, **kwargs)

  def key(self, args=None):
    """Returns the key that will uniquely identify
    a function (even when it is overloaded).
    """
    # if args not specified, extract the arguments from the
    # function definition
    if args is None:
      args = getfullargspec(self.fn).args

    return tuple([
      self.fn.__module__,
      self.fn.__class__,
      self.fn.__name__,
      len(args or []),
    ])
```

In the snippet above, the `key` function returns a tuple that uniquely identifies the function in the codebase and holds

- the module of the function
- class to which the function belongs
- name of the function
- number of arguments the function accepts

The overridden `__call__` method invokes the wrapped function and returns the computed value (nothing fancy here right now). This makes the instance callable just like the function and it behaves exactly like the wrapped function.

```
def area(l, b):
  return l * b

>>> func = Function(area)
>>> func.key()
('__main__', <class 'function'>, 'area', 2)
>>> func(3, 4)
12
```

In the example above, the function `area` is wrapped in `Function` instantiated in `func`. The `key()` returns the tuple whose first element is the module name `__main__`, second is the class `<class 'function'>`, the third is the function name `area` while the fourth is the number of arguments that function `area` accepts which is `2`.

The example also shows how we could just call the instance `func`, just like the usual `area` function, with arguments `3` and `4` and get the response `12`, which is exactly what we’d get is we would have called `area(3, 4)`. This behavior would come in handy in the later stage when we play with decorators.

## Building the virtual Namespace

Virtual Namespace, we build here, will store all the functions we gather during the definition phase. As there be only one namespace/registry we create a singleton class that holds the functions in a dictionary whose key will not be just a function name but the tuple we get from the `key` function, which contains elements that uniquely identify function in the entire codebase. Through this, we will be able to hold functions in the registry even if they have the same name (but different arguments) and thus facilitating function overloading.

```
class Namespace(object):
  """Namespace is the singleton class that is responsible
  for holding all the functions.
  """
  __instance = None

  def __init__(self):
    if self.__instance is None:
      self.function_map = dict()
      Namespace.__instance = self
    else:
      raise Exception("cannot instantiate a virtual Namespace again")

  @staticmethod
  def get_instance():
    if Namespace.__instance is None:
      Namespace()
    return Namespace.__instance

  def register(self, fn):
    """registers the function in the virtual namespace and returns
    an instance of callable Function that wraps the
    function fn.
    """
    func = Function(fn)
    self.function_map[func.key()] = fn
    return func
```

The `Namespace` has a method `register` that takes function `fn` as an argument, creates a unique key for it, stores it in the dictionary and returns `fn` wrapped within an instance of `Function`. This means the return value from the `register` function is also callable and (till now) its behavior is exactly the same as the wrapped function `fn`.

```
def area(l, b):
  return l * b

>>> namespace = Namespace.get_instance()
>>> func = namespace.register(area)
>>> func(3, 4)
12
```

## Using decorators as a hook

Now that we have defined a virtual namespace with an ability to register a function, we need a hook that gets called during function definition; and here use Python decorators. In Python, a decorator wraps a function and allows us to add new functionality to an existing function without modifying its structure. A decorator accepts the wrapped function `fn` as an argument and returns another function that gets invoked instead. This function accepts `args` and `kwargs` passed during function invocation and returns the value.

A sample decorator that times execution of a function is demonstrated below

```
import time


def my_decorator(fn):
  """my_decorator is a custom decorator that wraps any function
  and prints on stdout the time for execution.
  """
  def wrapper_function(*args, **kwargs):
    start_time = time.time()

    # invoking the wrapped function and getting the return value.
    value = fn(*args, **kwargs)
    print("the function execution took:", time.time() - start_time, "seconds")

    # returning the value got after invoking the wrapped function
    return value

  return wrapper_function


@my_decorator
def area(l, b):
  return l * b


>>> area(3, 4)
the function execution took: 9.5367431640625e-07 seconds
12
```

In the example above we define a decorator named `my_decorator` that wraps function `area` and prints on `stdout` the time it took for the execution.

The decorator function `my_decorator` is called every time (so that it wraps the decorated function and store this new wrapper function in Python’s local or global namespace) the interpreter encounters a function definition, and it is an ideal hook, for us, to register the function in our virtual namespace. Hence we create our decorator named `overload` which registers the function in virtual namespace and returns a callable to be invoked.

```
def overload(fn):
  """overload is the decorator that wraps the function
  and returns a callable object of type Function.
  """
  return Namespace.get_instance().register(fn)
```

The `overload` decorator returns an instance of `Function`, as returned by `.register()` the function of the namespace. Now whenever the function (decorated by `overload`) is called, it invokes the function returned by the `.register()` function - an instance of `Function` and the `__call__` method gets executed with specified `args` and `kwargs` passed during invocation. Now what remains is implementing the `__call__` method in class `Function` such that it invokes the appropriate function given the arguments passed during invocation.

## Finding the right function from the namespace

The scope of disambiguation, apart from the usuals module class and name, is the number of arguments the function accepts and hence we define a method called `get` in our virtual namespace that accepts the function from the python’s namespace (will be the last definition for the same name - as we did not alter the default behavior of Python’s namespace) and the arguments passed during invocation (our disambiguation factor) and returns the disambiguated function to be invoked.

The role of this `get` function is to decide which implementation of a function (if overloaded) is to be invoked. The process of getting the appropriate function is pretty simple - from the function and the arguments create the unique key using `key` function (as was done while registering) and see if it exists in the function registry; if it does then fetch the implementation stored against it.

```
def get(self, fn, *args):
  """get returns the matching function from the virtual namespace.

  return None if it did not fund any matching function.
  """
  func = Function(fn)
  return self.function_map.get(func.key(args=args))
```

The `get` function creates an instance of `Function` just so that it could use the `key` function to get a unique key and not replicate the logic. The key is then used to fetch the appropriate function from the function registry.

## Invoking the function

As stated above, the `__call__` method within class `Function` is invoked every time a function decorated with an `overload` decorator is called. We use this function to fetch the appropriate function using the `get` function of namespace and invoke the required implementation of the overloaded function. The `__call__` method is implemented as follows

```
def __call__(self, *args, **kwargs):
  """Overriding the __call__ function which makes the
  instance callable.
  """
  # fetching the function to be invoked from the virtual namespace
  # through the arguments.
  fn = Namespace.get_instance().get(self.fn, *args)
  if not fn:
    raise Exception("no matching function found.")

  # invoking the wrapped function and returning the value.
  return fn(*args, **kwargs)
```

The method fetches the appropriate function from the virtual namespace and if it did not find any function it raises an `Exception` and if it does, it invokes that function and returns the value.

## Function overloading in action

Once all the code is put into place we define two functions named `area`: one calculates the area of a rectangle and the other calculate the area of a circle. Both functions are defined below and decorated with an `overload` decorator.

```
@overload
def area(l, b):
  return l * b

@overload
def area(r):
  import math
  return math.pi * r ** 2


>>> area(3, 4)
12
>>> area(7)
153.93804002589985
```

When we invoke `area` with one argument it returns the area of a circle and when we pass two arguments it invokes the function that computes the area of a rectangle thus overloading the function `area`. You can find the entire working demo [here](https://repl.it/@arpitbbhayani/Python-Function-Overloading).

> Python supports function overloading using [functools.singledispatch](https://docs.python.org/3/library/functools.html#functools.singledispatch) since Python 3.4 and supports overloading on class and instance methods using [functools.singledispatchmethod](https://docs.python.org/3/library/functools.html#functools.singledispatchmethod) since Python 3.8. Thanks [Harry Percival](https://twitter.com/hjwp) for the correction.

# Conclusion

Python does not support function overloading but by using common language constructs we hacked a solution to it. We used decorators and a user-maintained namespace to overload functions and used the number of arguments as a disambiguation factor. We could also use data types (defined in decorator) of arguments for disambiguation - which allows functions with the same number of arguments but different types to overload. The granularity of overload is only limited by function `getfullargspec` and our imagination. A neater, cleaner and more efficient approach is also possible with the above constructs so feel free to implement one and tweet me [@arpit_bhayani](https://twitter.com/arpit_bhayani), I will be thrilled to learn what you have done with it.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/isolation-forest

Isolation Forest Algorithm for Anomaly Detection

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Advanced and Approximate Algorithms](/knowledge-base/algorithms-and-explorations)

# Isolation Forest Algorithm for Anomaly Detection

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Anomaly detection is identifying something that could not be stated as “normal”; the definition of “normal” depends on the phenomenon that is being observed and the properties it bears. In this article, we dive deep into an unsupervised anomaly detection algorithm called [Isolation Forest](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf). This algorithm beautifully exploits the characteristics of anomalies, keeping it independent of data distributions making the approach novel.

### Characteristics of anomalies

Since anomalies deviate from normal, they are few in numbers (minority) and/or have attribute values that are very different from those of normal. The paper nicely puts it as **few and different**. These characteristics of anomalies make them more susceptible to isolation than normal points and form the guiding principle of the Isolation Forest algorithm.

# The usual approach for detecting anomalies

The existing models train to see what constitutes “normal” and then label everything that does not conform to this definition as anomalies. Almost every single algorithm has its own way of defining a normal point/instance; some do it through statistical methods, some use classification or clustering but in the end, the process remains the same - define normal and filter out everything else.

### The issue with the usual approach

The usual methods are not optimized to detect anomalies, instead, they are optimized to find normal instances, because of which the result of anomaly detection either contains too many false positives or might detect too few anomalies.
Many of these methods are computationally complex and hence suit low dimensional and/or small-sized data.

Isolation Forest algorithm addresses both of the above concerns and provides an efficient and accurate way to detect anomalies.

# The algorithm

Now we take a go through the algorithm, and dissect it stage by stage and in the process understand the math behind it. Fasten your seat belts, it’s going to be a bumpy ride.

## The core principle

The core of the algorithm is to “isolate” anomalies by creating decision trees over random attributes. The random partitioning produces noticeable shorter paths for anomalies since

- fewer instances (of anomalies) result in smaller partitions
- distinguishable attribute values are more likely to be separated in early partitioning

Hence, when a forest of random trees collectively produces shorter path lengths for some particular points, then they are highly likely to be anomalies.

![Decision tree splits for normal points and anomalies](https://user-images.githubusercontent.com/4745789/73243800-804fc000-41ce-11ea-826f-14cbc407af99.png)

The diagram above shows the number of splits required to isolate a normal point and an anomaly. Splits, represented through blue lines, happens at random on a random attribute and in the process building a decision tree. The number of splits determines the level at which the isolation happened and will be used to generate the anomaly score.

The process is repeated multiple times and we note the isolation level for each point/instance. Once the iterations are over, we generate an anomaly score for each point/instance, suggesting its likeliness to be an anomaly. The score is a function of the average level at which the point was isolated. The top `m` gathered on the basis of the score, are labeled as anomalies.

## Construction of decision tree

The decision tree is constructed by splitting the sub-sample points/instances over a split value of a randomly selected attribute such that the instances whose corresponding attribute value is smaller than the split value goes left and the others go right, and the process is continued recursively until the tree is fully constructed. The split value is selected at random between the minimum and maximum values of the selected attribute.

There are two types of node in the decision tree

### Internal Node

Internal nodes are non-leaf and contain the split value, split attribute and pointers to two child sub-trees. An internal node is always a parent to two child sub-trees making the entire decision tree a proper binary tree.

### External Node

External nodes are leaf nodes that could not be split further and reside at the bottom of the tree. Each external node will hold the size of the un-built subtree which is used to calculate the anomaly score.

![Decision tree with internal and external nodes](https://user-images.githubusercontent.com/4745789/73272711-d5a8c300-4208-11ea-9bb7-80894312f16c.png)

## Why sub-sampling helps

The Isolation Forest algorithm works well when the trees are created, not from the entire dataset, but from a sub-sampled data set. This is very different from almost all other techniques where they thrive on data and demands more of it for greater accuracy. Sub-sampling works wonder in this algorithm because normal instances can interfere with the isolation process by being a little closer to the anomalies.

![Importance of sub-sampling in Isolation Forest](https://user-images.githubusercontent.com/4745789/73296518-df91ec80-422f-11ea-8c6b-2a2fcbf8afc8.png)

The image above shows how sub-sampling actually makes a clear separation between normal points and anomalies. In the original dataset, we see that normal points and very close to anomalies making detection tougher and inaccurate (with a lot of false negatives). Because of sub-sampling, we could see a clear separation of anomalies and normal instances. This makes the entire process of anomaly detection efficient and accurate.

### Optimizing decision tree construction

Since anomalies are susceptible to isolation and have a tendency to reside closer to the root of the decision tree, we construct the decision tree till it reaches a certain height `max_height` and not split points further. This height is the height post which we are (almost) sure that there could not be any anomalies.

```
def construct_tree(X, current_height, max_height):
  """The function constructs a tree/sub-tree on points X.

  current_height: represents the height of the current tree to
    the root of the decision tree.
  max_height: the max height of the tree that should be constructed.

  The current_height and max_height only exists to make the algorithm efficient
  as we assume that no anomalies exist at depth >= max_height.
  """
  if current_height >= max_height:
    # here we are sure that no anomalies exist hence we
    # directly construct the external node.
    return new_external_node(X)

  # pick any attribute at random.
  attribute = get_random_attribute(X)

  # for set of inputs X, for the tree we get a random value
  # for the chosen attribute. preferably around the median.
  split_value = get_random_value(max_value, min_value)

  # split X instances based on `split_values` into Xl and Xr
  Xl = filter(X, lambda x: X[attribute] < split_value)
  Xr = filter(X, lambda x: X[attribute] >= split_value)

  # build an internal node with its left subtree created from Xl
  # and right subtree created from Xr, recursively.
  return new_internal_node(
    left=construct_tree(Xl, current_height + 1, max_height),
    right=construct_tree(Xr, current_height + 1, max_height),
    split_attribute=attribute,
    split_value=split_value,
  )
```

## Constructing the forest

The process of tree construction is repeated multiple times and each time we pick a random sub-sample and construct the tree. There are no strict rules to determine the number of iterations, but in general, we could say the more the merrier. The sub-sampling count is also a parameter and could change depending on the data set.

The pseudocode for forest construction is as follows

```
def construct_forest(X, trees_count, subsample_count):
  """The function constructs a forest from given inputs/data points X.
  """
  forest = []
  for i in range(0, trees_count):
    # max_height is in fact the average height of the tree that would be
    # constructed from given points. This acts as max_height for the
    # construction because we are only interested in data points that have
    # shorter-than-average path lengths, as those points are more likely
    # to be anomalies.
    max_height = math.ceil(math.log2(subsample_count))

    # create a sample with cardinality of `subsample_count` from X
    X_sample = get_sample(X, subsample_count)

    # construct the decision tree from the sample
    tree = construct_tree(X_sample, 0, max_height)

    # add the tree to the forest
    forest.append(tree)

  return forest
```

While constructing the tree we pass `max_height` as `log2(nodes_count)` as that is the average height of a proper binary tree that could be constructed from `nodes_count` number of nodes. Since anomalies reside closer to the root node it is highly unlikely that any anomaly will isolate after the tree has reached height `max_height`. This helps us save a lot of computation and tree construction making it computationally and memory efficient.

## Scoring the anomalies

Every anomaly detection algorithm has to score its data points/instances and quantify the confidence the algorithm has on its potential anomalies. The generated anomaly score has to be bounded and comparable. In Isolation Forest, that fact that anomalies always stay closer to the root, becomes our guiding and defining insight that will help us build a scoring function. The anomaly score will a function of path length which is defined as

> Path Length `h(x)` of a point `x` is the number of edges `x` traverses from the root node.

As the maximum possible height of the tree grows by order of `n`, the average height grows by `log(n)` - this makes normalizing of the scoring function a little tricky. To remedy this we use the insights from the structure of the decision tree. The decision tree has two types of nodes internal and external such that external has no child while internal is a parent to exactly two nodes - which means the decision tree is a proper binary tree and hence we conclude

> The average path length `h(x)` for external node termination is the same as the average path length of unsuccessful search in BST.

In a BST, an unsuccessful search always terminates at a `NULL` pointer and if we treat external node of the decision tree as `NULL` of BST, then we could say that average path length of external node termination is same as average path length of unsuccessful search in BST (constructed only from internal nodes of the decision tree), and it is given by

![BST unsuccessful search estimation](https://user-images.githubusercontent.com/4745789/73191802-198ac200-414e-11ea-9500-039483b6e780.png)

where `H(i)` is the [harmonic number](https://en.wikipedia.org/wiki/Harmonic_number) and it can be estimated by `ln(i) + 0.5772156649` ([Euler–Mascheroni constant](https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant)). `c(n)` is the average of path length `h(x)` given `n`, we use it to normalize `h(x)`.

_To understand the derivation in detail refer to the references at the end of this article._

The anomaly score of an instance `x` is defined as

![scoring function](https://user-images.githubusercontent.com/4745789/73192432-075d5380-414f-11ea-86dc-ae6acda7b7d4.png)

where `E(h(x))` is the average path length (average of `h(x)`) from a collection of isolation trees. From the scoring function defined above, we could deduce that if

- the score is very close to 1, then they are definitely anomalies
- the score is much smaller than 0.5, then they are quite safe to be regarded as normal instances, and
- all the instances return around 0.5, then the entire sample does not really have any distinct anomaly

## Evaluating anomalies

In the evaluation stage, an anomaly score is derived from the expected path length `E(h(x))` for each test instance. Using `get_path_length` function (pseudocode below), a single path length `h(x)` is calculated by traversing through the decision tree.

If iteration terminates at an external node where `size > 1` then the return value is `e` (number of edges traversed till current node) plus an adjustment `c(size)`, estimated from the formula above. This adjustment is for the unbuilt decision tree (for efficiency) beyond the max height. When `h(x)` is obtained for each node of each tree, an anomaly score is produced by computing `s(x, sample_size)`. Sorting instances by the score `s` in descending order and getting top `m` will yield us `m` anomalies.

```
def get_path_length(x, T, e):
  """The function returns the path length h(x) of an instance
  x in tree `T`.

  here e is the number of edges traversed from the root till the current
  subtree T.
  """
  if is_external_node(T):
    # when T is the root of an external node subtree
    # we estimate path length and return.

    # here c is the function which estimates the average path length
    # for external node termination.
    return e + c(len(T))

  # T is the root of an internal node then we
  if x[T.split_attribute] < T[split_value]:
    # instance x may lie in left subtree
    return get_path_length(x, T.left, e + 1)
  else:
    # instance x may lie in right subtree
    return get_path_length(x, T.right, e + 1)
```

## References for BST unsuccessful search estimation

- [IIT KGP, Algorithms, Lecture Notes - Page 7](https://cse.iitkgp.ac.in/~pb/algo-1-pb-10.pdf)
- [What is real big-O of search in BST?](https://www.cs.csustan.edu/~john/classes/previous_semesters/cs3100_datastructures/2000_04_Fall/Examples/Trees/averageSearchInBST.html)
- [CMU CMSC 420: Lecture 5 - Slide 13](https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/trees.pdf)
- [CISE UFL: Data Structures, Algorithms, & Applications - 1st Proof](https://www.cise.ufl.edu/~sahni/dsaac/public/exer/c18/e47.htm)

# Conclusion

The isolation forest algorithm thrives on sub-sampled data and does not need to build the tree from the entire data set; it works well with sub-sampled data. While constructing the tree, we need not build tree taller than `max_height` (very cheap to compute), making it low on memory footprint. Since the algorithm does not depend on computationally expensive operations like distance or density calculation, it executes really fast. The training stage has a linear time complexity with a low constant and hence could be used in a real-time online system.

I hope this article helped you to understand Isolation Forest, an unsupervised anomaly detection algorithm. I stumbled upon this through an engineering [blog](https://lambda.grofers.com/anomaly-detection-using-isolation-forest-80b3a3d1a9d8) of [Grofers](https://grofers.com/). This algorithm was very interesting to me because of its novel approach and hence I dived deep into it. FYI: In 2018, Isolation Forest was extended by [Sahand Hariri, Matias Carrasco Kind, Robert J. Brunner](https://arxiv.org/pdf/1811.02141.pdf). I have not read the Extended Isolation Forest algorithm but have definitely added it to my reading list. I recommend that if you liked this algorithm you should definitely give the extended version a skim.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/image-steganography

Image Steganography - the art of hiding data in images

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)
- [Cryptography](/knowledge-base/algorithms-and-explorations)

- [KB](/knowledge-base)
- [Adva...](/knowledge-base/algorithms-and-explorations)
- [Cryptography](/knowledge-base/algorithms-and-explorations)

# Image Steganography - the art of hiding data in images

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Would you shave your head and get it tattooed? Probably no, but a slave in ancient Greece was made to do so in the 440 BCE by a ruler named [Histiaeus](https://en.wikipedia.org/wiki/Histiaeus). The text that was tattooed was a secret message that Histiaeus wanted to send to his son-in-law Aristagoras in Miletus. After his hair grew back the slave left for Miletus and upon his arrival, his head was shaved again and the message was revealed which told Aristagoras to revolt against the Persians and start the [Ionian revolt](https://en.wikipedia.org/wiki/Ionian_Revolt).

This art of concealing message is called Steganography. The word is derived from the Greek word “στεγαυω” which means “secret or covered writing”. In modern times, steganography can be looked into as the study of the art and science of communicating in a way that hides the presence of the communication.

Steganography continued over time to develop into new levels. Invisible inks, microdots, writing behind postal stamps are all examples of steganography in its physical form. Most of these early developments happened during World War I and II where everyone was trying to outsmart each other. The left half of the image below is a bunch of microdots, sent by German spies and intercepted by Allied intelligence, and the right half is the camera that was used to print such microdots.

![Microdots and Microdot Camera](https://user-images.githubusercontent.com/4745789/72497176-da0ccd80-3851-11ea-96b0-759d7e62f451.png)

### Steganography and Cryptography

Since the rise of the Internet, making communication more secure has been a priority. This lead to the development of the field of Cryptography that deals with hiding the meaning of a message. The techniques of cryptography try to ensure that it becomes extremely difficult to extract the true meaning of the message when it goes into the wrong hands.

Sometimes, it becomes necessary to not only hide the meaning of the message but also hide its existence, and the field that deals with this is called Steganography. Both cryptography and steganography, protect the information in their own way but neither alone is perfect and can be compromised. Hence a hybrid approach where we encrypt the message and then hide its presence amplifies the security.

Today steganography is mostly used on computers with digital data, like Image, Audio, Video, Network packets, etc, acting as the carriers. There are a bunch of techniques for each of them but this article aims to provide an exhaustive overview of Image Steganography.

# Image Steganography

Images are an excellent medium for concealing information because they provide a high degree of redundancy - which means that there are lots of bits that are there to provide accuracy far greater than necessary for the object’s use (or display). Steganography techniques exploit these redundant bits to hide the information/payload by altering them in such a way that alterations cannot be detected easily by humans or computers.

## Color depth and definition

An image is a collection of numbers that defines color intensities in different areas of the image. It is arranged in a gird, which is the resolution of the image, and each point on the grid is called a pixel. Each pixel is defined by a fixed number of bits and this is its color scheme. The smallest color depth is 8 bit (in monochrome and greyscale images) and it displays 256 different colors or shades of grey as shown below.

![8-bit grayscale monochrome image](https://user-images.githubusercontent.com/4745789/72497072-8e5a2400-3851-11ea-9b28-8705bbfea070.png)

Digital color images are typically stored in 24-bit pixel depth and uses the RGB color model. All color variations for the pixels of a 24-bit image are derived from three primary colors: red, green and blue, and each primary color is represented by 8 bits. Thus each pixel takes one from a palette of 16-million colors.

![24-bit color palette](https://user-images.githubusercontent.com/4745789/72497287-23f5b380-3852-11ea-96e8-e5c8ffca0c9f.png)

## Compression

When working with high-resolution images with greater color depth, the size of the raw file can become big and it becomes impossible to transmit it over a standard internet connection. To remedy this, compressed image formats were developed which, as you would have guessed, compresses the pixel information and keeps file sizes fairly small, making it efficient for transmission.

Compression techniques can be broadly classified into the following two classes

### Lossy Compression

Lossy compression removes redundancies that are too small for the human eye to differentiate which makes the compressed files a close approximate, but not an exact duplicate of the original one. A famous file format that does lossy compression is [JPEG](https://en.wikipedia.org/wiki/JPEG).

### Lossless Compression

Lossless compression never removes any information from the original image, but instead represents data in mathematical formulas maintaining the integrity of the original image and when uncompressed, the file is a bit-by-bit copy of the original. Formats that do lossless compression are [PNG](https://en.wikipedia.org/wiki/Portable_Network_Graphics), [GIF](https://en.wikipedia.org/wiki/GIF), and [BMP](https://en.wikipedia.org/wiki/BMP_file_format).

Steganographic techniques take into account file formats, compression methods, and picture semantics and exploit them to find redundancies and use them to conceal secret information and can be broadly classified into two: spatial domain and frequency domain techniques, and we take a deeper look into both.

# Spatial Domain Techniques

Spatial domain techniques embed the secret message/payload in the intensity of the pixels directly; which means they update the pixel data by either inserting or substituting bits. Lossless images are best suited for these techniques as compression would not alter the embedded data. These techniques have to be aware of the image format to make concealing information fool-proof.

## LSB Substitution

This technique converts the secret message/payload into a bitstream and substitutes them into a least significant bit (the 8th bit) of some or all bytes inside an image. The alterations happen on the least significant bit which changes the intensity by +-1 which is extremely difficult for the human eye to detect.

![LSB substitution](https://user-images.githubusercontent.com/4745789/72587393-1eb06b80-391b-11ea-89ce-eb72be220a89.png)

When using a 24-bit image, a bit of each of the red, green and blue color components is substituted. Since there are 256 possible intensities of each primary color, changing the LSB of pixel results in small changes in the intensity of the colors.

![24-bit image LSB substitution](https://user-images.githubusercontent.com/4745789/72589054-2de5e800-3920-11ea-9c0a-c9f878fcd525.png)

See if you can spot what has changed in the images below. The image on the right has about 1KB long text message embedded through LSB substitution but looks the same as the original image.

![LSB substitution cat image difference](https://user-images.githubusercontent.com/4745789/72535218-31d12600-389e-11ea-9463-011fa42e430c.png)

In a 24 bit image we can store 3 bits in each pixel hence an 800 × 600 pixel image, can thus store a total amount of 1,440,000 bits or 180,000 bytes ~ 175KB of embedded data.

## Extending LSB to k-LSB

To hold more data into the image we can substitute not `1` but `k` least significant bits. But when we do so the image starts to distort which is never a good sign but a well-chosen image could do the trick and you wouldn’t notice any difference.

## Randomized LSB

A regular LSB substitution technique starts substituting from pixel `0` and goes till `n` making this method highly predictable. To make things slightly challenging sender and receiver could share a secret key through which they agree on the certain pixels that will be altered making the technique more robust.

## Adaptive LSB

Adaptive LSB uses k-bit LSB and varies `k` as per the sensitivity of the image region over which it is applied. The method analyzes the edges, brightness, and texture of the image and calculates the value of `k` for that region and then does regular k-LSB substitution on it. It keeps the value of `k` high at a not-so-sensitive image region and low at the sensitive region. These alterations ensure that the overall quality of the image is balanced and distortions harder to detect.

**Pixel-value differencing (PVD)** scheme is a concrete implementation of adaptive LSB and it uses the difference of values between two consecutive pixels in a block to determine the number of secret bits to be embedded.

## LSB and Palette Based Images

The persistence of Palette Based Images is very interesting. There is a color lookup table which holds all the colors that are used in the image. Each pixel is represented as a single byte and the pixel data is an index to the color palette. [GIF](https://en.wikipedia.org/wiki/GIF) images work on this principle; it cannot have a bit depth greater than 8, thus the maximum number of colors that a GIF can store is 256. Now if we perform LSB substitution to pixel data then it changes the index in the lookup table (palette) and the new value (after substitution), that points to the index on the lookup table (palette), could point to a different color and the change will be evident. We could still do steganography on palette-based images using following workarounds

![palette-based image](https://user-images.githubusercontent.com/4745789/72600791-4ebb3700-393a-11ea-8e3e-2ddf389e85d1.png)

### Sorting the palette

The LSB substitution alters the value by +-1 and hence it will always point to a neighboring entry in the table. Hence we sort the palette by color then this will make adjacent lookup table entries similar to each other and minimize the distortion.

### Add new colors to the palette

If the original image has fewer colors then we could add similar colors in color palette/lookup table and then perform regular LSB substitution. Again the +-1 alteration will make that pixel point to some similar color in the lookup table.

## Other techniques

Apart from the above-mentioned LSB substitution technique, there are techniques that exploit some aspect of the image and embeds data. I would highly recommend you at least give a skim to each of the below:

- [Edges based data embedding method (EBE)](https://link.springer.com/article/10.1186/1687-417X-2014-8)
- [Random pixel embedding method (RPE)](https://ieeexplore.ieee.org/abstract/document/8276335)
- [Mapping pixel to hidden data method](https://www.researchgate.net/publication/26623039_Image_Steganography_by_Mapping_Pixels_to_Letters)
- [Labeling or connectivity method](https://www.researchgate.net/publication/239551978_Labeling_Method_in_Steganography)

# Frequency Domain Techniques

Spatial domain techniques directly start putting in data from payload into an image but Frequency-domain techniques will first transform the image and then embed the data. The transformation step ensures that the message is hidden in less sensitive areas of the image, making the hiding more robust and makes the entire process independent of the image format. The areas in which the information is hidden are usually less exposed to compression, cropping, and image processing.

These techniques are relatively complex to comprehend and require a bit of advanced mathematics to understand thoroughly. Images with lossy compression are ideal candidates and hence we dive a little deep into how JPEG steganography works.

## JPEG steganography

To understand how steganography works for JPEG files, we will look into: how the raw data is compressed by JPEG and then we see how we could hide data in it.

### JPEG Compression

According to research, the human eye is more sensitive to changes in the brightness (luminance) of a pixel than to changes in its color. We interpret brightness and color by contrast with adjacent regions. The compression phase takes advantage of this insight and transforms the image from RGB color to [YCbCr](https://en.wikipedia.org/wiki/YCbCr) representation - separating brightness from color. In YCbCr representation, the Y component corresponds to luminance (brightness - black-white) and Cb (yellow-blue) and Cr (green-red) components for chrominance (color). Now we discard some of the color data by downsampling it to half in both horizontal and vertical directions thus directly reducing the size of the file by a factor of 2.

![YCbCr transformation](https://user-images.githubusercontent.com/4745789/72549559-f1ca6d00-38b6-11ea-9760-bd1f35dbf455.png)

Now the image, in YCbCr representation, is processed in blocks of 8 x 8 and we perform [Discrete Cosine Transform (DCT)](https://en.wikipedia.org/wiki/Discrete_cosine_transform) on each, then quantized (rounding) 64 values into 1 by taking the average. The quantization step is the one that removes redundant information from the image. To dive more into DCT on JPEG, I would recommend you watch this [Computerphile video](https://www.youtube.com/watch?v=Q2aEzeMDHMA).

This is the first stage of JPEG compression which is lossy. Now this image data is then losslessly compressed using the standard [Huffman encoding](https://en.wikipedia.org/wiki/Huffman_coding).

### JPEG Steganography

Since JPEG images are already lossily compressed (redundant bits are already thrown out) it was thought that steganography would not be possible on it. So if we would try to hide or embed any message in it, it might get either lost, destroyed or altered during compression, adding some noticeable changes to the image. The complete JPEG encoding process is as shown in the diagram below

![JPEG Process](https://user-images.githubusercontent.com/4745789/72615006-a87f2980-3959-11ea-872f-733c9523a411.png)

The entire process could be split into two stages, the first is where redundancy is removed and the second is where the data is encoded using Huffman encoding. During the DCT transformation phase, rounding errors occur in the coefficient data that are not noticeable and this makes the algorithm lossy. Once this stage is over we have a chance to perform usual LSB substitution and embed the message. Since stage 2 of JPEG compression is lossless, due to Huffman encoding, we are sure that none of our substituted data will be lost. Thus we sandwich the steganography between the lossy and lossless stages of JPEG compression.

## Other techniques

Apart from the above-mentioned DCT technique, there are techniques that use a different form of transform signal and embeds secret data. To name a few

- [Discrete Fourier transformation technique (DFT)](https://link.springer.com/chapter/10.1007/978-3-642-20998-7_39)
- [Discrete Wavelet transformation technique (DWT)](https://www.insight-centre.org/sites/default/files/publications/17.197_a_steganography_technique_for_images_based_on_wavelet_transform.pdf)
- [Lossless or reversible method (DCT)](https://www.researchgate.net/publication/330565811_Hiding_data_in_images_using_DCT_steganography_techniques_with_compression_algorithms)
- [Embedding in coefficient bits](http://www.ijcee.org/papers/533-P0025.pdf)

# Conclusion

This is the first article in the series of Steganography that detailed out Image Steganography. I hope you reaped some benefits out of it. The future articles on Steganography will talk about how it is done on carriers like Audio, Network, [DNA](https://www.sciencedirect.com/science/article/pii/S1877050917319804) and [Quantum states](https://arxiv.org/abs/1006.1934) and will also dive into one of the most interesting applications of Steganography - a [Steganographic File System](https://en.wikipedia.org/wiki/Steganographic_file_system). So stay tuned and watch this space for more.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/long-integers-python

How Python supports integers of infinite length - a deep dive into CPython

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Language Internals](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

- [KB](/knowledge-base)
- [Lang...](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

# How Python supports integers of infinite length - a deep dive into CPython

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

When you code in a low-level language like C, you worry about picking the right data type and qualifiers for your integers; at every step, you need to think if `int` would suffice or should you go for a `long` or even higher to a `long double`. But while coding in python, you need not worry about these “trivial” things because python supports integers of arbitrary size.

In C, when you try to compute 220000 using builtin `powl` function it gives you `inf` as the output.

```
#include <stdio.h>
#include <math.h>

int main(void) {
  printf("%Lf\n", powl(2, 20000));
  return 0;
}

$ ./a.out
inf
```

But for python, it is a piece of cake 🎂

```
>>> 2 ** 20000
39802768403379665923543072061912024537047727804924259387134 ...
...
... 6021 digits long ...
...
6309376
```

Python must be doing something beautiful internally to support integers of arbitrary sizes and today we find out what’s under the hood!

# Representation and definition

An integer in Python is a C struct defined as following

```
struct _longobject {
    PyObject_VAR_HEAD
    digit ob_digit[1];
};
```

`PyObject_VAR_HEAD` is a macro that expands into a `PyVarObject` that has the following structure

```
typedef struct {
    PyObject ob_base;
    Py_ssize_t ob_size; /* Number of items in variable part */
} PyVarObject;
```

Other types that has `PyObject_VAR_HEAD` are

- `PyBytesObject`
- `PyTupleObject`
- `PyListObject`

This indicates that an integer, just like a `tuple` or a `list`, is variable in length and this is our first insight into how it could support gigantically long integers. The `_longobject` after macro expansion could be roughly seen as

```
struct _longobject {
    PyObject ob_base;
    Py_ssize_t ob_size; /* Number of items in variable part */
    digit ob_digit[1];
};
```

> These are some meta fields in the `PyObject` struct, used for reference counting (garbage collection), but that we would require a separate article. The field that we will focus on is `ob_digit` and to some extent `ob_size`.

### Decoding `ob_digit`

`ob_digit` is an array of type `digit`, typedef’ed from `uint32_t`, statically allocated to length `1`. Since it is an array, `ob_digit` primarily is a `digit *`, pointer to `digit`, and hence if required could be malloced to any length. This makes it possible for python to represent and handle gigantically long integers.

Generally, In low-level languages like C, the precision of integers is limited to 64-bit, but Python implements [Arbitrary-precision integers](https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic). Since Python 3 all integers are represented as a bignum and these are limited only by the available memory of the host system.

### Decoding `ob_size`

`ob_size` holds the count of elements in `ob_digit`. To be more efficient while allocating the memory to array `ob_digit`, python over-provisions and then relies on the value of `ob_size` to determine the actual number of elements held int the array.

# Storage

A naive way to store an integer digit-wise is by actually storing a decimal digit in one item of the array and then operations like addition and subtraction could be performed just like grade school mathematics.

With this approach, a number `5238` will be stored as

![representation of 5238 in a naive way](https://user-images.githubusercontent.com/4745789/71915727-5e03ed00-31a2-11ea-99c1-cdf28e74b595.png)

This approach is inefficient as we will be using up 32 bits of digit (`uint32_t`) to store a decimal digit that actually ranges only from 0 to 9 and could have been easily represented by mere 4 bits, and while writing something as versatile as python, a core developer has to be more resourceful than this.

So, can we do better? for sure, otherwise, this article should hold no place on the internet. Let’s dive into how python stores a super long integer.

## The pythonic way

Instead of storing just one decimal digit in each item of the array `ob_digit`, python converts the number from base 10 to base 230 and calls each of element as `digit` which ranges from 0 to 230 - 1.

In the hexadecimal number system, the base is 16 ~ 24 this means each “digit” of a hexadecimal number ranges from 0 to 15 of the decimal system. Similarly for python, “digit” is in base 230 which means it will range from 0 to 230 - 1 = 1073741823 of the decimal system.

This way python efficiently uses almost all of the allocated space of 32 bits per digit and keeps itself resourceful and still performs operations such as addition and subtraction like grade school mathematics.

> Depending on the platform, Python uses either 32-bit unsigned integer arrays with 30-bit digits or 16-bit unsigned integer arrays with 15-bit digits. It requires a couple of bits to perform operations that will be discussed in some future articles.

### Example: 1152921504606846976

As mentioned, for Python a “digit” is base 230 hence if you convert `1152921504606846976` into base 230 you get `100`

**1152921504606846976** = **1** \_ (230)2 + **0** \_ (230)1 + **0** \* (230)0

Since `ob_digit` persists it least significant digit first, it gets stored as `001` in 3 different digits.

The `_longobject` struct for this value will hold

- `ob_size` as `3`
- `ob_digit` as `[0, 0, 1]`

![representation of 1152921504606846976 in a pythonic way](https://user-images.githubusercontent.com/4745789/72000622-b5b95b80-3269-11ea-9e76-1755cd648f0d.png)

I have created a [demo REPL](https://repl.it/@arpitbbhayani/super-long-int?language=python3) that will output the way python is storing integers internally and also has reference to struct members like `ob_size`, `ob_refcount`, etc.

# Operations on super long integers

Now that we have a fair idea on how python supports and implements arbitrary precision integers its time to understand how various mathematical operations happen on them.

## Addition

Integers are persisted “digit-wise”, this means the addition is as simple as what we learned in the grade school and python’s source code shows us that this is exactly how it is implemented as well. The function named [x_add](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L3116) in file [longobject.c](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c) performs the addition of two numbers.

```
...
    for (i = 0; i < size_b; ++i) {
        carry += a->ob_digit[i] + b->ob_digit[i];
        z->ob_digit[i] = carry & PyLong_MASK;
        carry >>= PyLong_SHIFT;
    }
    for (; i < size_a; ++i) {
        carry += a->ob_digit[i];
        z->ob_digit[i] = carry & PyLong_MASK;
        carry >>= PyLong_SHIFT;
    }
    z->ob_digit[i] = carry;
...
```

The code snippet above is taken from `x_add` function and you could see that it iterates over the digits and performs addition digit-wise and computes and propagates carry.

> Things become interesting when the result of the addition is a negative number. The sign of `ob_size` is the sign of the integer, which means, if you have a negative number then `ob_size` will be negative. The absolute value of `ob_size` will determine the number of digits in `ob_digit`.

## Subtraction

Similar to how addition is implemented, subtraction also happens digit-wise. The function named [x_sub](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L3150) in file [longobject.c](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c) performs subtraction of two numbers.

```
...
    for (i = 0; i < size_b; ++i) {
        borrow = a->ob_digit[i] - b->ob_digit[i] - borrow;
        z->ob_digit[i] = borrow & PyLong_MASK;
        borrow >>= PyLong_SHIFT;
        borrow &= 1; /* Keep only one sign bit */
    }
    for (; i < size_a; ++i) {
        borrow = a->ob_digit[i] - borrow;
        z->ob_digit[i] = borrow & PyLong_MASK;
        borrow >>= PyLong_SHIFT;
        borrow &= 1; /* Keep only one sign bit */
    }
...
```

The code snippet above is taken from `x_sub` function and you could see how it iterates over the digits and performs subtraction and computes and propagates burrow. Very similar to addition indeed.

## Multiplication

Again a naive way to implement multiplication will be what we learned in grade school math but it won’t be very efficient. Python, in order to keep things efficient implements the [Karatsuba algorithm](https://en.wikipedia.org/wiki/Karatsuba_algorithm) that multiplies two n-digit numbers in O( nlog23) elementary steps.

The algorithm is slightly complicated is out of the scope of this article but you can find its implementation in [k_mul](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L3397) and
[k_lopsided_mul](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L3618) functions in file [longobject.c](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c).

## Division and other operations

All operations on integers are defined in the file [longobject.c](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c) and it is very simple to locate and trace each one. Warning: it will take some time to understand each one in detail so grab some popcorn before you start skimming.

# Optimization of commonly-used integers

Python [preallocates](https://docs.python.org/3/c-api/long.html#c.PyLong_FromLong) small integers in a range of -5 to 256. This allocation happens during initialization and since we cannot update integers (immutability) these preallocated integers are singletons and are directly referenced instead of reallocating. This means every time we use/creates a small integer, python instead of reallocating just returns the reference of preallocated one.

This optimization can be traced in the macro `IS_SMALL_INT` and the function [get_small_int](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L43) in [longobject.c](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L35). This way python saves a lot of space and computation for commonly used integers.

---

This essay is heavily inspired, and to some extent copied, from [Artem Golubin](https://rushter.com)’s post - [Python internals: Arbitrary-precision integer implementation](https://rushter.com/blog/python-integer-implementation/). In case you want a detailed deep dive on CPython Integers or CPython Internals in general, I recommend you checkout the [CPython Internal Series](https://rushter.com/blog/tags/cpython/) by Artem Golubin.

Thank you Artem Golubin for all the amazing CPython Internal articles.
This essay is heavily inspired, and to some extent copied, from [Artem Golubin](https://rushter.com)’s post - [Python internals: Arbitrary-precision integer implementation](https://rushter.com/blog/python-integer-implementation/). In case you want a detailed deep dive on CPython Integers or CPython Internals in general, I recommend you check out the [CPython Internal Series](https://rushter.com/blog/tags/cpython/) by Artem Golubin.

Thank you Artem Golubin for all the amazing CPython Internal articles.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/i-changed-my-python

I changed my python for fun!

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Language Internals](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

- [KB](/knowledge-base)
- [Lang...](/knowledge-base/language-internals)
- [CPython](/knowledge-base/language-internals)

# I changed my python for fun!

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Did you ever take a peek at Python’s source code? I didn’t and hence I decided to have some fun with it this week. After cloning the repository I realized how well written is the code that makes python what it is. In the process of exploring the codebase, I thought of making some changes, not big optimizations but some minor tweaks that will help me understand how Python is implemented in C and along the course learn some internals. To make things fun and interesting I thought of changing how addition work by making it incorrect and unpredictable which means `a + b` will internally do one of the following operations, at random

- `a + b`
- `a - b`
- `a * b`
- `a / b`
- `a ** b`

After forking and cloning the source code of [python](https://github.com/python/cpython), I broke down the task into following sub-tasks

- find the entry point (the main function) of python
- find where addition happens
- find how to call other perform operations like subtraction, multiplication, etc on python objects.
- write a function that picks one of the operators at random
- write a function that applies an operator on the two operands

Before getting into how I did it, take a look below and see what it does

![Random Math Operator in Python](https://user-images.githubusercontent.com/4745789/71643972-d96b2780-2ce6-11ea-894c-fd638dc95d7c.gif)

You would see how performing addition on numbers `4` and `6` evaluates to `0`, `10` and `24` depending on the operation it picked randomly.

> Note, the change I made will only work when one of the operands is a variable. If the entire expression contains constants then it will be evaluated as regular infix expression.

# Implementation

Operations in python work on opcodes very similar to the one that a microprocessor has. Depending on opcodes that the code is translated to, the operation is performed using operands (if required). The addition operation of python requires two operands and opcode is named `BINARY_ADD` and has value `23`. When the executor encounters this opcode, it fetches the two operands from top of the stack, performs addition and then pushes back the result on the stack. The code snippet below will give you a good idea of what python does when it encounters `BINARY_ADD`.

```
case TARGET(BINARY_ADD): {
    PyObject *right = POP();
    PyObject *left = TOP();
    PyObject *sum;
    if (PyUnicode_CheckExact(left) &&
             PyUnicode_CheckExact(right)) {
        sum = unicode_concatenate(tstate, left, right, f, next_instr);
    }
    else {
        sum = PyNumber_Add(left, right);
    }
    SET_TOP(sum);
    ...
}
```

> One thing to observe here is how it concatenates when both operands are unicode/string.

### Checking if operands are numbers

For checking if both the operands for `BINARY_ADD` operation are numbers I used the predefined function named `PyNumber_Check` which checks if object referenced by `PyObject` is number or not.

```
if (PyNumber_Check(left) && PyNumber_Check(right)) {
        // Both the operands are numbers
}
```

### Writing a random function

For generating random integer I used the current time in seconds from the system using `datetime.h` library and took modulus with the max value. The code snippet below picks a random number from `[0, max)`.

```
int
get_random_number(int max) {
    return time(NULL) % max;
}
```

### Functions to perform other operations

Similar to the function `PyNumber_Add` which adds two python objects (if possible), there are functions named `PyNumber_Subtract`, `PyNumber_Multiply`, `PyNumber_FloorDivide`, and `PyNumber_Power` which performs operations as suggested by their names. I wrote a util function that takes two operands and an operator and returns the resulting python object after performing the required operation.

```
PyObject *
binary_operate(PyObject * left, PyObject * right, char operator) {
    switch (operator) {
        case '+':
            return PyNumber_Add(left, right);
        case '-':
            return PyNumber_Subtract(left, right);
        case '*':
            return PyNumber_Multiply(left, right);
        case '/':
            return PyNumber_FloorDivide(left, right);
        case '^':
            return PyNumber_Power(left, right, Py_None);
        default:
            return NULL;
    }
}
```

### The new `BINARY_ADD` implementation

Now as have everything required to make our `BINARY_ADD` unpredictable and following code snippet is very close to how it could be implemented.

```
case TARGET(BINARY_ADD): {
    PyObject *right = POP();
    PyObject *left = TOP();
    PyObject *result;
    if (PyUnicode_CheckExact(left) &&
             PyUnicode_CheckExact(right)) {
        result = unicode_concatenate(tstate, left, right, f, next_instr);
    }
    else {
        // Do this operation only when both the operands are numbers and
        // the evaluation was initiated from interactive interpreter (shell)
        if (PyNumber_Check(left) && PyNumber_Check(right)) {
            char operator = get_random_operator();
            result = binary_operate(left, right, operator);
            printf(
                "::::: %s + %s was evaluated as %s %c %s, hence to the value\n",
                ReprStr(left), ReprStr(right),
                ReprStr(left), operator, ReprStr(right)
            );
        } else {
            result = PyNumber_Add(left, right);
        }
        ...
    }
    ...
    SET_TOP(result);
    ...
}
```

# Challenges

After making all the required changes I ran `make` to build my new python binary and to my surprise, the code wouldn’t build. The reason was that the function where I made the changes was called during build and initialization phases and due to incorrectness induced in the `BINARY_ADD` the process ended in **Segmentation Faults** as now it has a function that instead of adding two numbers was subtracting, multiplying, dividing and raising to power at random.

To fix this issue I had to ensure that this random picking of operator only happened when the operation is asked from the interactive shell and should continue its normal execution for others. The function that gets called during an interactive shell is `PyRun_InteractiveLoopFlags` and hence I started passing a flag named `source` to all the functions till my trail reaches the opcode evaluation flow. The value of this `source` is set to `1` when it is triggered from the interactive shell for others the default value passed is `0`. Once I had this `source` field in place with the proper value being passed from various initiations, everything worked like a charm.

You can find the detailed diff at [github.com/arpitbbhayani/cpython/pull/1/files](https://github.com/arpitbbhayani/cpython/pull/1/files).

# Conclusion

It was fun to change the python’s source code, I would recommend you to do this as well. It is always better if you know how things work internally and more importantly understand the complexities that are abstracted to make the application developers’ experience seamless.

You can find the source code at [arpitbbhayani/cpython/tree/01-randomized-math-operators](https://github.com/arpitbbhayani/cpython/tree/01-randomized-math-operators). Feel free to fork it and make some changes of your own and share it with me. I will be thrilled to learn what you did with it.

If you want to dive deep into python’s source I highly recommend you to read [realpython.com/cpython-source-code-guide/](https://realpython.com/cpython-source-code-guide/). It is an excellent guide to get you started and understand the language semantics and coding practices of a core python developer. Once you know the basics, navigating through the codebase is a walk in the park.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/benchmark-and-compare-pagination-approach-in-mongodb

I benchmarked different pagination strategies in MongoDB

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Benchmarks and Prototypes](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Benchmarks and Prototypes](/knowledge-base/database-engineering)

# I benchmarked different pagination strategies in MongoDB

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

[MongoDB](https://www.mongodb.com/) is a document based data store and hence pagination is one of the most common use case of it. So when do you paginate the response? The answer is pretty neat; you paginate whenever you want to process result in chunks. Some common scenarios are

- Batch processing
- Showing huge set of results on user interface

There are multiple approaches through which you can [paginate your result set in MongoDB](/blogs/fast-and-efficient-pagination-in-mongodb). This blog post is dedicated for results of benchmark of two approaches and its analysis, so here we go …

Benchmark has been done over a non-indexed collection. Each document of the collection looks something like this

```
    {
        "_id" : ObjectId("5936d17263623919cd5165bd"),
        "name" : "Lisa Rogers",
        "marks" : 34
    }
```

All records of a collection are fetched page-wise. Size of each page is fixed during fetch of the collection. Each page is fetched _3_ times and average of, time to fetch one “page”, 3 is recorded.

Following image shows the how two approach fares against each other.

![MongoDB Pagination Benchmark Results](https://user-images.githubusercontent.com/4745789/63220692-cb3ec380-c1aa-11e9-9882-27bf52cbaa84.png)

A key observation to note is that, till 500-600 count, both the approaches are comparable, but once it crosses that threshold, there is sudden rise in response time for `skip` and `limit` approach than other. The approach using `_id` and `limit` almost gives constant performance and is independent of size of the result set.

I tried running this test on different machines with different disks but results were similar. I think diving deep in MongoDB’s database drivier will yield better information about this behavior. You could see some spikes in the response times, that are because of Disk Contention.

In short:

- For huge result set, paginating using `_id` and `limit` is far better than using `skip` and `limit`.
- For smaller result set, it does not matter, but prefer skip and limit.

An interesting thing I observed is that after page size crosses 100, the gap between the two approach reduces to some extent. I am yet to perform detailed benchmark on that as such use-case (where page-size is more than 100) is pretty rare in practical applications.

You can find the Python code used for this benchmark [here](https://github.com/arpitbbhayani/mongo-pagination-benchmark). If you have any suggestion or improvement, do let me know.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/mongodb-cursor-skip-is-slow

Why MongoDB's cursor.skip() is slow?

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Benchmarks and Prototypes](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Benchmarks and Prototypes](/knowledge-base/database-engineering)

# Why MongoDB's cursor.skip() is slow?

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

MongoDB’s cursor object has a method called `skip`, which as per [documentation and definition](https://docs.mongodb.com/manual/reference/method/cursor.skip/#definition), controls where MongoDB begins returning results. Thus in combination with function [limit](https://docs.mongodb.com/manual/reference/method/cursor.limit/), one can easily have paginated results.

I have written a blog post on [how you can have Fast and Efficient Pagination in MongoDB](/blogs/fast-and-efficient-pagination-in-mongodb).

But while going through the documentation of skip, there is something interesting to notice. There is a small warning in [MongoDB documentation](https://docs.mongodb.com/manual/reference/method/cursor.skip/#behavior), that states

> The `cursor.skip()` method is often expensive because it requires the server to walk from the beginning of the collection or index to get the offset or skip position before beginning to return results. As the offset (e.g. `pageNumber` above) increases, `cursor.skip()` will become slower and more CPU intensive. With larger collections, `cursor.skip()` may become IO bound.

In short, MongoDB has to iterate over documents to skip them. Thus when collection or result set is huge and you need to skip documents for pagination, the call to `cursor.skip` will be expensive. While going through the source code of `skip` I found out that it does not use any index and hence gets slower when result set increases in size.

This also implies that if you use `skip` then the “skipping speed” will not improve even if you index the field.

But what if the size of result set is small? is calling `skip` still a terrible idea?
If skip was so terrible, then MongoDB team and community must had taken that decision long back. But they haven’t … why?

Because it is very efficient and fast for smaller result set. I have taken this opportunity to [benchmark and compare](/blogs/benchmark-and-compare-pagination-approach-in-mongodb) the [two approach for pagination](/blogs/fast-and-efficient-pagination-in-mongodb) and there I found out skip and limit based pagination works well for smaller result sets.

In conclusion, skip is not as bad one might think. But you must understand your use case well so as to make an informed decision.

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/fast-and-efficient-pagination-in-mongodb

How to paginate faster with consistent performance?

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [Database Engineering](/knowledge-base/database-engineering)
- [Benchmarks and Prototypes](/knowledge-base/database-engineering)

- [KB](/knowledge-base)
- [Data...](/knowledge-base/database-engineering)
- [Benchmarks and Prototypes](/knowledge-base/database-engineering)

# How to paginate faster with consistent performance?

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

[MongoDB](https://www.mongodb.com/) is a document based data store and hence pagination is one of the most common use case of it. So when do you paginate the response? The answer is pretty neat; you paginate whenever you want to process result in chunks. Some common scenarios are

- Batch processing
- Showing huge set of results on user interface

Paginating on client and server side are both really very expensive and should not be considered. Hence pagination is generally handled at database level and databases are optimized for such needs too.

Below I shall explain you the 2 approaches through which you can easily paginate your MongoDB responses.
Sample Document

```
    {
        "_id" : ObjectId("5936d17263623919cd5165bd"),
        "name" : "Lisa Rogers",
        "marks" : 34
    }
```

## Approach 1: Using `cursor.skip` and `cursor.limit`

MongoDB cursor has two methods that makes paging easy; they are

- `cursor.skip()`
- `cursor.limit()`

`skip(n)` will skip `n` documents from the cursor while `limit(n)` will cap the number of documents to be returned from the cursor. Thus combination of two naturally paginates the response.

In Mongo Shell your pagination code looks something like this

```
// Page 1
db.students.find().limit(5);

// Page 2
db.students.find().skip(5).limit(5);

// Page 3
db.students.find().skip(5).limit(5);
```

`.find()` will return a cursor pointing to all documents of the collection and then for each page we skip some and consume some. Through continuous skip and limit we get pagination in MongoDB.

I am fond of Python and hence here is a small trivial function to implement pagination:

```
    def skiplimit(page_size, page_num):
        """returns a set of documents belonging to page number `page_num`
        where size of each page is `page_size`.
        """
        # Calculate number of documents to skip
        skips = page_size * (page_num - 1)

        # Skip and limit
        cursor = db['students'].find().skip(skips).limit(page_size)

        # Return documents
        return [x for x in cursor]
```

## Approach 2: Using `_id` and `limit`

This approach will make effective use of default index on `_id` and nature of `ObjectId`.
I bet you didn’t know that a [Mongodb ObjectId](https://docs.mongodb.com/manual/reference/bson-types/#objectid) is a 12 byte structure containing

- a 4-byte value representing the seconds since the Unix epoch,
- a 3-byte machine identifier,
- a 2-byte process id, and
- a 3-byte counter, starting with a random value.

Even I didn’t until I read the [documentation](https://docs.mongodb.com/manual/reference/bson-types/#objectid). Apart from its structure there is one very interesting property of ObjectId; which is - _ObjectId has natural ordering_

What does it mean? It simplifies that we can apply all the _less-than-s_ and all the _greater-than-s you_ want to it. If you don’t believe me, open Mongo shell and execute following set of commands

```
    > ObjectId("5936d49863623919cd56f52d") > ObjectId("5936d49863623919cd56f52e")
    false
    > ObjectId("5936d49863623919cd56f52d") > ObjectId("5936d49863623919cd56f52a")
    true
```

Using this property of ObjectId and also taking into consideration the fact that `_id` is always indexed, we can devise following approach for pagination:

1. Fetch a page of documents from database
2. Get the document id of the last document of the page
3. Retrieve documents greater than that id

In Mongo Shell your pagination code looks something like this

```
    // Page 1
    db.students.find().limit(10)

    // Page 2
    last_id = ...  # logic to get last_id
    db.students.find({'_id': {'$gt': last_id}}).limit(10)

    // Page 3
    last_id = ... # logic to get last_id
    db.students.find({'_id': {'$gt': last_id}}).limit(10)
```

Again, I am fond of Python and here is the Python implementation of this approach.

```
    def idlimit(page_size, last_id=None):
        """Function returns `page_size` number of documents after last_id
        and the new last_id.
        """
        if last_id is None:
            # When it is first page
            cursor = db['students'].find().limit(page_size)
        else:
            cursor = db['students'].find({'_id': {'$gt': last_id}}).limit(page_size)

        # Get the data
        data = [x for x in cursor]

        if not data:
            # No documents left
            return None, None

        # Since documents are naturally ordered with _id, last document will
        # have max id.
        last_id = data[-1]['_id']

        # Return data and last_id
        return data, last_id
```

> If you are using a field other than `_id` for offset, make sure the field is indexed and properly ordered else the performance will suffer.

## Closing Remarks

Both of the above approaches are valid and correct. But as we know, in field of Computer Science, whenever there are multiple options to achieve something, one always outperforms the other. Same is the situation here as well.

Turns out, there is a severe problem with skip function. I have tried to jot it down in [this blog post](/blogs/mongodb-cursor-skip-is-slow). Because of which second approach has advantage over first. But that is not it; I wrote a simple [python code](https://github.com/arpitbbhayani/mongo-pagination-benchmark) to benchmark the two approaches for various combinations and it turns out `skip` performs better in some case. The results are compiled into [this blog post](/blogs/benchmark-and-compare-pagination-approach-in-mongodb).

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs/making-http-requests-using-netcat

Understanding HTTP protocol using netcat

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

- [Knowledge Base](/knowledge-base)
- [System Design](/knowledge-base/system-design)
- [Protocols](/knowledge-base/system-design)

- [KB](/knowledge-base)
- [Syst...](/knowledge-base/system-design)
- [Protocols](/knowledge-base/system-design)

# Understanding HTTP protocol using netcat

![](https://edge.arpitbhayani.me/img/arpit-5.jpg)

[Arpit Bhayani](/)

curious, tinkerer, and explorer

Majority of the traffic over the internet is HTTP Traffic. There is a HTTP Client which wants some data from HTTP Server, so it creates a HTTP Request Message in the protocol understandable by the server and sends it. Server reads the message, understands it, acts accordingly and replies back with HTTP Response.

This complete process is abstracted by the tools like [curl](https://curl.haxx.se/), requests libraries and utilities like [Postman](https://www.getpostman.com/). Instead of using these tools and utilities, we shall go by the hard way and see HTTP messages in action.

## The Webserver

For experimentation purpose let’s create a very basic webserver in [Python Flask framework](flask.pocoo.org) that exposes a trivial Hello World end point.

### Python webserver script

```
from flask import Flask
app = Flask(__name__)

@app.route('/hello')
def hello():
    return "Hello, World!"

app.run(port=3000)
```

### Installing requirements

```
pip install flask
```

### Start the webserver

```
python hello.py
```

The server listens on port _3000_ . If you hit from the browser <http://localhost:3000/hello>, you should see _Hello, World!_ rendered.

## The HTTP Request Message

A HTTP Client talks to HTTP Server via a common protocol that is understandable by the two parties. A sample HTTP request message looks something like

```
GET /hello.html HTTP/1.1
User-Agent: Mozilla/4.0 (compatible; MSIE5.01; Windows NT)
Host: www.sample-server.com
Accept-Language: en-us
Accept-Encoding: gzip, deflate
Connection: Keep-Alive
```

To understand more about HTTP Request messages, see references at the end of this article.

The HTTP Communication happens over a TCP Connection. So we create a TCP connection with the server and try to get response from it. To get a TCP connection I will use _netcat_.

## Netcat

_netcat_ is the utility that is used for just about anything under the sun involving TCP or UDP. It can open TCP connections, send UDP packets, listen on arbitrary TCP and UDP ports, do port scanning, and deal with both IPv4 and IPv6.

The webserver that was created above is listening on port _3000_ . Lets create a TCP Connection and connect to it using _netcat_.

```
netcat localhost 3000
```

The command along with creating a TCP connection, will also open a STDIN. Anything passed in that input stream will reach the server via the connection. Lets see what happens when we provide _This is a sample_ as input.

![bad-request](https://user-images.githubusercontent.com/4745789/63222752-65156900-c1c9-11e9-90ec-ed06362d5d83.jpg)

The input message given is not a valid HTTP message hence server responded with a status code of _400_ which is for [Bad Request](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html). And if you closely observe the server logs on flask application, you will see an entry of our last request.

Since the server is a HTTP Server, so it understands HTTP request. Let’s create one to hit our exposed API endpoint _/hello_ .

The HTTP request message for this request looks something like this

```
GET /hello HTTP/1.1
```

And you should see output like this

![get-request](https://user-images.githubusercontent.com/4745789/63222756-78283900-c1c9-11e9-943a-60513ddbde86.jpg)

The HTTP Server understands the message sent from the client and it responded back as directed by the source code.

## Complex Requests and HTTP Request Messages

### GET method with query params and headers

Following method exposes an endpoint which accepts a [query parameter](https://en.wikipedia.org/wiki/Query_string) named _name_, and returns a response with _name_ in it.

```
from flask import request

@app.route('/user')
def get_user():
    name = request.args.get('name')
    return "Requested for name = %s" % name
```

#### HTTP Request Message

Provide the HTTP request message below when STDIN opens up after you execute _netcat_ command and connect with the server.

```
GET /user?name=arpit HTTP/1.1
```

#### Output

![get-request-with-query-params](https://user-images.githubusercontent.com/4745789/63222764-87a78200-c1c9-11e9-83bc-edbd6cbfb32c.jpg)

### Basic POST Method example

Following method accepts form data via HTTP POST method and returns a dummy response with _username_ and _password_ in it.

```
from flask import request

@app.route('/login', methods=['POST'])
def login():
    username = request.form.get('username')
    password = request.form.get('password')
    return "Login successful for %s:%s" % (username, password)
```

#### HTTP Request Message

Provide the HTTP request message below when STDIN opens up after you execute _netcat_ command and connect with the server.

```
POST /login HTTP/1.1
Content-Type: application/x-www-form-urlencoded
Content-Length: 32

username=arpit&password=welcome
```

#### Output

![post-request-with-form-data](https://user-images.githubusercontent.com/4745789/63222769-9c841580-c1c9-11e9-8593-7289b2a40a20.jpg)

### POST Method with JSON Request Body

Following method accepts JSON data that contains a field _id_ with integer value via HTTP POST method and returns a dummy response with _id_ in it.

```
from flask import request

@app.route('/save', methods=['POST'])
def save_user():
    user_data = request.json
    return 'Saving user with id = %d' % (user_data.get('id'))
```

#### HTTP Request Message

Provide the HTTP request message below when STDIN opens up after you execute _netcat_ command and connect with the server.

```
POST /save HTTP/1.1
Content-Type: application/json
Content-Length: 30

{"id": 1092, "name": "Arpit"}
```

#### Output

![post-request-with-json-data](https://user-images.githubusercontent.com/4745789/63222775-ad348b80-c1c9-11e9-91ee-07933e37604d.jpg)

## Conclusion

The hard way to hit REST endpoints was not hard at all ;-) Stay curious and dive deep.

## References:

1. [HTTP/1.1: HTTP Message](https://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html)
2. [HTTP Requests - Tutorialspoint](http://www.tutorialspoint.com/http/http_requests.htm)
3. [The TCP/IP Guide - HTTP Request Message Format](http://www.tcpipguide.com/free/t_HTTPRequestMessageFormat.htm)
4. [HTTP Status Codes](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html)
5. [Netcat man page](http://linux.die.net/man/1/nc)
6. [HTTP Methods](https://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Creator of [DiceDB](https://github.com/dicedb/dice), ex-Google Dataproc, ex-Amazon Fast Data, ex-Director of Engg. SRE and
Data Engineering at Unacademy. I spark engineering curiosity through my
no-fluff engineering videos on
[YouTube](https://www.youtube.com/c/ArpitBhayani)
and my courses

- [System Design Masterclass](/masterclass)
- [System Design for Beginners](/system-design-for-beginners)
- [Redis Internals](/redis-internals)

[YouTube](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/blogs

Blogs | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

## Blogs

Every week, I documente and articulate my acquired knowledge and
personal perspectives on topics that captivate my interest. Here are
all blogs I wrote to date.

- Oct 19, 2024
  :

  [3P framework to know when it is the right time to make a switch](/blogs/3p)

- Oct 02, 2024
  :

  [Leverage the equilibrium](/blogs/leverage-the-equilibrium)

- Sep 10, 2024
  :

  [Not everything needs to be dumbed down](/blogs/not-everything-needs-to-be-dumbed-down)

- Aug 26, 2024
  :

  [The best resource is mythical](/blogs/best-resource-is-mythical)

- Aug 19, 2024
  :

  [It's not about what you know, but about how you think](/blogs/retention-vs-understanding)

- Aug 13, 2024
  :

  [Know a lot, a lot](/blogs/know-a-lot)

- Aug 12, 2024
  :

  [Going out of syllabus is okay](/blogs/out-of-syllabus)

- Aug 06, 2024
  :

  [Always negotiate the offer](/blogs/negotiate-the-offer)

- Jul 31, 2024
  :

  [Never bad-mouth your ex-employer](/blogs/never-bad-mouth-your-ex-exployer)

- Jul 29, 2024
  :

  [Prove you are a culture-fit](/blogs/culture-fit)

- Jul 23, 2024
  :

  [Quantify your resume, through and through](/blogs/quantification-in-resume)

- Jul 15, 2024
  :

  [Hiring is subjective and unfair](/blogs/hiring-is-unfair)

- Jul 08, 2024
  :

  [Questions you should ask to your interviewer](/blogs/questions-for-interviewers)

- Jul 03, 2024
  :

  [Be curious, not judgemental](/blogs/be-curious-not-judgemental)

- Jul 01, 2024
  :

  [Collaborate and communicate better](/blogs/collaboration-communication)

- Jun 20, 2024
  :

  [Once you are out of vicious interview cycle](/blogs/out-of-vicious-interview-cycle)

- Jun 17, 2024
  :

  [Pitch projects, not ideas](/blogs/pitch-projects-not-ideas)

- Jun 10, 2024
  :

  [Read design docs, even if they seem unrelated](/blogs/read-design-docs)

- May 29, 2024
  :

  [Be excited when production goes down](/blogs/read-rca-docs)

- May 27, 2024
  :

  [Being a generalist does not mean being mediocre](/blogs/generalist-not-mediocre)

- May 21, 2024
  :

  [Summaries create an illusion of mastery](/blogs/do-not-rely-on-summaries)

- May 13, 2024
  :

  [You are not hired to just ship features](/blogs/do-not-just-ship-features)

- May 06, 2024
  :

  [Structure your System Design interviews](/blogs/structure-your-design-interviews)

- Mar 27, 2024
  :

  [Never let your company inflate your title to retain you](/blogs/title-inflation)

- Mar 11, 2024
  :

  [Find your own project](/blogs/find-your-own-project)

- Jan 15, 2024
  :

  [Six pointers to crack coding and design interviews](/blogs/six-pointers-to-crack-coding-and-design-interviews)

- Nov 16, 2023
  :

  [Keep yourself unblocked](/blogs/keep-yourself-unblocked)

- Mar 07, 2022
  :

  [Genetic algorithm to solve the Knapsack Problem](/blogs/genetic-knapsack)

- Feb 21, 2022
  :

  [Pseudorandom number generator uusing LFSR](/blogs/pseudorandom-number-generation-lfsr)

- Feb 07, 2022
  :

  [How indexes work on partitioned and sharded data?](/blogs/how-indexes-work-on-partitioned-and-sharded-data)

- Jan 31, 2022
  :

  [Data partitioning strategies for distributed databases](/blogs/some-data-partitioning-strategies-for-distributed-data-stores)

- Jan 24, 2022
  :

  [What is Data Partitioning and why it matters at scale](/blogs/data-partitioning)

- Jan 16, 2022
  :

  [What is leaderless replication and how it works?](/blogs/leaderless-replication)

- Jan 03, 2022
  :

  [How to resolve conflicts in multi-master setup](/blogs/conflict-resolution)

- Nov 28, 2021
  :

  [How to detect conflicts in a multi-master setup](/blogs/conflict-detection)

- Nov 03, 2021
  :

  [What is multi-master replication and why do we need it?](/blogs/multi-master-replication)

- Oct 03, 2021
  :

  [What are monotonic reads, and why do we need them?](/blogs/monotonic-reads)

- Sep 22, 2021
  :

  [Read-your-write consistency](/blogs/read-your-write-consistency)

- Sep 07, 2021
  :

  [How to handle database outages?](/blogs/handling-outages-master-replica)

- Aug 23, 2021
  :

  [What happens when we add a new replica?](/blogs/new-replica)

- Aug 15, 2021
  :

  [Understanding replication formats in a master-replica setup](/blogs/replication-formats)

- Aug 10, 2021
  :

  [Two ways to replicate data across database cluster](/blogs/replication-strategies)

- Aug 07, 2021
  :

  [What is a master-replica setup and why it matters?](/blogs/master-replica-replication)

- Jul 19, 2021
  :

  [Understanding Durability in ACID](/blogs/durability)

- Jul 07, 2021
  :

  [Understanding Isolation in ACID](/blogs/isolation)

- Jul 02, 2021
  :

  [Understanding Atomicity in ACID](/blogs/atomicity)

- Jul 02, 2021
  :

  [Understanding Consistency in ACID](/blogs/consistency)

- Jun 22, 2021
  :

  [Common architectural patterns in distributed systems](/blogs/architectures-in-distributed-systems)

- Jun 17, 2021
  :

  [Assumptions people make while designing distributed systems](/blogs/mistaken-beliefs-of-distributed-systems)

- Jun 09, 2021
  :

  [Fork Bomb](/blogs/fork-bomb)

- Apr 28, 2021
  :

  [How Python evaluates chained comparison operators](/blogs/chained-operators-python)

- Apr 19, 2021
  :

  [How to design a taxonomy on a Relational DB](/blogs/taxonomy-on-sql)

- Apr 01, 2021
  :

  [Changing the Python's walrus operator - a deep dive into CPython](/blogs/the-weird-walrus)

- Feb 15, 2021
  :

  [Fully Persistent Arrays - a datastructure that let's us time travel](/blogs/fully-persistent-arrays)

- Feb 07, 2021
  :

  [Introduction to Persistent Data Structures](/blogs/persistent-data-structures-introduction)

- Jan 10, 2021
  :

  [How python optimises the runtime using constant folding](/blogs/constant-folding-python)

- Dec 20, 2020
  :

  [String interning in Python - a deep dive into CPython](/blogs/string-interning-python)

- Dec 13, 2020
  :

  [Building a simple recursion tree visualizer for Python](/blogs/recursion-visualizer-python)

- Dec 06, 2020
  :

  [Flajolet Martin algorithm for approximate counting](/blogs/flajolet-martin)

- Nov 29, 2020
  :

  [2Q-cache algorithm for disk-backed databases](/blogs/2q-cache)

- Nov 22, 2020
  :

  [Israeli Queues](/blogs/israeli-queues)

- Nov 16, 2020
  :

  [How video games programatically generate terrains?](/blogs/1d-terrain)

- Nov 08, 2020
  :

  [Quantifying set similarity using Jaccard similarity coefficient and MinHash](/blogs/jaccard-minhash)

- Nov 01, 2020
  :

  [Smooth out time-series data using Kurtosis](/blogs/ts-smoothing)

- Aug 23, 2020
  :

  [Constant time implementation of the LFU cache eviction algorithm](/blogs/lfu)

- Aug 02, 2020
  :

  [Morris's algorithm for approximate counting](/blogs/morris-counter)

- Jul 26, 2020
  :

  [Slowsort - the slowest sorting algorithm, ever](/blogs/slowsort)

- Jul 19, 2020
  :

  [Bitcask - A Log-Structured fast KV store](/blogs/bitcask)

- Jul 12, 2020
  :

  [Phi φ Accrual - a realistic failure detection algorithm](/blogs/phi-accrual)

- Jul 05, 2020
  :

  [Traits of a 10x engineer](/blogs/10x-engineer)

- Jul 05, 2020
  :

  [Deciphering Repeated-key XOR Ciphertext](/blogs/decipher-repeated-key-xor)

- Jun 21, 2020
  :

  [Deciphering Single-byte XOR Ciphertext](/blogs/decipher-single-xor)

- Jun 14, 2020
  :

  [Making Python integers iterable - a deep dive into CPython](/blogs/python-iterable-integers)

- Jun 07, 2020
  :

  [Stucture composition in C - implementing inheritence in C](/blogs/inheritance-c)

- May 31, 2020
  :

  [The RUM Conjecture](/blogs/rum)

- May 24, 2020
  :

  [Consistent Hashing - explanation and implementation](/blogs/consistent-hashing)

- May 17, 2020
  :

  [Python caches smaller integers - a deep dive into CPython](/blogs/python-caches-integers)

- May 10, 2020
  :

  [Fractional Cascading - a way to speed up binary searches](/blogs/fractional-cascading)

- May 03, 2020
  :

  [What is copy-on-write and why it matters?](/blogs/copy-on-write)

- Apr 26, 2020
  :

  [A midpoint insertion caching strategy to avoid cache bust](/blogs/midpoint-insertion-caching-strategy)

- Apr 19, 2020
  :

  [Building Finite State Machines with Python Coroutines](/blogs/fsm-python)

- Apr 12, 2020
  :

  [Bayesian average to compute average rating, properly](/blogs/bayesian-average)

- Apr 05, 2020
  :

  [Designing and implementing a Sliding Window based Rate Limiter](/blogs/sliding-window-ratelimiter)

- Mar 10, 2020
  :

  [The math behind Inverse Document Frequency - the IDF of TFIDF](/blogs/idf)

- Feb 28, 2020
  :

  [Eight Rituals to be a Better Programmer](/blogs/better-programmer)

- Feb 21, 2020
  :

  [Personalize your Python Prompt](/blogs/python-prompts)

- Feb 14, 2020
  :

  [Pseudorandom Number Generation using Cellular Automata - Rule 30](/blogs/rule-30-cellular-automata)

- Feb 07, 2020
  :

  [Implementing functional overloading in Python](/blogs/function-overloading)

- Jan 31, 2020
  :

  [Isolation Forest Algorithm for Anomaly Detection](/blogs/isolation-forest)

- Jan 17, 2020
  :

  [Image Steganography - the art of hiding data in images](/blogs/image-steganography)

- Jan 10, 2020
  :

  [How Python supports integers of infinite length - a deep dive into CPython](/blogs/long-integers-python)

- Jan 03, 2020
  :

  [I changed my python for fun!](/blogs/i-changed-my-python)

- Mar 24, 2017
  :

  [I benchmarked different pagination strategies in MongoDB](/blogs/benchmark-and-compare-pagination-approach-in-mongodb)

- Mar 23, 2017
  :

  [Why MongoDB's cursor.skip() is slow?](/blogs/mongodb-cursor-skip-is-slow)

- Mar 22, 2017
  :

  [How to paginate faster with consistent performance?](/blogs/fast-and-efficient-pagination-in-mongodb)

- Apr 12, 2016
  :

  [Understanding HTTP protocol using netcat](/blogs/making-http-requests-using-netcat)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/knowledge-base

Knowledge Base | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

# My Knowledge Base

This is a collection of my journey of active learning from blogs,
books, and papers organized by topics. Here you'll find my notes,
videos, and write-ups. Hope it helps.

## [Advanced algorithms and Musings](/knowledge-base/algorithms-and-explorations)

My learnings and notes from exploring advanced algorithms and building prototypes based on core CS concepts and everything that amused me.

[Explore →](/knowledge-base/algorithms-and-explorations)

## [Career Growth](/knowledge-base/career-growth)

My learnings and notes from 10+ years of experience on how to become a better engineer and accelerate the career growth.

[Explore →](/knowledge-base/career-growth)

## [Cloud Systems and Optimizations](/knowledge-base/cloud-systems-and-optimizations)

My learnings and notes from exploring cloud systems.

[Explore →](/knowledge-base/cloud-systems-and-optimizations)

## [Database Engineering](/knowledge-base/database-engineering)

My learnings and notes around databases covering the internals, concepts, and even algorithms, across single node and even distributed datastores.

[Explore →](/knowledge-base/database-engineering)

## [Language Internals](/knowledge-base/language-internals)

My learnings and notes from exploring CPython and Golang along with some foundational topics in language construction.

[Explore →](/knowledge-base/language-internals)

## [System Design](/knowledge-base/system-design)

My learnings and notes from designing systems and studying from books, blogs, and papers for 10+ years across various industries.

[Explore →](/knowledge-base/system-design)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/bookshelf

Bookshelf | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

## Bookshelf

I read a few books every month around various topics and here are
some of them that I recommend.

### Personal Productivity and Growth

---

[![](https://m.media-amazon.com/images/I/71xuOyQIvFL._SL1500_.jpg)](https://amzn.to/40RLRsc)

[![](https://m.media-amazon.com/images/I/71+aukusbGL._SL1500_.jpg)](https://amzn.to/3IkdG49)

[![](https://m.media-amazon.com/images/I/71Igxu7V6mL._SL1500_.jpg)](https://amzn.to/3Ktx1lW)

[![](https://m.media-amazon.com/images/I/71k7V0QvcdL._SL1500_.jpg)](https://amzn.to/3YU4dYr)

[![](https://m.media-amazon.com/images/I/61TkOFwquPL._SL1500_.jpg)](https://amzn.to/3kcuTVi)

[![](https://m.media-amazon.com/images/I/71+LSoiRfGL._SL1500_.jpg)](https://amzn.to/3kdjp3T)

[![](https://m.media-amazon.com/images/I/81Ls+SBCLiL._SL1500_.jpg)](https://amzn.to/3khnxjh)

### Programming

---

[![](https://m.media-amazon.com/images/I/51yaxPX4BFL.jpg)](https://amzn.to/3Xku1fW)

[![](https://m.media-amazon.com/images/I/61biXaIiAGL._SL1400_.jpg)](https://amzn.to/3EMGGRn)

[![](https://m.media-amazon.com/images/I/61u0ZgHIjdL._SL1500_.jpg)](https://amzn.to/3XgsRC6)

[![](https://m.media-amazon.com/images/I/71Oc54-NHKL._SL1360_.jpg)](https://amzn.to/47kXDyy)

[![](https://m.media-amazon.com/images/I/61fIrkH4jgL._SL1281_.jpg)](https://amzn.to/3tJkv8j)

[![](https://m.media-amazon.com/images/I/71S1AZ8YqfL._SL1317_.jpg)](https://amzn.to/43SGb1y)

[![](https://m.media-amazon.com/images/I/41EYLUDaF8L.jpg)](https://amzn.to/3IRXwjT)

[![](https://m.media-amazon.com/images/I/71jkPWDXhpL._SL1360_.jpg)](https://amzn.to/43XNK7d)

### Hash Table Internals

---

[![](https://m.media-amazon.com/images/I/61Qr6xXSYJL._SL1254_.jpg)](https://amzn.to/3ErD3Ps)

### Backend System Design

---

[![](https://m.media-amazon.com/images/I/71YL95jVDpL._SL1331_.jpg)](https://amzn.to/3gq2WXR)

[![](https://m.media-amazon.com/images/I/71QrFo8N3tL._SL1317_.jpg)](https://amzn.to/3UQQ5NF)

### Database Engineering

---

[![](https://m.media-amazon.com/images/I/61annl+fY4L.jpg)](https://amzn.to/3AuPSay)

[![](https://m.media-amazon.com/images/I/71uJFctCDFL._SL1317_.jpg)](https://amzn.to/3V36Btw)

### Concurrency

---

[![](https://m.media-amazon.com/images/I/71ho2R8EEKL._SL1400_.jpg)](https://amzn.to/3lWBXFN)

### Information Retrieval

---

[![](https://m.media-amazon.com/images/I/71ifPvg4C7L._SL1500_.jpg)](https://amzn.to/3AvL2dc)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/papershelf

Papershelf | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

## Papershelf

I read a few papers every week around various topics that interests
me and here are some of them that I found amusing categorized by
topics. Here's the process I follow to [read, understand, and remember a paper](https://youtu.be/V5-KxHdmbDo).

- Sep 15, 2024
  :

  [On-demand Container Loading in AWS Lambda](/blogs/on-demand-container-loading-in-aws-lambda)

- Sep 03, 2024
  :

  [SQL Has Problems. We Can Fix Them: Pipe Syntax In SQL](/blogs/sql-has-problems-we-can-fix-them-pipe-syntax-in-sql)

- Aug 28, 2024
  :

  [NanoLog: A Nanosecond Scale Logging System](/blogs/nanolog-a-nanosecond-scale-logging-system)

- Aug 21, 2024
  :

  [WTF: The Who to Follow Service at Twitter](/blogs/wtf-the-who-to-follow-service-at-twitter)

I have been reading papers for a very long time and the notes I took
were not public. I am in the process of making the notes public and this
might take some time. So, the papers you see below this notification are
links and the ones you see above are my notes and explanations for each
one of them. Slowly, I will be pushing notes for all the papers I've
read.

- [The Chubby lock service for loosely-coupled distributed systems](https://drive.google.com/file/d/1o5Rto3ex5iDUONNPJf8Wvp43E7wWpdsR/view?usp=sharing)
- [Bigtable: A Distributed Storage System for Structured Data](https://drive.google.com/file/d/1o7HrswgtMRYTuDOFsEiAnhpRaYhWxpRt/view?usp=drive_link)
- [On-the-fly Sharing for Streamed Aggregation](https://drive.google.com/file/d/156BTit4ZbFdq526_2sdvLBy6WIHV5tmx/view?usp=sharing)
- [MapReduce: Simplified Data Processing on Large Clusters](https://drive.google.com/file/d/1fcJP_WE8j0L-QdxQhBSEzkyel8szFzbH/view?usp=sharing)
- [How to break software](https://drive.google.com/file/d/1zr4qYypLoaW521NX0WIlV_TXKpa8ZuDG/view?usp=sharing)
- [Web Search for a Planet: The Google Cluster Architecture](https://drive.google.com/file/d/1Cs1-ENNZFDcxLh9MB_p7j-IUCboIuwMQ/view?usp=sharing)
- [Amazon DynamoDB: A scalable, predictably performant, and fully managed NoSQL database service](https://drive.google.com/file/d/1ztxrZTh3Gn9WWoqprDCU80MjOoXnL6Es/view?usp=sharing)
- [Amazon Redshift re-invented](https://drive.google.com/file/d/16Pb3BSWkmNJx0Dato3NVt36nDDKbkkRy/view?usp=sharing)
- [Scalable blocking for very large databases](https://drive.google.com/file/d/1RMELaWQ5sPbomHeq5bwwYH79xI6PBiTY/view?usp=sharing)
- [Firecracker: Lightweight virtualization for serverless applications](https://drive.google.com/file/d/1u-eQlDjrVn7lzOmX-Cv2si_Wj16H0Z-h/view?usp=drive_link)
- [Millions of tiny databases](https://drive.google.com/file/d/16K17SnbgAcFM7j7_qsJJwMX8dYBv9t-n/view?usp=drive_link)
- [Amazon Redshift and the Case for Simpler Data Warehouses](https://drive.google.com/file/d/1LwAAnoE2B17AkZ0hU5EBrjwUTcce7qQG/view?usp=drive_link)
- [Amazon Aurora: Design considerations for high throughput cloud-native relational databases](https://drive.google.com/file/d/1Aqp80fFRz6A2KiIxoVgB0gYZecoxgZUW/view?usp=drive_link)
- [Near-duplicate Question Detection](https://drive.google.com/file/d/1MuqS6WO9wFjOFwtQnp590Tz28o_y6Aco/view?usp=drive_link)
- [Striking the right chord: A comprehensive approach to Amazon Music search spell correction](https://drive.google.com/file/d/1SeBP_vLMohz7qlQsOX1pS9hB-8CAGstm/view?usp=drive_link)
- [Stage: Query Execution Time Prediction in Amazon Redshift](https://drive.google.com/file/d/1ZkernfuCbAMQOfbfOvK8Gjb9-JJnQOVg/view?usp=sharing)
- [A flexible large-scale similar product identification system in e-commerce](https://drive.google.com/file/d/16fqZFuri2WWKM6bfsy-HpWe0hWkdWZdV/view?usp=drive_link)
- [Intelligent Scaling in Amazon Redshift](https://drive.google.com/file/d/1E7cb5Ttj21JvI3svJC0QnkhncycX2PS-/view?usp=drive_link)
- [Serverless Runtime / Database Co-Design With Asynchronous I/O](https://drive.google.com/file/d/1nuURga5TdctAorXCRPOraIgiLwT0n6S-/view?usp=sharing)
- [Predicate Caching: Query-Driven Secondary Indexing for Cloud DataWarehouses](https://drive.google.com/file/d/1K-tWD8-SnQbenEajEHKfPOOL7va-3viA/view?usp=drive_link)
- [Query Attribute Recommendation at Amazon Search](https://drive.google.com/file/d/1ItSnpBjjhIyamZFt-82zriHt4Qvo_DBy/view?usp=sharing)
- [ROSE: Robust caches for Amazon product search](https://drive.google.com/file/d/13YwkeCc1XwnRVDVkcuE7MKt2r7aqld24/view?usp=drive_link)
- [The story of AWS Glue](https://drive.google.com/file/d/1CxK5bTV8ZQgNFe3TI582W9N8PyLa5nK3/view?usp=drive_link)
- [DecLog: Decentralized Logging in Non-Volatile Memory for Time Series Database Systems](https://drive.google.com/file/d/1CJwsj-dRcoc_wtkterUQ9l4xtYPOr3On/view?usp=drive_link)
- [SILK: Preventing Latency Spikes in LSM Key-Value Stores](https://drive.google.com/file/d/1RCBW70TNXqGowl4I7cPjRJjzpfZqI0rs/view?usp=drive_link)
- [Umbra: A Disk-Based System with In-Memory Performance](https://drive.google.com/file/d/1V4sahz-i4Z9wFBsZcj3mHxNAoSXaX53x/view?usp=drive_link)
- [Amazon MemoryDB: A fast and durable memory-first cloud database](https://drive.google.com/file/d/1W1nQ8wiT2upuOHXD9d1PZwALJSZx0MFB/view?usp=drive_link)
- [Take Out the TraChe: Maximizing (Tra)nsactional Ca(che) Hit Rate](https://drive.google.com/file/d/11BQhr3FtKBHrKFvmtNcmOsOnn9NGQ0li/view?usp=drive_link)
- [Distributed Transactions at Scale in Amazon DynamoDB](https://drive.google.com/file/d/1Yg2R-wN7KKugx-R4yc8c080XsXtBB0JT/view?usp=drive_link)
- [TiDB: A Raft-based HTAP Database](https://drive.google.com/file/d/114Xn8vqP3jrQKNS-9-XdPeLaHMKBO4UV/view?usp=drive_link)
- [Kora: A Cloud-Native Event Streaming Platform For Kafka](https://drive.google.com/file/d/1DwH_8_oqgkv8jNkoITqIYPx2-LZnspBx/view?usp=drive_link)
- [PolarDB-SCC: A Cloud-Native Database](https://drive.google.com/file/d/1HHEsBWa2MZnJ290ZCAEQK6lNc_X4Vp-2/view?usp=drive_link)
- [Epoxy: ACID Transactions Across Diverse Data Stores](https://drive.google.com/file/d/1-fp25FGGIBRqR1MTwG9hUbTm-CMIy9ad/view?usp=drive_link)
- [Scalable OLTP in the Cloud: What’s the BIG DEAL?](https://drive.google.com/file/d/1WOXxSBhoDyWe2ZpmOVBD4tcJiFuJTpHq/view?usp=drive_link)
- [BonsaiKV: Key-Value Store with Tiered and Heterogeneous Memory System](https://drive.google.com/file/d/15ow-jHUbzi9Hvue8VwYebpcO4wrhbZRW/view?usp=drive_link)
- [Automated Unit Test Improvement using Large Language Models at Meta](https://drive.google.com/file/d/1ckfwOOPfg3xTsw2CVoSC8nmlq6LfZoEm/view?usp=drive_link)
- [Designing Access Methods: The RUM Conjecture](https://drive.google.com/file/d/1EeZ4vCy8m0Mb06Z-uc-zbtZfQ30ERF6z/view?usp=drive_link)
- [Probabilistic Counting Algorithms for Database Applications - Flajolet-Martin](https://drive.google.com/file/d/147IAuDTmHuSS74xd27_HD3NKm14euOPT/view?usp=drive_link)
- [Cache-Efficient Top-k Aggregation over High Cardinality Large Datasets](https://drive.google.com/file/d/1fIasc3HCalvsu-CTJp0fyGihiz5xD_UV/view?usp=drive_link)
- [SIEVE - an Efficient Turn-Key Eviction Algorithm for Web Caches](https://drive.google.com/file/d/1CMJMCSXW9MrEe5s9i6EQYoTlmzh-fgzk/view?usp=drive_link)
- [Vector Database: Storage and Retrieval Technique, Challenge](https://drive.google.com/file/d/1HamJjegLEvLSebnEN-uKLWhQTiZ-UJdd/view?usp=drive_link)
- [Panda: Performance Debugging for Databases using LLM Agents](https://drive.google.com/file/d/16zfCBxo-xqhMhrq48dnQmCov-FAn3Lkk/view?usp=drive_link)
- [Magnet: A scalable and performant shuffle architecture for Apache Spark](https://drive.google.com/file/d/1xjKIl7SC8tqJeEdN7wGxg956QHRogo2W/view?usp=sharing)
- [ZIP: Lazy Imputation during Query Processing](https://drive.google.com/file/d/1yuMg3x4kgZ6eHVoWMgBBttH2C6wsJEa-/view?usp=sharing)
- [Anycast as a Load Balancing feature](https://drive.google.com/file/d/1209iTFzMJQDqkPNCneLSp56RyXZqfxoa/view?usp=share_link)
- [The Impact of Thread-Per-Core Architecture on Application Tail Latency](https://drive.google.com/file/d/1EJHkuxRJMxK_yFQpUftKW8LaFr2SQDSC/view?usp=sharing)
- [TreeLine - An Update-In-Place Key-Value Store for Modern Storage](https://drive.google.com/file/d/1MLkQIO9xqSMc6jv9lbqz32lWBRoTLjT_/view?usp=drive_link)
- [Manu: A Cloud Native Vector Database Management System](https://drive.google.com/file/d/1gLl_gSzt6cjnvdPpX40kdaOCiTyMNt4z/view?usp=drive_link)
- [Parallelism-Optimizing Data Placement for Faster Data-Parallel Computations](https://drive.google.com/file/d/1rO1FoyU2F0JrNmf5JOZfuPiYmpanwy6b/view?usp=drive_link)
- [Faster sorting algorithms discovered using deep reinforcement learning](https://drive.google.com/file/d/16n47YlDxbCXG5qZiu258eK-3y8_Aa9Ru/view?usp=sharing)
- [A Relational Model of Data for Large Shared Data Banks](https://drive.google.com/file/d/1_27sKT2kzGCuAL9hEldGO26p_qOhvhS6/view?usp=share_link)
- [Improving Language Understanding by Generative Pre-Training](https://drive.google.com/file/d/1yDyKWk4RhC40jbY2evevDnuuxKpIkAxi/view?usp=share_link)
- [Language Models are Few-Shot Learners](https://drive.google.com/file/d/1ICUPRGbARL1L_JgMrKfrYQ6xKzh46pjT/view?usp=share_link)
- [Attention Is All You Need](https://drive.google.com/file/d/1NI4fHNYauNvH3ynRuhi11Ey5s1-BOpmE/view?usp=share_link)
- [Amazon DynamoDB A Scalable, Predictably Performant, and Fully Managed NoSQL Database Service](https://drive.google.com/file/d/1nA7iL9b_WLlQKhuzAV9RlgsrSrWKDsG4/view?usp=share_link)
- [Dynamo Amazon’s Highly Available Key-value Store](https://drive.google.com/file/d/1dIX26Vyiva_qxO_5syfMa85ynFpVPIDO/view?usp=share_link)
- [The Google File System](https://drive.google.com/file/d/1S_hYRcjdo7aR0ShXuIuK5ePQm2U0FEs2/view?usp=share_link)
- [Neural Machine Translation of Rare Words with Subword Units](https://drive.google.com/file/d/1XTvz9HwZ1mlh7D7tfxlILcMgv7IkWkKp/view?usp=share_link)
- [The Bloom Paradox: When not to Use a Bloom Filter](https://drive.google.com/file/d/1luxcdZBCxo-ty9sgmEQ1rDs5AF4334-3/view)
- [The Deletable Bloom Filter](https://drive.google.com/file/d/1f-LFOroH5WihpfSENXCQJGyM4EWOAgc8/view?usp=share_link)
- [Zanzibar - Google's Consistent, Global Authorization System](https://drive.google.com/file/d/1Z3Uzhm-9dhG1DMhyAwDxMhy989N1BIXG/view?usp=share_link)
- [Space-Time Trade-offs in Hash Coding with Allowable Errors](https://drive.google.com/file/d/1tWyRo5ofJgMmZpXrSMO6FjfGI0_NFS54/view?usp=share_link)
- [Gorilla - A Fast, Scalable, In Memory Time Series Database](https://drive.google.com/file/d/13jFQkD2OmydymjPFLnsvsQUwMwhmynIh/view?usp=sharing)
- [Understanding BitTorrent - An Experimental Perspective](https://drive.google.com/file/d/17lvNCfgI2xwMA65VDBcKSPhq0iJqHXVU/view?usp=sharing)
- [Exploiting BitTorrent For Fun (But Not Profit)](https://drive.google.com/file/d/13qLIQFaytcTUD0pQEhP_7eeqN_Ryv3bh/view?usp=sharing)
- [Rarest First and Choke Algorithms Are Enough](https://drive.google.com/file/d/1GqnGoiQbrbxdn1oVPLVd3z8q-XLaoaIh/view?usp=sharing)
- [Implementation of a BitTorrent client - B. Sc. Thesis](https://drive.google.com/file/d/1mCgITghlVle3rFmJzd3Um7B5Wu_k3shb/view?usp=sharing)
- [Kademlia - a Peer-to-peer Information System based on XOR Metric](https://drive.google.com/file/d/1EREYP8U1jkxsbsLJvjKhSsz7Scc4xo2c/view?usp=sharing)
- [Peer-to-peer networking with BitTorrent](https://drive.google.com/file/d/1VS37P6J3v_trRCHzCOWtS-9lGGcaVq4a/view?usp=sharing)
- [Free Riding in BitTorrent is Cheap](https://drive.google.com/file/d/1JEu085WKpy0I-X_enknDs08TH6bnVc0T/view?usp=sharing)
- [Go To Statement Considered Harmful](https://drive.google.com/file/d/1qMOCSfTgyPKF6HFS1QFrek_A2pRMmwtK/view?usp=sharing)
- [Bitcoin - A Peer-to-Peer Electronic Cash System](https://drive.google.com/file/d/1R0B4ZD67-W4fPmd0EphOXxqWvujZAzHt/view?usp=sharing)
- [Understanding Inverse Document Frequency On theoretical arguments for IDF](https://drive.google.com/file/d/11cw9-riCQ5HJ0R2EOHxRijXfV1r76E7Y/view?usp=sharing)
- [MyRocks: LSM-Tree Database Storage Engine Serving Facebook's Social Graph](https://drive.google.com/file/d/1lOJcjJ-r8IaujMNx8btUy-VSKe4R5cnY/view?usp=sharing)
- [Isolation Forest](https://drive.google.com/file/d/1yTEFQaEizA-4oPuC4I1NI7XDbu1WXm1W/view?usp=sharing)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/masterclass

System Design Masterclass | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

# The System Design Masterclass

A no-fluff masterclass that reignites the passion in engineering and
helps **SDE-2, SDE-3, and above** become great at designing and implementing
_scalable_, _fault-tolerant_, and
_highly available_ systems.

Admissions for the
February 2025

Cohort are open and only
41 seats

left.

If you are a beginner in System Design then check out my
[System Design for Beginner](/sys-design) course.

## Key Details

- Every single system will be designed in a highly practical manner,
  something you can ship to production
- We will cover key implementation details and prototype for systems
  we discuss
- Sessions are not monologues but are filled with brainstorming
  allowing us to naturally evolve our solution
- We discuss all possible trade-offs helping you build the right
  reasoning behind opting for a particular approach
- Sessions go on for a minimum 3 hours and do not end until every
  single question is answered
- You will have lifetime access to the recordings irrespective of
  which course you choose
- The course is language agnostic, the prototypes will be in Go, but
  nothing very Go specific required

### February 2025 Cohort

8 weeks course
•
41 seats
left

Starts
15th February, 2025

9:00 am to 12:00 pm IST
on Saturdays and Sundays

---

During the live sessions, you will learn the
**intuition** behind building scalable systems from Arpit. You
will also be interacting with the entire cohort and learning from
their experiences.

Enroll Now

₹49,999
$649
============

inclusive of all the taxes

YOU'LL GET

✔
50+ hours of Live Classes on Weekends IST

✔
One 30-mins 1:1 Mentorship Session

✔
Lifetime access to the cohort recordings

✔
Lifetime access to the Network and Community

✔
Open forums and interaction with the cohort

✔
Doubt resolution during and post live sessions

✔
14 days no-questions-asked refund policy

✔
Language of communication will be strictly english

[Get it reimbursed by your employer](/expense)

[Enroll Now
→](/masterclass/pay)

Note: there is no discount on the course pricing; I feel
discounts are unfair to the folks who paid the full; the
price of the course is subject to a yearly hike.

If you have questions or need any clarifications before
enrolling, please reach out to me at
[arpit.masterclass@gmail.com](mailto:arpit.masterclass@gmail.com).

### Learn at your own pace

50+ hours

16 session
recordings

~28 systems
covered

---

You will get access to the recordings of one of my past but best
cohorts happened to date; so that you binge watch quickly and
prepare yourself for the task at hand.

Buy Now

₹39,999
$599
============

inclusive of all the taxes

YOU'LL GET

✔
50+ hours of recordings of my best cohort

✔
One 30-mins 1:1 (within 60 days from purchase)

✔
Lifetime access to the cohort recordings

✔
Lifetime access to the Network and Community

✔
Doubt resolution on Discord

✔
3 days no-questions-asked refund policy

✔
Language of communication will be strictly english

[Get it reimbursed by your employer](/expense)

[Purchase Recordings
→](/masterclass/payrec)

Note: there is no discount on the course pricing; I feel
discounts are unfair to the folks who paid the full; the
price of the course is subject to a yearly hike.

If you have questions or need any clarifications before
enrolling, please reach out to me at
[arpit.masterclass@gmail.com](mailto:arpit.masterclass@gmail.com).

## The Program

This is a flagship,
intermediate-level
cohort-based course aimed at providing an
exclusive and
crisp learning experience. The
program will cover most of the topics under System Design and Software
Architecture including but not limited to - Architecting Social Networks,
Building Storage Engines and, Designing High Throughput Systems.

The program will have a blend of Live Classes happening on
Weekends, 1:1 Mentorship sessions, and self-assignments. The
program is designed to be
intense and
crisp to accelerate learning.
In the course, we will

- have in-depth discussions about trade-offs and tech stack
- discuss the evolution of architecture as the traffic surges
- be covering key design decisions for efficient implementation
- be doing a lot of brainstorming just like how it happens in
  your org
- be designing cost-efficient architectures, mimicking the
  real-world design

## Program outline

You can find detailed problem statements in this
[Github repository](https://github.com/relogX/system-design-questions).

Foundation

### Week 1

The first week is about learning the macro and micro components of System Design by taking real examples and issues at scale.

▼
Topics and agenda

- Designing Online/Offline indicator
- Connection pool and its internals
- Caching issues at scale and how to solve them
- Async processing, Delegation, and Kafka Essentials
- Different Communication Paradigms

Checkout detailed
[problem statements](https://github.com/relogX/system-design-questions)
▸

Databases

### Week 2

This week, we learn everything about databases, from SQL to NoSQL to Embedded, learn how to scale them.

▼
Topics and agenda

- Pessimistic Locking on Relational DBs
- Designing and Scaling SQL-backed KV Store
- How to scale relational databases
- NoSQL Database Trade-offs
- Designing Slack's Realtime Text Communication
- Scaling Websockets

Checkout detailed
[problem statements](https://github.com/relogX/system-design-questions)
▸

Going Distributed

### Week 3

The third week will be about understanding Distributed Systems and the challenges that come while building them.

▼
Topics and agenda

- Designing Load Balancers
- Scaling Load Balancers
- Implementing Remote and Distributed Locks
- Distribtued ID Generators

Checkout detailed
[problem statements](https://github.com/relogX/system-design-questions)
▸

Building Social Networks

### Week 4

This is when we start modeling and building real-world systems, and we start with Social Networks.

▼
Topics and agenda

- Designing and implementing Photos Upload at scale
- Implementing Private Photos for Instagram
- Designing Gravatar and Dynamic OG Images
- Designing Concurrenct HashTag Counter
- Designing Message Indicators

Checkout detailed
[problem statements](https://github.com/relogX/system-design-questions)
▸

Building Storages

### Week 5

This week will be about building ephemeral, persistent, reliable and durable storage engines.

▼
Topics and agenda

- Implementing Single-node Cache like Redis
- Designing Distributed Caches
- Designing a Word Dictionary without a DB
- Designing Log-Structured KV Store

Checkout detailed
[problem statements](https://github.com/relogX/system-design-questions)
▸

Building High Throughput Systems

### Week 6

This week, we level-up and build systems that are heavily concurrent and required to deliver very high throughput.

▼
Topics and agenda

- Designing S3
- Designing Multi-tiered Orders for Amazon
- Designing LSM Trees Ground Up
- Designing Video Processing Pipeline
- Designing Live Streaming

Checkout detailed
[problem statements](https://github.com/relogX/system-design-questions)
▸

IR Systems and Adhoc Designs

### Week 7

This week is about understanding Information Retrieval Systems and discuss some interesting adhoc systems.

▼
Topics and agenda

- Designing Recent Searches
- Designing Cricbuzz's Text Commentary
- Instagram Live Reactions
- Designing Distributed Task Scheduler
- Designing and Implementing Flash Sale

Checkout detailed
[problem statements](https://github.com/relogX/system-design-questions)
▸

Building Algorithmic Systems

### Week 8

The final week will be about exploring systems that are driven by a super-clever algorithm.

▼
Topics and agenda

- Approach behind GitHub's File Sync
- Understanding and Scaling GeoSpatial Search
- Designing and Implementing User Affinity Service

Checkout detailed
[problem statements](https://github.com/relogX/system-design-questions)
▸

## Program pre-requisites

The course operates at an intermediate-level and you would get
100x value out of it if

- You are SDE-2, SDE-3, and above or have at least 2 years of
  work experience
- You have a basic understanding of System Design
- You have completed [System Design Pre-requisite Playlist](https://www.youtube.com/playlist?list=PLsdq-3Z1EPT36NJXTutvKcreetuHCr9a-)
- You are interested in learning how real systems are built and
  implemented
- Have watched at least a few System Design videos on YouTube
  from me or other YouTubers
- You have a basic knowledge of public cloud offerings like S3,
  SQS, etc.
- Have couple of months of time over weekends that you can
  dedicate towards upskilling

I have created this
[playlist](https://www.youtube.com/playlist?list=PLsdq-3Z1EPT36NJXTutvKcreetuHCr9a-)
of videos that would give you a good headstart for this course. Feel
free to go down the rabbit hole and explore in detail the tech mentioned
in the playlist. You can also choose to enroll in my
[System Design for Beginners](/sys-design)
course that is meant to cover system design from absolute scratch.

## Wall of Love

Some formal/informal messages I received about this course.

![](https://github.com/user-attachments/assets/7380038e-580b-43ae-9909-0c83926e6a11)

![](https://github.com/user-attachments/assets/634d7855-f28d-4372-a164-133f9a1f83d0)

![](https://github.com/arpitbbhayani/articles/assets/4745789/fd222e77-9a61-4f98-8849-8ab42f8cb3bd)

![](https://github.com/arpitbbhayani/articles/assets/4745789/e2a62fce-fc39-4bdb-be1a-d30d5eea0fe4)

![](https://github.com/arpitbbhayani/articles/assets/4745789/16afb094-d506-4c83-9fe5-9af9c9880ad2)

![](https://user-images.githubusercontent.com/4745789/266771127-3220b146-97f0-46d9-8c1e-408771afff8a.jpg)

![](https://user-images.githubusercontent.com/4745789/266771133-35157bac-3648-4cd2-ae34-d216a89106fa.PNG)

![](https://user-images.githubusercontent.com/4745789/266771138-07a1d83d-2020-4596-a3f0-51b80234739b.jpg)

![](https://user-images.githubusercontent.com/4745789/266771139-fd93c7ef-e96f-4bf8-ac4f-83867b591070.jpg)

![](https://user-images.githubusercontent.com/4745789/266771141-f12849b1-56d6-4aee-a4ac-b9bebf63de9a.jpg)

![](https://user-images.githubusercontent.com/4745789/266771145-0c20bc15-23d9-4aac-8e5e-359ecd2a6697.jpg)

![](https://user-images.githubusercontent.com/4745789/266771148-e0c35896-7ff0-4dee-a73a-658ab59d048e.PNG)

![](https://user-images.githubusercontent.com/4745789/266771571-4d696aaf-ab61-4529-ad35-ce37fc7f2659.png)

![](https://user-images.githubusercontent.com/4745789/266771573-434058c5-9a52-4b21-a187-2fa0e8152a11.png)

## Why a cohort based course?

## cohort

/ˈkəʊhɔːt/

_noun_

A cohort is a small group of students who work through a
curriculum together to achieve the same learning objective.

### Small and Focussed Group

Each cohort will have close to ~70 people ensuring you have a richer learning experience.

### Thrive Together

Unlike MOOCs where the information flow is unidirectional, the cohort here will thrive on interactions and collaborations.

### Discuss and Learn

Learning happens when we discuss, and hence everyone is encouraged to speak, put forth opinions, and discuss.

## Why should you join?

The primary objective of this program is to make you comfortable
at building systems that are
scalable,
fault-tolerant, and
reliable. But here is what
you could reap out of it.

### Design systems like a pro

The course will make you comfortable at designing any system, no matter how stringent the requirements are.

### Know the unknowns

Learn some of the most interesting concepts, super-clever algorithms, and sophisticated architectures. You are bound to have mind-blown moments.

### 1:1 Mentorship

When in a dilemma about an architectural decision, engineering challenges, career advice, or general mentorship, get your doubts cleared during your 1:1s with Arpit.

### Upskill and level-up

Learn the designing paradigms and upskill yourself to accelerate your career growth and stand out.

### Network and Community

Perhaps the most interesting takeaway will be your new network, friends, and a lot of memories.

## The world is learning

People from all over the world have mastered System Design through
this course.

![System Desgin Masterclass Demographic](https://github.com/arpitbbhayani/articles/assets/4745789/2b53a65b-dd06-43af-b0b1-2ba7f79f47a6)

1700+

ENGINEERS

26

COHORTS

28

COUNTRIES

## Who took this course?

Folks belonging to some of the best companies and high thriving
startups have taken this course, the list includes the likes of

![Tesla](https://user-images.githubusercontent.com/4745789/115353876-a64bf080-a1d6-11eb-8abd-da1e948c289d.png)

![Google](https://user-images.githubusercontent.com/4745789/131295363-04dc4fdf-4674-49f7-a453-90a42ec581f9.png)

![Microsoft](https://user-images.githubusercontent.com/4745789/131295468-f8f52c8a-0bc1-4765-baa6-009490eb808e.png)

![Amazon](https://user-images.githubusercontent.com/4745789/115350867-40119e80-a1d3-11eb-8ed1-3d689b2bf9b2.png)

![GitHub](https://user-images.githubusercontent.com/4745789/131295948-fe68928f-69de-476e-a1e5-bd0843d91e8c.png)

![Flipkart](https://user-images.githubusercontent.com/4745789/115350183-684ccd80-a1d2-11eb-9891-620ff043bcfa.png)

![PayTM](https://user-images.githubusercontent.com/4745789/123135938-0299ed80-d470-11eb-823a-54c0fc9647dd.png)

![OYO](https://user-images.githubusercontent.com/4745789/131298059-a0c027bc-0076-437a-9c05-cd274a564aea.png)

![PayPal](https://user-images.githubusercontent.com/4745789/131295708-22a60122-645f-41bc-a810-265a8b4a2bd0.png)

![Grab](https://user-images.githubusercontent.com/4745789/131296241-97d9a0b9-e4c2-477e-8cbd-badb9289a5b5.png)

![MakeMyTrip](https://user-images.githubusercontent.com/4745789/131296448-35521312-94d1-43f7-9648-331cac056ad1.png)

![Dream11](https://user-images.githubusercontent.com/4745789/131296736-5735286e-811f-4d3f-a95c-9280bd063b93.png)

![Unacademy](https://user-images.githubusercontent.com/4745789/131297119-59302b5d-2375-428e-ad80-9230a06a9b67.png)

![BrowserStack](https://user-images.githubusercontent.com/4745789/115350976-5f103080-a1d3-11eb-812d-99568d38e3e3.png)

![Practo](https://user-images.githubusercontent.com/4745789/123136405-8358e980-d470-11eb-8889-7d34e1c9e11e.png)

![Yelp](https://user-images.githubusercontent.com/4745789/115351392-d47c0100-a1d3-11eb-8418-ae0a4e344d9f.png)

![Gojek](https://user-images.githubusercontent.com/4745789/143415343-8dc38908-401f-4f69-a15f-563c04ee2841.png)

![Scaler](https://user-images.githubusercontent.com/4745789/143414790-6c4d69d6-fc3b-4129-b64e-2f6eb46f7e21.png)

# Teaching style

Here are some of the videos that will give you a peek into my teaching
style how I teach and the depth I go into

[![](https://i.ytimg.com/vi/wXvljefXyEo/maxresdefault.jpg)](https://youtu.be/wXvljefXyEo)

[![](https://i.ytimg.com/vi/9iAJjtvBwyI/maxresdefault.jpg)](https://youtu.be/9iAJjtvBwyI)

[![](https://i.ytimg.com/vi/7v-wrJjcg4k/maxresdefault.jpg)](https://youtu.be/7v-wrJjcg4k)

[![](https://i.ytimg.com/vi/--YbYCfMnxc/maxresdefault.jpg)](https://youtu.be/--YbYCfMnxc)

[![](https://i.ytimg.com/vi/AiPGbVjl3JY/maxresdefault.jpg)](https://youtu.be/AiPGbVjl3JY)

[![](https://i.ytimg.com/vi/2Bwzh-881PU/maxresdefault.jpg)](https://youtu.be/2Bwzh-881PU)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

# Hey, I am Arpit

### curious, tinkerer, and explorer

I am a software engineer and an engineering leader passionate about
System Architecture, Databases Internals, Language Internals, and
Advanced Algorithms. I am the creator and maintainer of [DiceDB](https://github.com/dicedb/dice)

- an open source, reactive, multi-tenant, cache optimized for modern
  hardware.

In 2024, I took the leap of faith and co-founded [Profile.fyi](https://profile.fyi), where I took care of all things tech and product. Before this, I
was a Staff Engineer at
[Google](https://cloud.google.com/) leading
the
[Dataproc](https://cloud.google.com/dataproc) India team in providing a managed big data ecosystem to GCP customers.
I was also part of [Amazon's](https://www.amazon.com/)
Fast Data Team and took care of cold tiering of hot data and providing
a seamless query interface across all tiers.

I held engineering leadership positions (both IC and management) at
[Unacademy](https://unacademy.com/),
where I built, grew, and led Search, Site Reliability Engineering
(SRE) teams, and Data Engineering teams. I hold a total of 10+ years
of experience in scaling backend services, taking products and teams
from 0 to 1, and beyond.

I keep diving deep into engineering details and share my learnings
by across my
[socials](https://twitter.com/arpit_bhayani) and videos on
[YouTube](https://youtube.com/c/ArpitBhayani).

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

## What worked the best?

From the reviews and feedbacks I gathered, here are a few key
things that worked for folks who took this course.

mental models and frameworks

structured and well organised

quality and non-repetitive content

minute implementation-details

open ended discussion

much more than blogs we find

## What people say about this course?

Some testimonials from the people who recently took this course.

#### Prateek Singh

I had no interviews coming up, I had two months where I took a leap to learn some new stuff. If you are enrolling just to learn and learn how to learn as a software developer, you are in for a ride. The course dives deep into building and highly practical and scalable systems, largely driven by his own personal experience and learning.

From designing distributed systems, managing trade-offs in the CAP theorem, to optimizing performance in high-load scenarios, Arpit's approach breaks down complex topics into understandable, actionable insights from the first principles. If you’re part of decision-making conversations in your organization—whether it's about optimizing performance, scaling architectures, or balancing system trade-offs—this course will equip you with the knowledge to make informed and practical decisions.

I loved the emphasis on pragmatism and the focus on how to get the job done in the most efficient, simple and scalable way possible. You’re surrounded by brilliant engineers from diverse backgrounds, and seeing everyone converge on a solution is truly magical. Whether you're building AI-driven applications or designing large-scale systems, this course will help you grow into a better engineer, help create more impact and rewire your brain.

A few key takeaways for me were:

- You win some, you lose some.
- If it works for you, it works for you.
- Simple systems scale.
- Framework of opposites, a good solution is often in the middle.
- Trade-offs aren't just about space and time—sometimes, you can even trade off correctness to build practical, real-world systems.

#### Uday Chauhan

Arpit is a fantastic mentor who goes beyond just answering technical questions. His deep understanding of complex systems, especially ones I had never worked with, gave me the confidence to tackle unstructured problem statements with ease. He doesn’t just provide textbook answers—his real-world examples make the concepts click.

One thing that stands out is his motivational guidance, which kept me on track even when things got tough. In a market where many are just selling courses for the sake of MAANG preparation, this course stands out because it’s focused on solving real-world problems and building the right mindset.

Attending the sessions is genuinely fun! Especially the Q&A sessions after the course, where I get to hear about other people's challenges and see how those problems can be solved. It's really eye-opening to understand various perspectives and solutions.

Arpit genuinely cares about each student’s growth, and I appreciate how this course is tailored to practical challenges, not just theory. This is hands down one of the best learning experiences I’ve had!

#### Shahzad Ahamad

Arpit truly puts his heart into this course, dedicating his full effort to make each session as engaging and insightful as possible. He is, without a doubt, one of the best teachers I've ever had, with a natural talent for teaching. He starts with simple problem statements and skillfully guides us to the system design of enterprise-level applications, adding layers of complexity as we go. The entire course and schedule are fantastic, but the brainstorming sessions are the highlight. I never thought I would be so glued to my chair during a technical discussion, much like watching a suspenseful movie.

After following Arpit on social media for some time, I became convinced that I needed to take his master class. Although I was initially hesitant because of the fee, I am so glad I took the plunge. Arpit delivers far more than expected. The most important thing he brings is the spark of curiosity and a new way to approach problems. This course has been worth every bit of investment.

Thank you, Arpit, for an incredible learning experience, for sparking a new way of thinking in all of us, and for the great impact you're making in the developer community!

#### Dhanya Krishnan

“No fluff engineering” - This was the only quote I saw and I knew that I must join Arpit’s cohort. And trust me, the quote is true!

I know two types of Engineers - one who are passionate about their career and curious to learn and grow, the real doers who has made engineering products so valuable. Then there is this other category - I call them “Diwana’s” of engineering. They have got this love or craze for engineering, digging so passionately into stuff and seek this dopamine hit by engineering! They take engineering to heart. I am not here to categorise you or me in either of them, but I know for sure that both can benefit a lot from this cohort.

The cohort on System Design focuses on almost all the aspects of distributed systems in a very practical way. We have discussions and code examples on scalable design, concurrency, synchronous and asynchronous processing at scale, and also the protocols and design decisions in different scenarios. Arpit first gives a high level perspective and then we dig into the details so flawlessly, ending up designing an LSM tree, without even realising it. How cool is it ? :DTo keep things short, after my Masters in Distributed Systems Engineering, this was the place where I found the most clarified and organised knowledge base on large scale systems design. And, trust me I have grinded almost all the system design resources out there & can benchmark this course as great!

I would highly recommend freshers, mid-level and senior engineers to invest in yourself by joining this cohort.

Engineering is so beautiful. But, for you to see it, you must first look!

#### Viral Lalakia

Before joining the course, I had been following Arpit on multiple platforms for more than a couple of years. I was already amazed and impressed by the quality content in depth and by Arpit's teaching style. Little did I know that I was in for a huge surprise. The first week was enough for me to realize that I have found the treasure I have been looking for a few years to learn and grow as an engineer. Looking back, I can clearly see that my engineering lenses are more clear than ever before.

Arpit, as a combination of a teacher and an engineer, is exemplary. I have never met an engineer like him before. He is great at keeping things simple and no fluff, and still manages to dive deep into each aspect of the system design. He actually did all of that for each and every design we discussed. His energy and thought process are contagious.

This cohort was the best thing happened to me in the last few years as an engineer. It feels like I have gained a few years of experience in the last 8 weeks only, and I have the roadmap ready for my learning and growth as an engineer for months. The right mix of Arpit's approach for each session, knowledge, explanations, handling of the live sessions and encouraging cohort members to share their experiences makes this course extraordinary. The cherry on the top is, the sessions end only after all questions are answered. The bonus is the recordings, which may not seem like a big deal initially, but are extremely necessary to grasp each concept and every detail Arpit has covered in the 50+ hours.

Now, when the course is completed, I will dearly miss the sessions on weekends. After going on and on for 8 weeks without taking any breaks, this feeling of missing the sessions speaks the volume of the quality and the aftereffect it left on every one of us, especially me.

I would encourage all mid/senior+ engineers to enroll for this course if you are looking to unlock your own potential for the system design and being a better version of yourself as an engineer. Based on this heavenly journey I experienced in the last 8 weeks, I feel that I have received the lifelong return, which is humongous in comparison to the investment made. Totally worth it!

I really loved the course and enjoyed being part of it. My heartfelt gratitude and respect to Arpit for designing this course exceptionally well and delivering much more than what I expected!

#### Chirag S P

When I signed up for the course I came in with the expectation of learning different System Designs of various companies. Over the past 8 weeks I have come to realize on how to actually approach a problem. The way Arpit approached a System Design problem and how intuition based it can be, was truly eye opening.

Also I have come to realize how wonderful and vast the domain is. Also as Arpit mentions repeatedly in the course its not about drawing boxes, but understanding the context and actually approaching it from first principles.

Overall this course have made me fall in love with Engineering again and put me in a different trajectory that would help solve real world problems at whichever organization I work in.

PS: Arpit answered every question asked during the course and no question were left unanswered. That is truly commendable.

#### Mrunal

I've taken a few system design courses before, but they never really got into the nitty-gritty of implementation, where the real fun (and struggle) begins. System design is way more than just drawing boxes, and Arpit's course blew my mind :) Shoutout to my friend for recommending it – the cohort setup even got me to be a bit more disciplined (believe it or not)!

Arpit doesn't just talk about systems; he dives into the implementation details, the trade-offs involved, and the reasoning behind it all. It's not just about understanding existing systems; it's about levelling up your problem-solving skills for real-world design challenges.

The course format and Arpit's delivery were top-notch. I'm stoked that I caught every session live. Trust me, investing in this course was the best thing I did for my growth. You are an inspiration Arpit! Big thanks, for an awesome learning journey!

#### Digvijay

This course is the best investment I have made in my career!

I was looking for something which will actually help me upskill, gain knowledge and go deeper into things rather than talking about surface level things from interview point of view. Undoubtedly I got more than I had thought.

To be honest when I was looking for courses out there to learn System Design and to learn how things actually work in real world, I didn’t see much options and almost all of them were around Interview preparation

When I looked at the price of this course i.e. Rs 50,000 and compared it to other available options I thought it is 3-4 times of it and I asked myself will it be worth it. Today after going through this 2 months journey the answer is YES! it was completely worth it.

“The worth of good knowledge cannot be measured” Thanks a ton Arpit for the Knowledge and the learnings!

#### Kunal Vats

I didn’t just learn system design, I learned how to think; from 0 to all nuances and trade-offs. Unlike any other course, this one didn’t overwhelm me with jargon or shortcuts - it taught me how to solve real-world problems step by step. Everything is well structured.

Arpit's passion is totally contagious, and the regular prototyping, post-reads built my confidence.

And, I didn't start this course for interview shortcut (it was for learning and taking a leap from my slump); but by the end, I've got 5 good offers. And, I owe it all to this course and Arpit.

#### Prajwal

Working in the industry for just over three years, I had become complacent about the technologies around me and took their implementation for granted. What worked for me was Arpit reigniting my curiosity to delve deeper into the hows and whys of any system, library, or algorithm and think of them from a first principles perspective.

From high-level system design of "drawing boxes" to diving into SQL-level queries , connecting the tech with real-life business usecases,and then literally jumping into the IDE to code it out and see it run flawlessly. It was a cocktail of learnings.

I'm glad I signed up for this cohort. I will be using the insights and intuitions I've gained in my future endeavors. Thanks again, Arpit, for bringing this quality to the Indian tech space.

#### Mutcharla Praveen Kumar

Attending Aprit Bayani’s master class on system design has been a transformative experience for me, both professionally and personally. Before the course, I had a solid understanding of system design, but I often found myself unsure about how to scale systems effectively, especially in high-demand environments. Aprit’s course not only expanded my technical knowledge but completely shifted my mindset on how to approach the design and architecture of complex systems.

The master class was incredibly well-structured, breaking down intricate concepts like scalability, fault tolerance, distributed systems, and data consistency into digestible modules. Aprit’s teaching style was engaging, and he used real-world case studies that made the learning experience relatable and practical. I found myself constantly thinking, ‘This is exactly what I need to solve the problems I’ve been facing at work!’

One of the most impactful aspects of the course was how it addressed designing for scalability from the ground up, ensuring systems could handle growth without compromising performance or reliability. Aprit emphasized the importance of thinking beyond just the technical solution, encouraging us to consider how decisions affect the entire system, both in the short and long term.

What truly stood out for me was the immediate applicability of the knowledge. After completing the master class, I was able to implement several key principles into my work at Lenskart. The lessons I learned allowed me to design more scalable, efficient, and resilient systems, which had a direct impact on improving performance in our operations. It was like being given a new toolkit that I could apply directly to my day-to-day challenges.

Aprit’s insights and practical advice on tackling real-world scaling issues have fundamentally changed how I approach problem-solving in system design. I now feel much more confident in making architectural decisions that account for growth, high availability, and distributed environments. His master class is a must for anyone serious about taking their system design skills to the next level, especially in the fast-evolving tech landscape.

Overall, the course has been a turning point in my career, helping me evolve from just designing systems to thinking strategically about how to scale them for the future. I highly recommend Aprit Bayani’s master class to anyone looking to enhance their expertise in system design at scale.

#### Padmanabhan

Masterclass on System Design By Arpit was extremely good. It definitely re-ignited the engineering curiosity. It was completely practical where each and every session was packed with extremely good content and lot of hands-on topics were covered. Right from Connection pooling, Load Balancer, LSM Trees, CDNs, Consistent Hashing, Distributed Task Scheduler and so on. I was honestly excited for the weekends mainly for these sessions. Not only with respect to explaining the concepts, Arpit did a great job in clarifying all the questions. Though the sessions were scheduled for 3 hours, there was not even one class which would end at that time :).

Arpit gave a lot of real-time examples for each concept which helped in coming up with more options to solve the problem. Instead of giving the solution directly, he used to probe us which helped us in thinking in the right direction. The ultimate joy came from the fact that individuals tried implementing the prototype and seeing the results.

With the depth at which Arpit covered the concepts, it became really easy to design any systems. Arpit's cohort is a hidden gem among all other youtube videos which simply considers a theoretical approach which may not work in the long run. Few takeaways from this course which I will use in my day to day work are:

1. Business >> Product >> Engineering
2. Framework of Opposites - Choosing one option and considering exactly opposite of it.
3. Design anything considering that ONLY i will be implementing it.
4. Instead Simply going through a tutorial video or a link, trying to implement it which shows the actual nuances associated.

I would extremely recommend every Senior Engineer/ Architects out there to take up this course.

#### Prateek

I had two main goals when I decided to register for Arpit's videos: a) to identify gaps in my knowledge, and b) to gain insight into how staff engineers approach designing a wide array of systems. I can confidently say that both of my goals were met. If your aim is solely to pass engineering interviews, this testimonial may not be particularly helpful, and you might be better off with crash courses and prescribed books. However, after reaching a certain level, simply regurgitating the same information as every other engineer won't take you much further. Here's what I gained from Arpit's course:

- Arpit summarizes complex research papers exceptionally well. Simply knowing these papers exist was extremely helpful in navigating unfamiliar territories. I was able to organize my learning in such a way that whenever I'm curious about a system, I can now search for the most relevant research paper highlighting generic patterns.
- The patterns he explains are quite portable and mirror how real systems are actually designed. Having some relevant experience under my belt before watching his videos made them even more applicable to my work. If you're a few years into your career, you'll likely find his videos particularly helpful for practical application.
- The course covers variety of topics; if you are looking to just watch for interviews (I recommend not :-)), even then there's enough breadth in both product architecture and infrastructure.
- He encourages you to think critically about your exact database queries. It's easy to gloss over queries, but actually writing them down forces us to think harder and more precisely.
- Even though I purchased the videos, listening to others' questions were helpful, as more often than not, the doubts we have are shared by others and get addressed.

#### Soumya

Before I joined the cohort, my mood was low to put it mildly. But just the first week was enough to lift my spirits. Looking back, I couldn’t have picked a better time to join. This course helped me shake off the fear of coding the unknown. Now, I look forward to these extremely engaging and intense sessions every week.

Arpit has an amazing knack for breaking down big, complex systems into concrete, repeatable patterns. His years of dedication and extreme passion towards designing scalable distributed systems really show. For me, the best part and USP of the course is Arpit’s unique ability to break down and demonstrate how the core fundamental patterns that drive the most complex systems can be built in just a few hours to a couple of days, all with minimal setup or infrastructure. This practical, hands-on approach made the concepts highly relatable and achievable, significantly boosting my confidence and enthusiasm.

The cohort felt super relatable and had a very high engagement because the focus was on designing practical, real-world systems with a strong bias towards user and business empathy. Most of us connected with the real-world constraints and had our own backstories to share. Arpit managed to bring together highly motivated people with top-notch expertise, sparking intense and high-quality discussions.

As we near the end of the cohort, I feel highly motivated to further my expertise in the area of distributed and scalable systems. For me, this course rates right at the top. It’s at least a 100X return on your investment of money and time.

#### Vinamra Sareen

What a delight it from Arpit who shared his practical experience and discussed in detail pros and cons of each approach via building intuition. Such a relevant stuff it was and there was definitely no fluff in it.

I feel more confident than ever in taking part in discussions related to system design, I think of all cases, cost, engineering complexity, user experience while implementing solution.

Best part is years of experience is condensed and shared by Arpit, how he handled things, most common things we should consider and what not.

If someone wants to take deep dive into designing systems from intermediate to professional, this is the best investment you got to make. Arpit helps by building intuition on how to solve common to complex product problem using engineering. It is not about just using fancy terms like distributed, cloud and whatnot but rather understanding on hardware level how it will impact not just engineering but business revenue.

Again, amazing course love the sessions, content, how it was represented, practical experience in implementing stuff and long AMA at the end brought so many interesting insights from everyone.

#### Andreea Olaru

I would highly recommend taking this course, it is not only fun and engaging but it changes your way of thinking and approaching a new system by looking at the tradeoffs, crunching the numbers and putting yourself in your users' shoes.

In my timezone the course would start at 6:30 am and each morning i would wake-up with peak excitement because i would know i will be part in a bunch of interesting design discussions and debates.

Arpit's passion and excitement is contagious, after the sessions with him i would rediscover the joy of coding for fun again, he makes every complicated topic seem so easy that you think to yourself: "hmm would never have thought i would have the competence and confidence to implement an LFU strategy on O(1) time or a load balancer, yet after the session with Arpit i feel motivated to give it a try"

#### Aalia Lokhandwala

Arpit, you are an exceptional teacher. The way you have fostered our intuition over these 8 weeks is truly commendable. I've learned to recognize patterns, and as you often say, it's just copy-paste from there!

Being part of this cohort has been such a fun experience, and I found myself eagerly looking forward to the weekends because of it. The effort you have put into building this course is evident in the quality of the content we've discussed.

Not only have you helped me develop intuition, but you've also ignited a curiosity in me to explore further. Now, I read and research more before making any design decisions, and I find myself understanding technical papers more easily.

I can't thank you enough for your hard work and for always resolving our queries by the end of the day. Thank you so much for everything you do—your dedication is truly inspiring.

#### Vasavi

This year I completed the beginner course and rewatched the advanced lectures. I had stumbled upon your course by chance. The "No fluff" tag caught my attention, at a time when I was hungry for asli engineering discussions. I signed up and the course exceeded my expectations.

Thank you so much for putting such high quality content out there, for making it accurate and simple to grasp! Thank you for being encouraging, and for making the course discussion-based which made us think and not just consume! I have learnt a lot from your courses and videos - so much so that I have been getting appreciative comments on my knowledge, in system design discussions with peers. The content and your palpable passion in sharing it with the world reinstilled in me the inspiration, which had made me pursue computer science as a career in the first place - it reminded me of the joy in learning, understanding, and building systems with real world impact.

Thank you for all your efforts in teaching and inspiring! Hoping to keep in touch and continue learning from you. 🙂

#### Koteshwar Rao

I have done a couple of system design courses on Udemy and watched a bunch on YouTube as well, but none of them are even close to this.

What differentiates this course from the rest,

1. This course focuses on how to develop the intuition instead of question-answer type
2. It also highlights on how to think about businesses and product which is a very essential quality for Senior Engineers
3. The discussion doesn't start with different boxes/services already drawn and then explaining what each box does. We start with our basic intuition of how our system looks on Day 0 and then we understand what would be a problem with this design when we scale and then try to address each of them.
4. Discussions aren't about how to design Instagram, Cricbuzz, or E-Commerce but rather focus on a specific problem and then discuss deep on that. Ex: Image upload service, Flash sale, commentary for live match etc
5. We also realize how easy it is to prototype what we have learnt

One can understand how much passion and depth Arpit goes in his YouTube videos as well

#### Karan Patel

Previously, I had a fixed mindset regarding how I was going to tackle any system design question. Start with the functional and non-functional requirements, do some back-of-the-hand calculations to depict the scale of the system, start drawing boxes for application servers, load balancers, queues, and databases. While this approach works most of the time for cracking a few interview rounds, Arpit’s course forces you to ditch this conventional thought process and makes you dive deeper into systems and their actual implementations.

This course definitely is a goldmine not only in terms of the content but most importantly, how it sparks curiosity within you to think of system designs beyond just interviews.

1. This course will teach you how to build a solution for any system from the ground up with the right intuition and thought process.
2. Arpit has very smartly covered almost all the fundamental systems out there so that any new system can be easily built on top of it. “Just mix and match”.
3. One particular thing I liked about the course was the prototypes he created to explain the concepts. I mean it’s one thing to know the concept and completely different to see it live in action. Kudos for putting in the efforts to make the course more engaging.
4. If you were ever curious about how systems like the Flash sale, Cricbuzz commentary, Cricket live feed (My favourite discussion) work, look no further because these discussions will literally make you admire how awesome engineering truly is.

A course is only as good as the instructor and Surprise!! Surprise!! Arpit is too good at the teaching part as well. You can sense the enthusiasm and curiosity in his way of teaching as well. He would patiently answer all the questions asked even though the class duration was well past 4 hours. He is open to new ideas and solutions and doesn’t make you feel any lesser even for a bit.

Any senior engineer looking to make an impact at their workplace or waiting for that engineering spark to be reignited again, this course is a must! Thank you for proving that true engineering is well beyond interviews. "SUPERB!"

#### Saurabh Jagtap

This was the first one of the kind Cohort that I enrolled in. The way Arpit teaches is fabulous. I would say it is not even a teaching but feels more like an actual discussion that would go into an engineer's day-to-day life. He carries total empathy for a learner. The depth and details he carries in everything he says are fantastic. All the doubts and discussions will have an answer from Arpit. Folks tuning in to every cohort have the brightest minds, and the way discussion happens in cohort with Arpit along with such minds, is just a treat to experience.

What I loved the most was brainstorming sessions. It's never a monologue from Arpit. Sometimes we start to build and would never ever feel that we might be building something so big in just one half of the cohort's session, that too with proper brainstorming from people. The way he helps people to start dissecting the problem; feels like he is just showing the light, others are building the system unknowingly, and feels amazing after we see the output. The way he sparks the decision-making processes helps to build the right intuition, goes deep into the details, and picks every bit and piece of the system is just fantastic. It makes the session the most engaging.

You will get the hands-on experience of picking the simplicity of the system, importance of knowing the basics and other concepts in engineering, and dealing with the tradeoffs of every technology, and mostly the intuition-building

If I have to point out the learnings, they are something like this; and I believe most of the other people in the cohort would agree with me on this;

- Simple systems scale.
- Always prototype and implement.
- System design is like Lego blocks; you just have to place the right component in the right place, and it's not something that will be correct in the first iteration.
- Know the basics and concepts, then just pick it up and place it; you'll build crazy stuff.
- And probably the best: you get some, you lose some. Not a single system exists that can do everything.

This is not an ordinary system design cohort, you will ACTUALLY build systems that scale; for millions of users. And no joy is more than seeing it work.

#### Ajay Mahadevan

Arpit is undoubtedly one of the best engineers I've seen in my life. Started following his content on youtube quite a few months back and really liked the way he dives deep on interesting core topics. That brought me to his System Design Masterclass course. The initial Indian mindset made me contemplate on the price of the course but boy oh boy! I'm not regretting even a single bit for what the course was priced at! :)

This is a goldmine of content and as I say, if not all, most who have taken the course would agree that Arpit has truly sparked the engineering curiosity. He offers incredible value in what he teaches. Not exaggerating but I've had "wow" moments in every single class.

Arpit's credentials in engineering is as good as it comes. He is a seasoned pro and it shows in every session, the love and passion for engineering and his craft. With just 3 years of experience, when I entered the course, I was blown away in the first week itself. A serious eye opener for me into building systems for scale.

Arpit has curated the syllabus very beautifully and paces it really well. I'd really suggest to take the interactive live cohort rather than buying the compiled course cause live sessions are just something else! Its truly interactive. Almost every system is built by brainstorming with the folks in the cohort and Arpit helps converge to the right intuition and idea going step by step from scratch, questioning every decision made from the problem statement, design and implementation perspectives in depth. In short by the end of each system design you have the entire design laid out to you in which you understand the purpose of every component used, what it solves for and why this and not anything else, cause you were part of a lengthy discussion that arrived to it. Truly awesome stuff!

Its not only theory or surface level, he emphasizes so much on building prototypes for the concepts and systems we build in the cohort and he does a code walkthrough and explains his prototypes to show that its not hard to build prototypes. This also solidifies your understanding in the concept better as you often uncover interesting challenges when implementing. You also get access to a community and many great folks in the cohort. I felt like the dumbest person in the cohort and absolutely loved that cause there was so much to learn every time.

His patience and love for the work he does is so admirable and inspiring. Once the session is done he is open to taking and answering questions for however long it takes. Does not shy away or asks to stop asking questions after each session. I've seen this QnA sessions go for so long and he doesn't even flinch or calls quits. Massive respect for this Arpit!

Overall one of the best investments I've made in myself for my career.

I thank Arpit for this brilliant cohort and would 100% recommend it to anyone who has some experience in the field and looking to scale up to be senior engineer wanting to build systems the right way.

Don't miss out folks!

[Read all 245 testimonials

→](/masterclass/testimonials)

## Frequently asked questions

You can always drop me an email at
arpit.masterclass@gmail.com
for other questions.

#### What will be the language of communication and teaching?

I will be teaching the entire course in english and all the brainstorming with participants will be conducted in english.

#### Is this course right for me?

This course is for any engineer who wants to learn System Design. The program is most suited to someone who has some industry experience, at least 6 months.

#### Will you be giving teaching for the entire duration?

Yes. I will be teaching the entire course online and live over Zoom and will be providing feedback and 1:1 mentorship.

#### Will there be a class every day?

The Live Classes will happen on Saturdays and Sundays as per the time mentioned on the webite with a possible extension of 30/45 mins.

#### Can I get this course reimbursed from my company?

Talk to your manager and check if they can sponsor this course. The invoice that will be issued is a legally valid and sound invoice that can be used for any kind of reimbursement. Note: I will just share the invoice and certificate with you; and I will not be involved in any kind of reimbursement process, communication, or follow-ups from your finance and legal teams, it is between you and your employer.

#### When will I get the invoice?

The invoice will be issued only after the refund window is over. If you claim a refund, no invoice will be issued. To get an invoice write to me at arpit.masterclass@gmail.com.

#### What if I want the invoice in the name of my employer?

If you want the invoice in the name of your employer with tax details like GST number, please share them on my email arpit.masterclass@gmail.com within 48 hours of completing the payment; any request after 48 hours will not be entertained.

#### Will there be reserved slots for 1:1 mentorship?

1:1 mentorship calls will happen on-demand, you can block the slot from the learning portal itself.

#### What is the validity of 1:1s?

For system design masterclass live cohort, you can schedule 1:1 within 1 month of course completion and if you have purchased the recordings then 60 days from the date of purchase.

#### Will there be assignments and hands-on projects?

No. I would recommend you implement the core of every single system we discuss ensuring you apply what you learn. I highly encourage you to implement seek help during 1:1 sessions.

#### Will the course cover LLD?

The course will cover some aspects of Database Design and its internals, but it will not cover writing and designing classes, and low-level design patterns. The course is typically aimed at covering the massive spectrum of System Design and Software Architecture.

#### Are assignments and projects mandatory?

No. But it is advisable that you complete them to get a better understanding of the system, algorithm, and business logic.

#### Will we also implement and see the systems in action?

Due to time constraints, it is not possible to implement every system; it is recommended that you self-implement the system and understand the low-level details. The course will definitely cover systems from every aspect.

#### Will there be a recording available for future reference?

Every single Live Class will be recorded, and you will be given lifetime access to it.

#### Is there a refund policy?

14 days no-questions-asked refund window from the course commencement date (11:59:59 pm on the 13th day from the course commencement date) on cohort based course. 3 days no-questions-asked refund window from the date of course purchase for 'Learn at your own place' offering.

#### Can I share my learnings, resources on social media?

No. I hold complete right to cut-off the access to any course material if I find you sharing course material, learning, videos, and notes on social platforms or the internet.

#### Will I get access to other cohorts?

No. You will get lifetime access to the cohort you are part of and its recordings, or the recordings that you purchased.

#### Where are the classes conducted?

All Live Classes will be online, over Zoom, and all you need is an internet connection to attend the live sessions.

#### Will you be the only one teaching this course?

The entire course including Live Classes will be conducted by me, Arpit Bhayani, no external TAs, mentors, etc. You will get to learn everything from the horse's mouth.

#### What are the programming pre-requisites?

It would help if you had to have work experience of 6 months, plus a basic understanding of one of the cloud providers like AWS, GCP, plus some basic understanding of high-level system design by watching already available YouTube videos. You can also find a few videos on this ppage that I would recommend you through.

#### How is this course different from free videos on YouTube?

This cohort-based course aims to be live, intense, and interactive. Traditional MOOCs (existing videos on the internet) cannot offer these benefits as they are unidirectional and optimizes for one-to-many. With this course being taught live, you can get instant resolution to your doubts. The discussion, collaboration, and networking will have a major impact on your overall learning as there will be cross-pollination of information.

#### If I have already watched System Design videos on YouTube, is this course still helpful?

Yes. The kind of depth which is touched in this System Design is unmatched. Even if you have watched all the System Design videos out there, you will still have moments that will blow your mind.

#### Will I be getting an invoice of Payment?

Yes. An invoice will be issued to you with all the legal and necessary details. This means your employer can choose to process this invoice and provide reimbursement.

#### Will you issue a course completion certificate?

I do not generate the certificate for every candidate, but if you need it, just drop me a message, and I will issue one right away.

#### Can I use my Credit Card or avail EMI to make the payment?

Yes, we support Credit Card, Debit Card, UPI, and Credit Card based EMIs having a duration of 3 months, 6 months, 12 months, and 24 months as offered by Razorpay.

#### Can I share the account with multiple people?

I track the browsers and devices from which the course is being accessed and if I fnd anything suspicious, I hold the complete right to revoke the access of the course and not offer any refund.

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/system-design-for-beginners

System Design for Beginners | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

# System Design for Beginners

An in-depth, self-paced, and on-demand course that for early engineers
to become great at designing scalable, available, and extensible
systems at scale.

[Enroll Now](#pricing)
[Learning Portal →](https://courses.arpitbhayani.me)

If you are SDE-2, SDE-3, and above and already know system design, but
want to take your skills to the next level, do check out my live
cohort-based
[System Design Masterclass](/course) course.

### Enroll Now

# ₹19,999

inclusive of all the taxes

YOU'LL GET

✔
Watch and learn at your own pace

✔
35 videos covering the curriculum

✔
200+ doubts answered through text and videos

✔
Lifetime access to all course videos and doubts

✔
Language of communication will be strictly english

✔
Lifetime access to recordings of all doubt resolutions

✔
3 days no-questions-asked refund policy

✔
Bi-weekly doubt solving - Wednesday 7:30 pm to 8:30 pm IST

[Get it reimbursed by your employer](/expense)

[Enroll Now
→](/system-design-for-beginners/pay)

You will be able to access the course material through [courses.arpitbhayani.me](https://courses.arpitbhayani.me).

International folks, can also [pay in USD](https://rzp.io/l/svfN6G730) or [pay in EUR](https://rzp.io/l/pgbz8p7nJ2).
In case you want to pay in any other currency, just drop me an
email at arpit.masterclass@gmail.com.

If you have questions or need any clarifications before enrolling,
please reach out to me at
[arpit.masterclass@gmail.com](mailto:arpit.masterclass@gmail.com).

## The Course

This is a
beginner-friendly
self-paced course on system design aimed at providing an
exclusive and
crisp introduction to building
systems. The program will cover most of the topics under System Design
and Software Architecture from scratch assuming no prior knowledge.

## Course curriculum

Introduction

### Introduction to System Design

- What is system design?

  9 mins

- How to approach System Design?

  17 mins

- How to understand that you have built a good system?

  9 mins

Databases

### All Things Databases

- Relational databases

  20 mins

- Database Isolation Levels

  19 mins

- Scaling Databases

  18 mins

- Sharding and Partitioning

  16 mins

- Non-relational Databases

  15 mins

- Picking the right database

  13 mins

Caching

### All Things Caching

- Understanding Caching

  12 mins

- Populating and Scaling Caches

  11 mins

- Caching at different levels of architecture

  19 mins

Asynchronous Systems

### Asynchronous Processing

- Message Queues

  16 mins

- Message Streams and Kafka Essentials

  23 mins

- Real-time PubSub

  6 mins

Resiliency

### Designing for Resiliency

- Load Balancers

  16 mins

- Circuit Breakers

  13 mins

- Data Redundancy and Recovery

  9 mins

- Leader Election for auto-recovery

  8 mins

Essentials

### Essentials at Scale

- Bloom Filters

  21 mins

- Consistent Hashing

  27 mins

- Client-server and Communication Protocols

  22 mins

- Blob Storage and S3

  16 mins

- Introduction to Big Data

  17 mins

Consumer Facing

### Designing Consumer Systems

- Designing e-Commerce Product Listing

  20 mins

- Designing Tinder Feed

  32 mins

- Designing and Scaling Notifications

  38 mins

- Designing Twitter Trends

  30 mins

Common Utilities

### Designing Utilities

- Designing URL Shortners

  48 mins

- Designing API Rate Limiter

  26 mins

- Designing Realtime Abuse Masker

  22 mins

- Designing Web Crawler

  54 mins

Critical Systems

### Designing systems that matter

- Designing GitHub Gists

  28 mins

- Desinging Fraud Detection

  25 mins

- Designing Recommendation Engine

  37 mins

## Program pre-requisites

The course operates at a beginner-level and hence does not assume
any prior knowledge of system design. We would start with absolute
basics and then scale up to build real-world systems in a very
structured way. By the way, following are the pre-requisite for this
course

- a basic understanding of CS fundamentals
- a Google account because I only support Sign-in with Google :)

## Wall of Love

Some formal/informal messages I received about this course.

![](https://user-images.githubusercontent.com/4745789/266771125-b86d752c-6d9d-4e6b-879c-2d2c47d72354.jpg)

## Why should you join?

The primary objective of this program is to make you comfortable at
building
scalable,
fault-tolerant, and
reliable systems. But here is what
you could reap out of it.

### Design systems like a pro

The course will make you comfortable at designing any system, no matter how stringent the requirements are.

### Know the unknowns

Learn some of the most interesting concepts, super-clever algorithms, and sophisticated architectures. You are bound to have mind-blown moments.

### Upskill and level-up

Learn the designing paradigms and upskill yourself to accelerate your career growth and stand out.

## The world is learning

People from all over the world have mastered System Design through
my other System Design course meant for experienced engineers.

![System Desgin Masterclass Demographic](https://github.com/arpitbbhayani/articles/assets/4745789/2b53a65b-dd06-43af-b0b1-2ba7f79f47a6)

400+

ENGINEERS

28

COUNTRIES

220+

DOUBTS

## Who took my courses?

Folks belonging to some of the best companies and high thriving
startups have taken my intermediate system design course, the list
includes the likes of

![Tesla](https://user-images.githubusercontent.com/4745789/115353876-a64bf080-a1d6-11eb-8abd-da1e948c289d.png)

![Google](https://user-images.githubusercontent.com/4745789/131295363-04dc4fdf-4674-49f7-a453-90a42ec581f9.png)

![Microsoft](https://user-images.githubusercontent.com/4745789/131295468-f8f52c8a-0bc1-4765-baa6-009490eb808e.png)

![Amazon](https://user-images.githubusercontent.com/4745789/115350867-40119e80-a1d3-11eb-8ed1-3d689b2bf9b2.png)

![GitHub](https://user-images.githubusercontent.com/4745789/131295948-fe68928f-69de-476e-a1e5-bd0843d91e8c.png)

![Flipkart](https://user-images.githubusercontent.com/4745789/115350183-684ccd80-a1d2-11eb-9891-620ff043bcfa.png)

![PayTM](https://user-images.githubusercontent.com/4745789/123135938-0299ed80-d470-11eb-823a-54c0fc9647dd.png)

![OYO](https://user-images.githubusercontent.com/4745789/131298059-a0c027bc-0076-437a-9c05-cd274a564aea.png)

![PayPal](https://user-images.githubusercontent.com/4745789/131295708-22a60122-645f-41bc-a810-265a8b4a2bd0.png)

![Grab](https://user-images.githubusercontent.com/4745789/131296241-97d9a0b9-e4c2-477e-8cbd-badb9289a5b5.png)

![MakeMyTrip](https://user-images.githubusercontent.com/4745789/131296448-35521312-94d1-43f7-9648-331cac056ad1.png)

![Dream11](https://user-images.githubusercontent.com/4745789/131296736-5735286e-811f-4d3f-a95c-9280bd063b93.png)

![Unacademy](https://user-images.githubusercontent.com/4745789/131297119-59302b5d-2375-428e-ad80-9230a06a9b67.png)

![BrowserStack](https://user-images.githubusercontent.com/4745789/115350976-5f103080-a1d3-11eb-812d-99568d38e3e3.png)

![Practo](https://user-images.githubusercontent.com/4745789/123136405-8358e980-d470-11eb-8889-7d34e1c9e11e.png)

![Yelp](https://user-images.githubusercontent.com/4745789/115351392-d47c0100-a1d3-11eb-8418-ae0a4e344d9f.png)

![Gojek](https://user-images.githubusercontent.com/4745789/143415343-8dc38908-401f-4f69-a15f-563c04ee2841.png)

![Scaler](https://user-images.githubusercontent.com/4745789/143414790-6c4d69d6-fc3b-4129-b64e-2f6eb46f7e21.png)

# Teaching style

Here are some of the videos that will give you a peek into my teaching
style how I teach and the depth I go into

[![](https://i.ytimg.com/vi/wXvljefXyEo/maxresdefault.jpg)](https://youtu.be/wXvljefXyEo)

[![](https://i.ytimg.com/vi/9iAJjtvBwyI/maxresdefault.jpg)](https://youtu.be/9iAJjtvBwyI)

[![](https://i.ytimg.com/vi/7v-wrJjcg4k/maxresdefault.jpg)](https://youtu.be/7v-wrJjcg4k)

[![](https://i.ytimg.com/vi/--YbYCfMnxc/maxresdefault.jpg)](https://youtu.be/--YbYCfMnxc)

[![](https://i.ytimg.com/vi/AiPGbVjl3JY/maxresdefault.jpg)](https://youtu.be/AiPGbVjl3JY)

[![](https://i.ytimg.com/vi/2Bwzh-881PU/maxresdefault.jpg)](https://youtu.be/2Bwzh-881PU)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

# Hey, I am Arpit

### curious, tinkerer, and explorer

I am a software engineer and an engineering leader passionate about
System Architecture, Databases Internals, Language Internals, and
Advanced Algorithms. I am the creator and maintainer of [DiceDB](https://github.com/dicedb/dice)

- an open source, reactive, multi-tenant, cache optimized for modern
  hardware.

In 2024, I took the leap of faith and co-founded [Profile.fyi](https://profile.fyi), where I took care of all things tech and product. Before this, I
was a Staff Engineer at
[Google](https://cloud.google.com/) leading
the
[Dataproc](https://cloud.google.com/dataproc) India team in providing a managed big data ecosystem to GCP customers.
I was also part of [Amazon's](https://www.amazon.com/)
Fast Data Team and took care of cold tiering of hot data and providing
a seamless query interface across all tiers.

I held engineering leadership positions (both IC and management) at
[Unacademy](https://unacademy.com/),
where I built, grew, and led Search, Site Reliability Engineering
(SRE) teams, and Data Engineering teams. I hold a total of 10+ years
of experience in scaling backend services, taking products and teams
from 0 to 1, and beyond.

I keep diving deep into engineering details and share my learnings
by across my
[socials](https://twitter.com/arpit_bhayani) and videos on
[YouTube](https://youtube.com/c/ArpitBhayani).

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

## What people say about my courses?

Some testimonials from the people who recently took my courses.

#### Nityananda Gohain

One thing I can guarantee that after this course you can read any kind of system design book/blog with confidence and you will be able to relate to Arpits classes in someway or the other, also Arpit designed the course in such a way that you will be able to relate to previous classes and bring out patters in solving problems. My best investment ever :)

#### Tejas Agrawal

Had a super fun in learning how to learn and think system design. I am already using Arpit's teachings at my work, and I don't think anything more needs to be said.

#### Rohan Chougule

It's easy to throw jargons and assume that a system can be built. But then keeping things simple and starting from the basics, the approach to be taken, taking baby steps, and then heading towards a direction is indeed best taught by Arpit. Plus, his enthusiasm!

#### Sai Subramanyam Chittilla

Arpit is a passionate individual who teaches one of best system design course, his explanation comes from real implementation and exposure to the problem. He makes sure that individuals develop critical thinking when approaching system design.

I would highly recommend his course for any one who tries to build systems to scale.

## Frequently asked questions

You can always drop me an email at
arpit.masterclass@gmail.com
for other questions.

#### What will be the language of communication and teaching?

The entire course is taught in english and all doubt solving sessions will be conducted in english.

#### Is this a cohort based course?

No. This is a recorded self-paced course with bi-weekly doubt solving sessions with me.

#### How is this course different than your system design masterclass?

Masterclass and Beginner course are mutually exclusive. Beginner course is for absolute beginners and starts things from scratch. Masterclass assumes you have some understanding of systems and we take it forward from there. There will be very minimal overlaps in the topics I cover in the two courses. In general, if you have some understanding of systems and have watched existing system design videos, go for the masterclass. If you want to start from absolute scratch then opt for beginner one.

#### Is this course right for me?

This course is for any engineer who wants to get started with System Design. If you are a college student or a working professional wanting to start with System Design, this course is for you.

#### Is there a limit to number of doubt solving sessions I can attend?

No. But the discussion will be strictly related to topics discussed and covered in the course.

#### Will you be giving teaching for the entire duration?

Yes. I will be teaching the entire course.

#### Will the course cover LLD?

No. The course is focussed on High Level Design only.

#### Will we also implement and see the systems in action?

It is not possible to implement every system; it is recommended that you self-implement the system and understand the low-level details. The course will definitely cover systems from every aspect.

#### Will there be a recording available for future reference?

Yes. You will have lifetime access to the recordings of the course.

#### Is there a refund policy?

3 days no-questions-asked refund window from the date of course purchase (11:59:59 pm on the 3rd day from the date of purchase).

#### Can I share my learnings, resources on social media?

No. I hold complete right to cut-off the access to any course material if I find you sharing course material, learning, videos, and notes on social platforms or the internet.

#### Will you be the only one teaching this course?

The entire course will be conducted by me, Arpit Bhayani, no external TAs, mentors, etc. You will get to learn everything from the horse's mouth.

#### What are the programming pre-requisites?

There are no programming pre-requisites, but having a basic idea is always better.

#### How is this course different from your other system design course?

My System Design Masterclass is an intermediate-level course and hence assumes you know basic system design. This course assumes nothing and will ramp you up on everything you should know about it.

#### If I have already watched System Design videos on YouTube, is this course still helpful?

Yes. The kind of depth which is touched in this System Design is unmatched. Even if you have watched all the System Design videos out there, you will still have moments that will blow your mind.

#### Will I be getting an invoice of Payment?

Yes. An invoice will be issued to you with all the legal and necessary details. This means your employer can choose to process this invoice and provide reimbursement.

#### When will I get the invoice?

The invoice will be issued only after the refund window is over. If you claim a refund, no invoice will be issued. To get an invoice write to me at arpit.masterclass@gmail.com.

#### Will you issue a course completion certificate?

I do not generate the certificate for every candidate, but if you need it, just drop me a message, and I will issue one right away.

#### Can I use my Credit Card or avail EMI to make the payment?

Yes, we support Credit Card, Debit Card, UPI, and Credit Card based EMIs having a duration of 3 months, 6 months, 12 months, and 24 months as offered by Razorpay.

#### Can I share the account with multiple people?

I track the browsers and devices from which the course is being accessed and if I fnd anything suspicious, I hold the complete right to revoke the access of the course and not offer any refund.

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/redis-internals

Redis Internals | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

# Redis Internals

A self-paced course where you will learn Redis internals by actually
reimplementing its core features, like event loop, serialization
protocol, persistence, pipelining, eviction, and transactions, in
Golang.

[Enroll Now](#pricing)
[Learning Portal →](https://courses.arpitbhayani.me)

## The Course

Redis has to be the most versatile modern database out there.
Although it is heavily used as a cache, it can also be used as a
Message Broker, a Streaming engine, and so much more. This course
will be an answer to some of your most common questions about Redis

- why is it so fast? and how it can handle a large number of TCP
  connections while being single-threaded?

In this self-paced recorded course, we will be understanding the
internals of Redis by actually reimplementing it in Golang, and the features include

- event loop
- serialization protocol
- approximated LRU eviction
- command pipelining and transactions

Our reimplementation in Golang will be an actual drop-in replacement of Redis which means you would be able to connect to this
DB through any Redis client in the world! Our entire reimplementation
is open-sourced and can be found on Github at [DiceDB/dice](https://github.com/dicedb/dice).

## Course curriculum

CHAPTER 1

### Starting up

- Course Introduction

  10 mins

- What makes Redis special?

  22 mins

- Writing a Simple TCP Echo Server

  16 mins

CHAPTER 2

### RESP, PING, and Event Loop

- Speaking the Redis' Language

  16 mins

- Implementing RESP

  15 mins

- Implementing PING

  19 mins

- IO Multiplexing and Event Loops

  21 mins

- Handling multiple concurrent clients

  26 mins

CHAPTER 3

### GET, SET, and auto-delete

- Implementing GET, SET, and TTL

  22 mins

- Implementing EXPIRE and auto-deletion

  27 mins

- Eviction strategies and implementing simple-first

  20 mins

CHAPTER 4

### Pipelining and Persistence

- Implementing Command Pipelining

  21 mins

- Implementing AOF Persistence

  21 mins

CHAPTER 5

### Objects, statistics, and LRU

- Object, Encodings, and Implementing INCR

  25 mins

- Implementing INFO and allkeys-random eviction

  20 mins

- The Approximated LRU Algorithm

  23 mins

- Implementing the Approximated LRU Algorithm

  25 mins

CHAPTER 6

### Memory management

- Understanding how Redis caps memory

  13 mins

- Overriding malloc for better performance

  6 mins

CHAPTER 7

### Signals and Transactions

- Implementing Graceful Shutdown

  37 mins

- Implementing Transactions

  22 mins

CHAPTER 8

### Core Data Structures and Algorithms

- List Internals - Ziplist and Quicklist

  28 mins

- Set Internals - Intset

  13 mins

- Geospatial Queries and Geohash

  21 mins

- String Internals - Simple Dynamic Strings

  16 mins

- Hyperloglog and Cardinality Estimation

  13 mins

- LFU and Approximate Counting

  21 mins

## Why should you enroll?

The primary objective of this program is to make you fall in love
with database internals, help you understand the various components
of any database, and walk you through on building your own database
from scratch.

### Know the internals

Learning how a database is built from scratch and key design decisions that make it special has its own thrill.

### Know the unknowns

Learn some of the most interesting concepts and super-clever algorithms that makes Redis extra-special.

### Be a better engineer

Take yourself to the next level and become a better, an Asli Engineer.

### Doubt Resolution

Get your doubts cleared asynchronously on Discord or synchronously over a common 30-minute zoom call that would happen every 14 days.

### Network community

Network, interact, and learn by being a part of the Asli Engineering Discord community.

## Glimpse of the course

The first few videos of this course are available for free, watch them
here and then decide if you want to take this up or not

[![Course Introduction - Redis Internals by Arpit Bhayani](https://i.ytimg.com/vi/NXbOVLpage0/maxresdefault.jpg)](https://www.youtube.com/watch?v=NXbOVLpage0)

[![What makes Redis Special - Redis Internals by Arpit Bhayani](https://i.ytimg.com/vi/h30k7YixrMo/maxresdefault.jpg)](https://www.youtube.com/watch?v=h30k7YixrMo)

[![Writing a simple TCP echo server - Redis Internals by Arpit Bhayani](https://i.ytimg.com/vi/zlxdX9f4l50/maxresdefault.jpg)](https://www.youtube.com/watch?v=zlxdX9f4l50)

## Program pre-requisites

In this course, we would be reimplementing the core features of Redis,
like event loop, serialization protocol, persistence, pipelining, and
eviction; hence we would need a basic familiarity with Go. We will also
be writing our own Event Loop and because every OS exposes its own set
of System Calls, we will be restricting ourselves to Linux, and hence
having a Linux-based development environment is essential to follow. In
a gist, the program pre-requisites are

- basic familiarity with Go
- linux-based development environment
- a Google account because I only support Sign-in with Google :)

If you meet the pre-requisites and are excited to learn, enroll ↓

Buy Now

₹14,999
$199
============

inclusive of all the taxes

YOU'LL GET

✔
9 hours of Redis Internals

✔
Curated resources to explore internals better

✔
Source code of our Golang based implementation - Dice DB

✔
Bi-weekly doubt solving - Tuesday 7:30 pm to 8:00 pm IST

✔
Lifetime access to all the course videos and notes

✔
Lifetime access to the Discord Community

✔
Language of communication will be strictly english

✔
3 days no-questions-asked refund policy

[Get it reimbursed by your employer](/expense)

[Enroll Now
→](/redis-internals/pay)

International folks, can also [pay in USD](https://rzp.io/l/G0iw77L) or [pay in EUR](https://rzp.io/l/n4BeaCUcY).
In case you want to pay in any other currency, just drop me an
email at arpit.masterclass@gmail.com.

Note: there is no discount on the course pricing; I feel
discounts are unfair to the folks who paid the full; the price
of the course is subject to a yearly hike.

If you have questions or need any clarifications before enrolling,
please reach out to me at
[arpit.masterclass@gmail.com](mailto:arpit.masterclass@gmail.com).

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

# Hey, I am Arpit

### curious, tinkerer, and explorer

I am a software engineer and an engineering leader passionate about
System Architecture, Databases Internals, Language Internals, and
Advanced Algorithms. I am the creator and maintainer of [DiceDB](https://github.com/dicedb/dice)

- an open source, reactive, multi-tenant, cache optimized for modern
  hardware.

In 2024, I took the leap of faith and co-founded [Profile.fyi](https://profile.fyi), where I took care of all things tech and product. Before this, I
was a Staff Engineer at
[Google](https://cloud.google.com/) leading
the
[Dataproc](https://cloud.google.com/dataproc) India team in providing a managed big data ecosystem to GCP customers.
I was also part of [Amazon's](https://www.amazon.com/)
Fast Data Team and took care of cold tiering of hot data and providing
a seamless query interface across all tiers.

I held engineering leadership positions (both IC and management) at
[Unacademy](https://unacademy.com/),
where I built, grew, and led Search, Site Reliability Engineering
(SRE) teams, and Data Engineering teams. I hold a total of 10+ years
of experience in scaling backend services, taking products and teams
from 0 to 1, and beyond.

I keep diving deep into engineering details and share my learnings
by across my
[socials](https://twitter.com/arpit_bhayani) and videos on
[YouTube](https://youtube.com/c/ArpitBhayani).

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

## Frequently asked questions

You can always drop me an email at
arpit.masterclass@gmail.com
for other questions.

#### Is this course right for me?

This course is for any engineer who wants to understand the internals of Redis and build a KV store from scratch.

#### Will the sessions happen live?

The course is self-paced and upon the purchase, the access to the videos will be given.

#### Will you be resolving the doubts?

All the doubts will be resolved synchronously over a 30 min common zoom call that would happen every two weeks on Tuesday 7:30 pm IST, or asynchronously on the Discord server.

#### Can I get this course reimbursed from my company?

Talk to your manager and check if they can sponsor this course. The invoice that will be issued is a legally valid and sound invoice that can be used for any kind of reimbursement.

#### Will there be a recording available for future reference?

You will have lifetime access to the course.

#### Will I be able to change the cohort?

I will try but cannot guarantee to accomodate you in another cohort.

#### Is there a refund policy?

3 days no-questions-asked refund window from the date of course purchase.

#### What are the programming pre-requisites?

Basic familiarity with Golang and a Linux-based development environment.

#### Will I be getting an invoice of Payment?

Yes. An invoice will be issued to you with all the legal and necessary details. This means your employer can choose to process this invoice and provide reimbursement. Drop me an email at arpit.masterclass@gmail.com if you want a detailed invoice.

#### Will you issue a course completion certificate?

I generate the course completion certificate on-demand and hence if you seek one drop me an email at arpit.masterclass@gmail.com

#### Will you implement every single Redis command?

No. The commands and features that will be implemented are mentioned in the curriculum. Core Data Structures and Algorithms of Redis are theoretically covered in extreme detail. We will be following the aforementioned curriculum.

#### Will you live code?

No. I will be giving an exhaustive code walkthrough of our Golang based implementation and the code will evolve with every commit.

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/about-arpit-bhayani

About | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

# Hey, I am Arpit

### curious, tinkerer, and explorer

I am a software engineer and an engineering leader passionate about
System Architecture, Databases Internals, Language Internals, and
Advanced Algorithms. I am the creator and maintainer of [DiceDB](https://github.com/dicedb/dice)

- an open source, reactive, multi-tenant, cache optimized for modern
  hardware.

In 2024, I took the leap of faith and co-founded [Profile.fyi](https://profile.fyi), where I took care of all things tech and product. Before this, I
was a Staff Engineer at
[Google](https://cloud.google.com/) leading
the
[Dataproc](https://cloud.google.com/dataproc) India team in providing a managed big data ecosystem to GCP customers.
I was also part of [Amazon's](https://www.amazon.com/)
Fast Data Team and took care of cold tiering of hot data and providing
a seamless query interface across all tiers.

I held engineering leadership positions (both IC and management) at
[Unacademy](https://unacademy.com/),
where I built, grew, and led Search, Site Reliability Engineering
(SRE) teams, and Data Engineering teams. I hold a total of 10+ years
of experience in scaling backend services, taking products and teams
from 0 to 1, and beyond.

I keep diving deep into engineering details and share my learnings
by across my
[socials](https://twitter.com/arpit_bhayani) and videos on
[YouTube](https://youtube.com/c/ArpitBhayani).

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

![Arpit Bhayani](https://edge.arpitbhayani.me/img/arpit-5.jpg)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/terms-conditions

Terms and Conditions | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

# Terms and Conditions

Last updated: Jul 08, 2024

These terms and conditions outline the rules and regulations for the use
of arpitbhayani.me's Website, located at https://arpitbhayani.me, and
affiliated under Relog Deeptech Pvt. Ltd. The website is goverened by
Indian laws.

By accessing this website we assume you accept these terms and conditions.
Do not continue to use arpitbhayani.me if you do not agree to take all of
the terms and conditions stated on this page.

The following terminology applies to these Terms and Conditions, Privacy
Statement and Disclaimer Notice and all Agreements: "Client", "You" and
"Your" refers to you, the person log on this website and compliant to the
Company’s terms and conditions. "The Company", "Ourselves", "We", "Our"
and "Us", refers to our Company. "Party", "Parties", or "Us", refers to
both the Client and ourselves. All terms refer to the offer, acceptance
and consideration of payment necessary to undertake the process of our
assistance to the Client in the most appropriate manner for the express
purpose of meeting the Client’s needs in respect of provision of the
Company’s stated services, in accordance with and subject to, prevailing
law of Netherlands. Any use of the above terminology or other words in the
singular, plural, capitalization and/or he/she or they, are taken as
interchangeable and therefore as referring to same.

### **Cookies**

We employ the use of cookies. By accessing arpitbhayani.me, you agreed to
use cookies in agreement with the arpitbhayani.me's Privacy Policy.

Most interactive websites use cookies to let us retrieve the user’s
details for each visit. Cookies are used by our website to enable the
functionality of certain areas to make it easier for people visiting our
website. Some of our affiliate/advertising partners may also use cookies.

### **License**

Unless otherwise stated, arpitbhayani.me and/or its licensors own the
intellectual property rights for all material on arpitbhayani.me. All
intellectual property rights are reserved. You may access this from
arpitbhayani.me for your own personal use subjected to restrictions set in
these terms and conditions.

You must not:

- Republish material from arpitbhayani.me
- Sell, rent or sub-license material from arpitbhayani.me
- Reproduce, duplicate or copy material from arpitbhayani.me
- Redistribute content from arpitbhayani.me

You hereby grant arpitbhayani.me a non-exclusive license to use,
reproduce, edit and authorize others to use, reproduce and edit any of
your testimonials in any and all forms, formats or media.

No use of arpitbhayani.me's logo or other artwork will be allowed for
linking absent a trademark license agreement.

### **iFrames**

Without prior approval and written permission, you may not create frames
around our Webpages that alter in any way the visual presentation or
appearance of our Website.

### **Content Liability**

### **Your Privacy**

Please read Privacy Policy

### **Reservation of Rights**

### **Disclaimer**

To the maximum extent permitted by applicable law, we exclude all
representations, warranties and conditions relating to our website and the
use of this website. Nothing in this disclaimer will:

As long as the website and the information and services on the website are
provided free of charge, we will not be liable for any loss or damage of
any nature.

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/privacy-policy

Privacy Policy | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

# Privacy Policy

Last updated: Feb 02, 2021

Courses offered by Arpit Bhayani, accessible from arpitbhayani.me,
courses.arpitbhayani.me, and learn.relog.in are covered under this Privacy
Policy. Hereforth we will be referring all of the above entity as
"arpitbhayani.me". The document contains types of information that is
collected and recorded by arpitbhayani.me and how we use it.

If you have additional questions or require more information about our
Privacy Policy, do not hesitate to contact us.

This Privacy Policy applies only to our online activities and is valid for
visitors to our website with regards to the information that they shared
and/or collect in arpitbhayani.me. This policy is not applicable to any
information collected offline or via channels other than this website.

## Consent

By using our website, you hereby consent to our Privacy Policy and agree
to its terms.

## Information we collect

The personal information that you are asked to provide, and the reasons
why you are asked to provide it, will be made clear to you at the point we
ask you to provide your personal information.

If you contact us directly, we may receive additional information about
you such as your name, email address, phone number, the contents of the
message and/or attachments you may send us, and any other information you
may choose to provide.

When you register for an Account, we may ask for your contact information,
including items such as name, company name, address, email address, and
telephone number.

## How we use your information

We use the information we collect in various ways, including to:

- Provide, operate, and maintain our website
- Improve, personalize, and expand our website
- Understand and analyze how you use our website
- Develop new products, services, features, and functionality
- Communicate with you, either directly or through one of our partners,
  including for customer service, to provide you with updates and other
  information relating to the website, and for marketing and promotional
  purposes
- Send you emails
- Find and prevent fraud

## Log Files

arpitbhayani.me follows a standard procedure of using log files. These
files log visitors when they visit websites. All hosting companies do this
and a part of hosting services' analytics. The information collected by
log files include internet protocol (IP) addresses, browser type, Internet
Service Provider (ISP), date and time stamp, referring/exit pages, and
possibly the number of clicks. These are not linked to any information
that is personally identifiable. The purpose of the information is for
analyzing trends, administering the site, tracking users' movement on the
website, and gathering demographic information.

## Cookies and Web Beacons

Like any other website, arpitbhayani.me uses 'cookies'. These cookies are
used to store information including visitors' preferences, and the pages
on the website that the visitor accessed or visited. The information is
used to optimize the users' experience by customizing our web page content
based on visitors' browser type and/or other information.

For more general information on cookies, please read
["What Are Cookies"](https://www.cookieconsent.com/what-are-cookies/).

## Advertising Partners Privacy Policies

You may consult this list to find the Privacy Policy for each of the
advertising partners of arpitbhayani.me. Third-party ad servers or ad
networks uses technologies like cookies, JavaScript, or Web Beacons that
are used in their respective advertisements and links that appear on
arpitbhayani.me, which are sent directly to users' browser. They
automatically receive your IP address when this occurs. These technologies
are used to measure the effectiveness of their advertising campaigns
and/or to personalize the advertising content that you see on websites
that you visit. Note that arpitbhayani.me has no access to or control over
these cookies that are used by third-party advertisers.

## Third Party Privacy Policies

arpitbhayani.me's Privacy Policy does not apply to other advertisers or
websites. Thus, we are advising you to consult the respective Privacy
Policies of these third-party ad servers for more detailed information. It
may include their practices and instructions about how to opt-out of
certain options. You can choose to disable cookies through your individual
browser options. To know more detailed information about cookie management
with specific web browsers, it can be found at the browsers' respective
websites.

## CCPA Privacy Rights (Do Not Sell My Personal Information)

Under the CCPA, among other rights, California consumers have the right
to: Request that a business that collects a consumer's personal data
disclose the categories and specific pieces of personal data that a
business has collected about consumers. Request that a business delete any
personal data about the consumer that a business has collected. Request
that a business that sells a consumer's personal data, not sell the
consumer's personal data. If you make a request, we have one month to
respond to you. If you would like to exercise any of these rights, please
contact us.

## GDPR Data Protection Rights

We would like to make sure you are fully aware of all of your data
protection rights. Every user is entitled to the following:

The right to access – You have the right to request copies of your
personal data. We may charge you a small fee for this service.

The right to rectification – You have the right to request that we correct
any information you believe is inaccurate. You also have the right to
request that we complete the information you believe is incomplete.

The right to erasure – You have the right to request that we erase your
personal data, under certain conditions.

The right to restrict processing – You have the right to request that we
restrict the processing of your personal data, under certain conditions.

The right to object to processing – You have the right to object to our
processing of your personal data, under certain conditions.

The right to data portability – You have the right to request that we
transfer the data that we have collected to another organization, or
directly to you, under certain conditions.

If you make a request, we have one month to respond to you. If you would
like to exercise any of these rights, please contact us.

## Children's Information

Another part of our priority is adding protection for children while using
the internet. We encourage parents and guardians to observe, participate
in, and/or monitor and guide their online activity.

arpitbhayani.me does not knowingly collect any Personal Identifiable
Information from children under the age of 13. If you think that your
child provided this kind of information on our website, we strongly
encourage you to contact us immediately and we will do our best efforts to
promptly remove such information from our records.

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/refund-policy

Refund Policy | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

# Refund Policy

Last updated: Jul 08, 2024

If, for any reason, You are not completely satisfied with a purchase We
invite You to review our policy on refunds and returns. Refund processing
may take upto 5-7 business days. The following terms are applicable for
any products that You purchased with Us.

### Interpretation

The words of which the initial letter is capitalized have meanings defined
under the following conditions.

### **Your Order Cancellation Rights**

- 14 days refund window from the course commencement date on System Design
  Masterclas (https://arpitbhayani.me/masterclass/), no questions asked.
- 3 days no-questions-asked refund policy for 'Learn at your own place'
  offering under System Design Masterclas
  (https://arpitbhayani.me/masterclass/)
- 3 days no-questions-asked refund policy for 'Redis Internals'
  (https://arpitbhayani.me/redis-internals/)

In order to exercise Your right of cancellation, You must inform Us of
your decision by means of a clear statement. You can inform us of your
decision by:

By email: arpit.masterclass@gmail.com

### **Contact us**

If you have any questions about our Returns and Refunds Policy, please
contact us:

By email: arpit.masterclass@gmail.com

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/contact

Get in touch | Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

### Business Email

arpit.masterclass@gmail.com

### Business Contact

+91-9652602714

### Business Address

Relog Deeptech Pvt. Ltd.

203, Sagar Apartment,

Camp Road, Mangilal Plot,

Amravati, Maharashtra, 444602

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://arpitbhayani.me/collaborate

Collaborate with Arpit Bhayani

[Arpit Bhayani](/)

[Blogs](/blogs)
[From the First Principles](/first)
[Papershelf](/papershelf)

[Courses](/courses)
[Talks](/talks)
[Collaborate](/collaborate)

# Collaborate with me

I am on a mission to bring out the best engineering stories from
around the world. If you are doing something interesting and want me
to dissect and talk about it, drop me an email at `speak.with.arpit@gmail.com`.

![](https://github.com/arpitbbhayani/articles/assets/4745789/243251e3-7b00-4c09-947f-5faf37f200e4)

![](https://github.com/arpitbbhayani/articles/assets/4745789/ffb610a7-8731-4331-9079-475f912bee6f)

![](https://github.com/arpitbbhayani/articles/assets/4745789/0bedba31-4e80-460d-9d71-94a7a02b73a7)

![](https://github.com/arpitbbhayani/articles/assets/4745789/b3557dc1-b3b5-4df7-b4a4-3fd509706615)

![](https://github.com/arpitbbhayani/articles/assets/4745789/41f656ca-30f9-4623-93dc-8bc553237542)

# Here's how I can help

I have been actively writing and sharing my learnings on social
platforms like [LinkedIn](https://www.linkedin.com/in/arpitbhayani/),
[Twitter](https://twitter.com/arpit_bhayani), and
[YouTube](https://www.youtube.com/c/ArpitBhayani) and have built a total following of **350,000+ engineers**.

I post no-fluff deep-tech content across my social handles along with
detailed videos dissecting blogs and research papers on my YouTube.
People follow me for the things I post to become better and curious
senior engineers. I can help you

- showcases your engineering prowess,
- get the eyeballs of quality engineers on your product or org,
- attract great talent.

Here are some of the ways I propose we do it

- [a five part video series](https://www.youtube.com/watch?v=Jddce8sPFsc&list=PLsdq-3Z1EPT3s3nghXpr0NDpgN3ZIoK4O) dissecting your engineering blogs, or
- [a one-off social media post](https://www.linkedin.com/posts/arpitbhayani_asliengineering-activity-7191055241314742273-w8By?utm_source=share&utm_medium=member_desktop) showcasing your engineering prowess, or
- [a deep-tech engineering conversation](https://youtu.be/yhqW82Ka-gw?si=0Dpq1W9VpVPh_BxF) (podcast) on my channel

By the way, I have already done similar engagements with multiple
companies (details below), so, if you think I can help, drop me an
email at `speak.with.arpit@gmail.com`. Always up for a
chat.

# Past collaborations

All the videos on my channel are no-fluff and my audience prefers me
doing technical deep dives like outage dissections, blog dissections,
paper readings, and deep-tech podcasts as I have a tendency to explain
the dense concepts pretty well.

To date, I have worked with [JioCinema](https://jiocinema.com/),
[Rockset](https://rockset.com/),
[CockroachDB](https://www.cockroachlabs.com/),
[Razorpay](https://razorpay.com/),
[Replit](https://replit.com/),
[CodeCraters](https://codecrafters.io/),
and [Dukaan](https://mydukaan.io/) helping them
get the quality eyeballs or showcase their engineering prowess to the community.
Here are some of the videos podcasts I did

- [How JioCinema live streams IPL to 20M concurrent devices w/
  Prachi Sharma](https://www.youtube.com/watch?v=36N1Bz7qW0A)
- [How Dukaan moved their infra to bare metal (Hindi)](https://youtu.be/vFxQyZX84Ro)
- [Everything you need to know about CockroachDB w/ Ben Darnell](https://youtu.be/yhqW82Ka-gw?si=n1yGPVGcz40aF9HQ)
- [Razorpay's journey to microservices](https://youtu.be/yqkyq8TPWbg)

I have also dissected and explained engineering blogs and papers from
various companies on my YouTube. Here are some of them

- [How Reddit designed their metadata store to serve 100k req/sec at
  p99 of 17ms](https://www.youtube.com/watch?v=b9zh0SFbkqM)
- [How Airbnb designed and scaled its central authorization system -
  Himeji](https://youtube.com/watch?v=5FIPtC3xJSQ)
- [How Booking com designed and scaled their highly available and
  performant User Review System](https://youtube.com/watch?v=BFyWl9MNDjY)
- [The architecture of Grab's data layer that serves millions of
  orders every day](https://youtube.com/watch?v=KeV4erIm47o)
- [... and many more.](https://www.youtube.com/playlist?list=PLsdq-3Z1EPT0RrDebPvBNlmuXDfT6Qs2T)

So, if this sounds exciting and you think I can bring value to you,
drop me an email at `speak.with.arpit@gmail.com`
or reach out on my socials - [LinkedIn](https://www.linkedin.com/in/arpitbhayani/) and
[Twitter](https://twitter.com/arpit_bhayani).

[shoot me an email →](mailto:speak.with.arpit@gmail.com)

Writings and Learnings

[Blogs](/blogs)

[Knowledge Base](/knowledge-base)

[Bookshelf](/bookshelf)

[Papershelf](/papershelf)

Courses

[System Design Masterclass](/masterclass)

[System Design for Beginners](/system-design-for-beginners)

[Redis Internals](/redis-internals)

Legal and Contact

[About me](/about-arpit-bhayani)

[Terms and Conditions](/terms-conditions)

[Privacy Policy](/privacy-policy)

[Refund Policy](/refund-policy)

[Contact Me](/contact)

Everything Else

[Collaborate with me](/collaborate)

[DiceDB](https://github.com/DiceDB/Dice)

[Revine](https://revine.arpitbhayani.me)

[The Smarter Chimp](https://twitter.com/TheSmarterChimp)

Arpit's Newsletter
read by 100,000 engineers

Weekly essays on real-world system design, distributed systems, or a
deep dive into some super-clever algorithm.

[Subscribe on LinkedIn](https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/)
[Subscribe on Substack](https://arpit.substack.com)

[YouTube
YouTube (135k)](https://youtube.com/c/ArpitBhayani "Arpit Bhayani")
[Twitter
Twitter (70k)](https://twitter.com/arpit_bhayani "Follow Arpit Bhayani on Twitter")
[Linkedin
LinkedIn (200k)](https://linkedin.com/in/arpitbhayani "Follow Arpit Bhayani on LinkedIn")
[GitHub
GitHub (5k)](https://github.com/arpitbbhayani "Follow Arpit Bhayani on GitHub")

- © Arpit Bhayani, 2024

# https://github.com/DiceDB/Dice

GitHub - DiceDB/dice: DiceDB is an open source, redis-compliant, reactive, scalable, highly-available, unified cache optimized for modern hardware.

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FDiceDB%2FDice)

- Product

  - [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  - [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  - [Actions
    Automate any workflow](https://github.com/features/actions)
  - [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  - [Issues
    Plan and track work](https://github.com/features/issues)
  - [Code Review
    Manage code changes](https://github.com/features/code-review)
  - [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  - [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore

  - [All features](https://github.com/features)
  - [Documentation](https://docs.github.com)
  - [GitHub Skills](https://skills.github.com)
  - [Blog](https://github.blog)

- Solutions

  By company size

  - [Enterprises](https://github.com/enterprise)
  - [Small and medium teams](https://github.com/team)
  - [Startups](https://github.com/enterprise/startups)
  - [Nonprofits](/solutions/industry/nonprofits)
    By use case
  - [DevSecOps](/solutions/use-case/devsecops)
  - [DevOps](/solutions/use-case/devops)
  - [CI/CD](/solutions/use-case/ci-cd)
  - [View all use cases](/solutions/use-case)

  By industry

  - [Healthcare](/solutions/industry/healthcare)
  - [Financial services](/solutions/industry/financial-services)
  - [Manufacturing](/solutions/industry/manufacturing)
  - [Government](/solutions/industry/government)
  - [View all industries](/solutions/industry)

  [View all solutions](/solutions)

- Resources

  Topics

  - [AI](/resources/articles/ai)
  - [DevOps](/resources/articles/devops)
  - [Security](/resources/articles/security)
  - [Software Development](/resources/articles/software-development)
  - [View all](/resources/articles)

  Explore

  - [Learning Pathways](https://resources.github.com/learn/pathways)
  - [White papers, Ebooks, Webinars](https://resources.github.com)
  - [Customer Stories](https://github.com/customer-stories)
  - [Partners](https://partner.github.com)
  - [Executive Insights](https://github.com/solutions/executive-insights)

- Open Source

  - [GitHub Sponsors
    Fund open source developers](/sponsors)
  - [The ReadME Project
    GitHub community articles](https://github.com/readme)
    Repositories
  - [Topics](https://github.com/topics)
  - [Trending](https://github.com/trending)
  - [Collections](https://github.com/collections)

- Enterprise

  - [Enterprise platform
    AI-powered developer platform](/enterprise)
    Available add-ons
  - [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  - [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  - [Premium Support
    Enterprise-grade 24/7 support](/premium-support)

- [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

Cancel

Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

Cancel

Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FDiceDB%2FDice)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=DiceDB%2Fdice)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[DiceDB](/DiceDB)
/
**[dice](/DiceDB/dice)**
Public

- [Notifications](/login?return_to=%2FDiceDB%2Fdice) You must be signed in to change notification settings
- [Fork
  1.2k](/login?return_to=%2FDiceDB%2Fdice)
- [Star
  7.6k](/login?return_to=%2FDiceDB%2Fdice)

DiceDB is an open source, redis-compliant, reactive, scalable, highly-available, unified cache optimized for modern hardware.

[dicedb.io/](https://dicedb.io/ "https://dicedb.io/")

### License

[AGPL-3.0 license](/DiceDB/dice/blob/master/LICENSE)

[7.6k
stars](/DiceDB/dice/stargazers) [1.2k
forks](/DiceDB/dice/forks) [Branches](/DiceDB/dice/branches) [Tags](/DiceDB/dice/tags) [Activity](/DiceDB/dice/activity)
[Star](/login?return_to=%2FDiceDB%2Fdice)

[Notifications](/login?return_to=%2FDiceDB%2Fdice) You must be signed in to change notification settings

- [Code](/DiceDB/dice)
- [Issues
  85](/DiceDB/dice/issues)
- [Pull requests
  35](/DiceDB/dice/pulls)
- [Discussions](/DiceDB/dice/discussions)
- [Actions](/DiceDB/dice/actions)
- [Projects
  1](/DiceDB/dice/projects)
- [Wiki](/DiceDB/dice/wiki)
- [Security](/DiceDB/dice/security)
- [Insights](/DiceDB/dice/pulse)

Additional navigation options

- [Code](/DiceDB/dice)
- [Issues](/DiceDB/dice/issues)
- [Pull requests](/DiceDB/dice/pulls)
- [Discussions](/DiceDB/dice/discussions)
- [Actions](/DiceDB/dice/actions)
- [Projects](/DiceDB/dice/projects)
- [Wiki](/DiceDB/dice/wiki)
- [Security](/DiceDB/dice/security)
- [Insights](/DiceDB/dice/pulse)

# DiceDB/dice

master[Branches](/DiceDB/dice/branches)[Tags](/DiceDB/dice/tags)Go to fileCode

## Folders and files

| Name                                                                                                  |     | Name                                                                                                  | Last commit message | Last commit date |
| ----------------------------------------------------------------------------------------------------- | --- | ----------------------------------------------------------------------------------------------------- | ------------------- | ---------------- |
| Latest commit History[1,048 Commits](/DiceDB/dice/commits/master/)                                    |     |                                                                                                       |
| [.devcontainer](/DiceDB/dice/tree/master/.devcontainer ".devcontainer")                               |     | [.devcontainer](/DiceDB/dice/tree/master/.devcontainer ".devcontainer")                               |                     |                  |
| [.github](/DiceDB/dice/tree/master/.github ".github")                                                 |     | [.github](/DiceDB/dice/tree/master/.github ".github")                                                 |                     |                  |
| [CONTRIBUTING](/DiceDB/dice/tree/master/CONTRIBUTING "CONTRIBUTING")                                  |     | [CONTRIBUTING](/DiceDB/dice/tree/master/CONTRIBUTING "CONTRIBUTING")                                  |                     |                  |
| [bin](/DiceDB/dice/tree/master/bin "bin")                                                             |     | [bin](/DiceDB/dice/tree/master/bin "bin")                                                             |                     |                  |
| [config](/DiceDB/dice/tree/master/config "config")                                                    |     | [config](/DiceDB/dice/tree/master/config "config")                                                    |                     |                  |
| [docs](/DiceDB/dice/tree/master/docs "docs")                                                          |     | [docs](/DiceDB/dice/tree/master/docs "docs")                                                          |                     |                  |
| [examples](/DiceDB/dice/tree/master/examples "examples")                                              |     | [examples](/DiceDB/dice/tree/master/examples "examples")                                              |                     |                  |
| [integration_tests](/DiceDB/dice/tree/master/integration_tests "integration_tests")                   |     | [integration_tests](/DiceDB/dice/tree/master/integration_tests "integration_tests")                   |                     |                  |
| [internal](/DiceDB/dice/tree/master/internal "internal")                                              |     | [internal](/DiceDB/dice/tree/master/internal "internal")                                              |                     |                  |
| [mocks](/DiceDB/dice/tree/master/mocks "mocks")                                                       |     | [mocks](/DiceDB/dice/tree/master/mocks "mocks")                                                       |                     |                  |
| [pulumi](/DiceDB/dice/tree/master/pulumi "pulumi")                                                    |     | [pulumi](/DiceDB/dice/tree/master/pulumi "pulumi")                                                    |                     |                  |
| [testutils](/DiceDB/dice/tree/master/testutils "testutils")                                           |     | [testutils](/DiceDB/dice/tree/master/testutils "testutils")                                           |                     |                  |
| [.air.toml](/DiceDB/dice/blob/master/.air.toml ".air.toml")                                           |     | [.air.toml](/DiceDB/dice/blob/master/.air.toml ".air.toml")                                           |                     |                  |
| [.gitignore](/DiceDB/dice/blob/master/.gitignore ".gitignore")                                        |     | [.gitignore](/DiceDB/dice/blob/master/.gitignore ".gitignore")                                        |                     |                  |
| [.golangci.yaml](/DiceDB/dice/blob/master/.golangci.yaml ".golangci.yaml")                            |     | [.golangci.yaml](/DiceDB/dice/blob/master/.golangci.yaml ".golangci.yaml")                            |                     |                  |
| [.goreleaser.yaml](/DiceDB/dice/blob/master/.goreleaser.yaml ".goreleaser.yaml")                      |     | [.goreleaser.yaml](/DiceDB/dice/blob/master/.goreleaser.yaml ".goreleaser.yaml")                      |                     |                  |
| [.pre-commit-config.yaml](/DiceDB/dice/blob/master/.pre-commit-config.yaml ".pre-commit-config.yaml") |     | [.pre-commit-config.yaml](/DiceDB/dice/blob/master/.pre-commit-config.yaml ".pre-commit-config.yaml") |                     |                  |
| [CODE_OF_CONDUCT.md](/DiceDB/dice/blob/master/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md")                |     | [CODE_OF_CONDUCT.md](/DiceDB/dice/blob/master/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md")                |                     |                  |
| [DOC.md](/DiceDB/dice/blob/master/DOC.md "DOC.md")                                                    |     | [DOC.md](/DiceDB/dice/blob/master/DOC.md "DOC.md")                                                    |                     |                  |
| [Dockerfile](/DiceDB/dice/blob/master/Dockerfile "Dockerfile")                                        |     | [Dockerfile](/DiceDB/dice/blob/master/Dockerfile "Dockerfile")                                        |                     |                  |
| [LICENSE](/DiceDB/dice/blob/master/LICENSE "LICENSE")                                                 |     | [LICENSE](/DiceDB/dice/blob/master/LICENSE "LICENSE")                                                 |                     |                  |
| [Makefile](/DiceDB/dice/blob/master/Makefile "Makefile")                                              |     | [Makefile](/DiceDB/dice/blob/master/Makefile "Makefile")                                              |                     |                  |
| [README.md](/DiceDB/dice/blob/master/README.md "README.md")                                           |     | [README.md](/DiceDB/dice/blob/master/README.md "README.md")                                           |                     |                  |
| [build_protos.sh](/DiceDB/dice/blob/master/build_protos.sh "build_protos.sh")                         |     | [build_protos.sh](/DiceDB/dice/blob/master/build_protos.sh "build_protos.sh")                         |                     |                  |
| [dicedb.conf](/DiceDB/dice/blob/master/dicedb.conf "dicedb.conf")                                     |     | [dicedb.conf](/DiceDB/dice/blob/master/dicedb.conf "dicedb.conf")                                     |                     |                  |
| [go.mod](/DiceDB/dice/blob/master/go.mod "go.mod")                                                    |     | [go.mod](/DiceDB/dice/blob/master/go.mod "go.mod")                                                    |                     |                  |
| [go.sum](/DiceDB/dice/blob/master/go.sum "go.sum")                                                    |     | [go.sum](/DiceDB/dice/blob/master/go.sum "go.sum")                                                    |                     |                  |
| [main.go](/DiceDB/dice/blob/master/main.go "main.go")                                                 |     | [main.go](/DiceDB/dice/blob/master/main.go "main.go")                                                 |                     |                  |
| [runtest](/DiceDB/dice/blob/master/runtest "runtest")                                                 |     | [runtest](/DiceDB/dice/blob/master/runtest "runtest")                                                 |                     |                  |
| View all files                                                                                        |     |                                                                                                       |

## Repository files navigation

- [README](#)
- [Code of conduct](#)
- [AGPL-3.0 license](#)

# DiceDB

[![slatedb.io](https://camo.githubusercontent.com/2ea1932d2aca716022aa5380033cf9b43d6771344bbca1f2031145b3075327e0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736974652d6469636564622e696f2d3030413146463f7374796c653d666c61742d737175617265)](https://dicedb.io)
[![Docs](https://camo.githubusercontent.com/117fe163a6950a756a1b32d57e34f952f53ad6cb4730930f6d1d417a1fd83ff2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d3030413146463f7374796c653d666c61742d737175617265)](https://dicedb.io/get-started/installation/)
[![discord community](https://camo.githubusercontent.com/c32f29403a966a62cf366e8a5e66f868beebd5dc7e6fde3b05d1ef0bc4298ae2/68747470733a2f2f646362616467652e6c696d65732e70696e6b2f6170692f7365727665722f367238755857745868373f7374796c653d666c6174)](https://discord.gg/6r8uXWtXh7)

DiceDB is an open source, redis-compliant, reactive, scalable, highly available, unified cache optimized for modern hardware.

Caution

DiceDB is under active development and supports a subset of Redis commands. Inconsistent behavior is expected. Feel free to go through the [open issues](https://github.com/DiceDB/dice/issues) and contribute to help us speed up the development.

## Want to contribute?

We have multiple repositories where you can contribute. So, as per your interest, you can pick one and build a deeper understanding of the project on the go.

- [dicedb/dice](https://github.com/dicedb/dice) for core database features and engine / Stack - Go
- [dicedb/dicedb-cli](https://github.com/dicedb/dicedb-cli) command line interface for DiceDB / Stack - Go
- [dicedb/playground-mono](https://github.com/dicedb/playground-mono) backend APIs for DiceDB playground / Stack - Go
- [dicedb/alloy](https://github.com/dicedb/alloy) frontend and marketplace for DiceDB playground / Stack - NextJS

## Get started

### Setting up DiceDB with Docker

The easiest way to get started with DiceDB is using [Docker](https://www.docker.com/) by running the following command.

```
docker run -p 7379:7379 dicedb/dicedb --enable-watch
```

The above command will start the DiceDB server running locally on the port `7379` and you can connect
to it using [DiceDB CLI](https://github.com/DiceDB/dicedb-cli) and SDKs.

Tip

Since DiceDB is a drop-in replacement for Redis, you can also use any Redis CLI and SDK to connect to DiceDB.

## Supporters

[![JetBrains logo.](https://camo.githubusercontent.com/10b97e557d8d32622250903852c1b2db59c5d29d431bc3b2a0e55301219150b7/68747470733a2f2f7265736f75726365732e6a6574627261696e732e636f6d2f73746f726167652f70726f64756374732f636f6d70616e792f6272616e642f6c6f676f732f6a6574627261696e732e737667)](https://jb.gg/OpenSourceSupport)

### Setting up DiceDB from source for development and contributions

To run DiceDB for local development or running from source, you will need

1. [Golang](https://go.dev/)
2. Any of the below supported platform environments:
   1. [Linux based environment](https://en.wikipedia.org/wiki/Comparison_of_Linux_distributions)
   2. [OSX (Darwin) based environment](https://en.wikipedia.org/wiki/MacOS)
   3. WSL under Windows

```
git clone https://github.com/dicedb/dice
cd dice
go run main.go --enable-watch
```

You can skip passing the flag if you are not working with `.WATCH` feature.

1. Install GoLangCI

```
sudo su
curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b /bin v1.60.1
```

### Live Development Server

DiceDB provides a hot-reloading development environment, which allows you to instantly view your code changes in a live server. This functionality is supported by [Air](https://github.com/air-verse/air)

To Install Air on your system you have the following options.

1. If you're on go 1.22+

```
go install github.com/air-verse/air@latest
```

1. Install the Air binary

```
# binary will be installed at $(go env GOPATH)/bin/air
curl -sSfL https://raw.githubusercontent.com/air-verse/air/master/install.sh | sh -s -- -b $(go env GOPATH)/bin
```

Once `air` is installed you can verify the installation using the command `air -v`

To run the live DiceDB server for local development:

```
git clone https://github.com/dicedb/dice
cd dice
air
```

### Local Setup with Custom Config

By default, DiceDB will look for the configuration file at `./dicedb.conf`. (Linux, Darwin, and WSL)

Tip

If you want to use a custom configuration file, you can specify the path using the `-c` flag. and to output the configuration file to a specific location, you can specify the output dir path using the `-o` flag.

### Setting up CLI

The best way to connect to DiceDB is using DiceDB CLI and you can install it by running the following command

```
sudo su
curl -sL https://raw.githubusercontent.com/DiceDB/dicedb-cli/refs/heads/master/install.sh | sh
```

### Client Compatibility

DiceDB is fully compatible with Redis protocol, allowing you to connect using any existing Redis client or SDK.

Note

The `.WATCH` feature is only accessible through the DiceDB CLI.
If you are working on unsupported OS (as per above script), you can always follow the installation instructions mentioned in the [dicedb/cli](https://github.com/DiceDB/dicedb-cli) repository.

### Running Tests

Unit tests and integration tests are essential for ensuring correctness and in the case of DiceDB, both types of tests are available to validate its functionality.

For unit testing, you can execute individual unit tests by specifying the name of the test function using the `TEST_FUNC` environment variable and running the `make unittest-one` command. Alternatively, running `make unittest` will execute all unit tests.

### Executing one unit test

```
TEST_FUNC=<name of the test function> make unittest-one
TEST_FUNC=TestByteList make unittest-one
```

### Running all unit tests

```
make unittest
```

Integration tests, on the other hand, involve starting up the DiceDB server and running a series of commands to verify the expected end state and output. To execute a single integration test, you can set the `TEST_FUNC` environment variable to the name of the test function and run `make test-one`. Running `make test` will execute all integration tests.

### Executing a single integration test

```
TEST_FUNC=<name of the test function> make test-one
TEST_FUNC=TestSet make test-one
```

### Running all integration tests

```
make test
```

> Work to add more tests in DiceDB is in progress, and we will soon port the
> test [Redis suite](https://github.com/redis/redis/tree/f60370ce28b946c1146dcea77c9c399d39601aaa) to this codebase to ensure full compatibility.

## Running Benchmark

```
make run_benchmark
```

## Getting Started

To get started with building and contributing to DiceDB, please refer to the [issues](https://github.com/DiceDB/dice/issues) created in this repository.

## Docs

We use [Astro](https://astro.build/) framework to power the [dicedb.io website](https://dicedb.io) and [Starlight](https://starlight.astro.build/) to power the docs. Once you have NodeJS installed, fire the following commands to get your local version of [dicedb.io](https://dicedb.io) running.

```
cd docs
npm install
npm run dev
```

Once the server starts, visit <http://localhost:4321/> in your favourite browser. This runs with a hot reload which means any changes you make in the website and the documentation can be instantly viewed on the browser.

### Docs directory structure

1. `docs/src/content/docs/commands` is where all the commands are documented
2. `docs/src/content/docs/tutorials` is where all the tutorials are documented

## The Story

DiceDB started as a re-implementation of Redis in Golang with the idea of building a DB from scratch to understand the micro-nuances that come with its implementation. DiceDB isn’t just another database; it’s a platform purpose-built for the real-time era. As real-time systems become increasingly prevalent in modern applications, DiceDB’s hyper-optimized architecture is positioned to power the next generation of user experiences.

## How to contribute

The Code Contribution Guidelines are published at [CONTRIBUTING/README.md](/DiceDB/dice/blob/master/CONTRIBUTING/README.md); please read them before you start making any changes. This would allow us to have a consistent standard of coding practices and developer experience.

Contributors can join the [Discord Server](https://discord.gg/6r8uXWtXh7) for quick collaboration.

## Contributors

[![](https://camo.githubusercontent.com/8b2cef69a98197378e326b509d66a47a351ba0aaf8cf1cfafb8e2dcea73dc361/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d6469636564622f64696365)](https://github.com/dicedb/dice/graphs/contributors)

## Troubleshoot

### Forcefully killing the process

```
$ sudo netstat -atlpn | grep :7379
$ sudo kill -9 <process_id>

```

## About

DiceDB is an open source, redis-compliant, reactive, scalable, highly-available, unified cache optimized for modern hardware.

[dicedb.io/](https://dicedb.io/ "https://dicedb.io/")

### Topics

[golang](/topics/golang "Topic: golang")
[database](/topics/database "Topic: database")
[storage-engine](/topics/storage-engine "Topic: storage-engine")
[hacktoberfest](/topics/hacktoberfest "Topic: hacktoberfest")

### Resources

[Readme](#readme-ov-file)

### License

[AGPL-3.0 license](#AGPL-3.0-1-ov-file)

### Code of conduct

[Code of conduct](#coc-ov-file)

[Activity](/DiceDB/dice/activity)
[Custom properties](/DiceDB/dice/custom-properties)

### Stars

[**7.6k**
stars](/DiceDB/dice/stargazers)

### Watchers

[**113**
watching](/DiceDB/dice/watchers)

### Forks

[**1.2k**
forks](/DiceDB/dice/forks)
[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FDiceDB%2Fdice&report=DiceDB+%28user%29)

[Contributors
184](/DiceDB/dice/graphs/contributors)

---

[+ 170 contributors](/DiceDB/dice/graphs/contributors)

## Languages

- [Go
  99.3%](/DiceDB/dice/search?l=go)
- Other
  0.7%

## Footer

© 2025 GitHub, Inc.

### Footer navigation

- [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
- [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
- [Security](https://github.com/security)
- [Status](https://www.githubstatus.com/)
- [Docs](https://docs.github.com/)
- [Contact](https://support.github.com?tags=dotcom-footer)
- Manage cookies
- Do not share my personal information

You can’t perform that action at this time.

# https://revine.arpitbhayani.me

Revine | A programming language for kids

[![](https://user-images.githubusercontent.com/4745789/136668765-1b9d1f2f-708b-4447-a043-699d1eab9098.png)](/) [Docs](/docs) [Showcase](/showcase) [Project](/project) [Play](/play)  
 ![](https://user-images.githubusercontent.com/4745789/136668663-911365b9-ca60-4f36-be44-b6c80dbf485f.png)

Revine is an educational programming language that makes it fun to
build
animations
and
artwork.
The simplicity of the language makes it an excellent choice for kids
to learn computer programming.

[Goto Playground](/play) [Tutorial](/docs)

Try Revine [Open playground](/play)

Follow us on twitter
[@RevineLang](https://twitter.com/@RevineLang)

Built by
[Arpit Bhayani](https://arpitbhayani.me)

# https://twitter.com/TheSmarterChimp

x.com

# https://www.linkedin.com/newsletters/asli-engineering-6921728898456072192/

Arpit's Newsletter | LinkedIn

[LinkedIn](/?trk=news-guest_nav-header-logo)
Mumbai

Expand search

- Jobs
- People
- Learning

Dismiss

Dismiss

Dismiss

Dismiss

Dismiss

[Join now](https://www.linkedin.com/signup/cold-join?trk=news-guest_nav-header-join)
[Sign in](https://www.linkedin.com/uas/login?fromSignIn=true&trk=news-guest_nav-header-signin)

[Skip to main content](#main-content)

![Arpit's Newsletter]()

# Arpit's Newsletter

## A newsletter to help software engineers become better engineers and grow faster in their careers.

### Newsletter Published weekly

[Join to Subscribe](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_top-card-primary-button)

- [Report this newsletter](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_ellipsis-menu-sign-in-redirect)

Created by

![Arpit Bhayani]()

### [Arpit Bhayani](https://www.linkedin.com/in/arpitbhayani?trk=news-guest-profile-name)

[View profile](https://www.linkedin.com/in/arpitbhayani?trk=news-guest-view-profile)

## Editions

- [Report this post](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_ellipsis-menu-sign-in-redirect)

![Doubt yourself every day]()

### [Doubt yourself every day](https://www.linkedin.com/pulse/doubt-yourself-every-day-arpit-bhayani-2gqvc?trk=news-guest_share-article)

#### Arpit Bhayani on LinkedIn

[3 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_social-action-counts_comments-text)

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_like-cta)
[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_comment-cta)
[Share](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_share-cta)

- [Report this post](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_ellipsis-menu-sign-in-redirect)

![Not everything needs to be dumbed down]()

### [Not everything needs to be dumbed down](https://www.linkedin.com/pulse/everything-needs-dumbed-down-arpit-bhayani-fn3zc?trk=news-guest_share-article)

#### Arpit Bhayani on LinkedIn

[11 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_social-action-counts_comments-text)

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_like-cta)
[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_comment-cta)
[Share](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_share-cta)

- [Report this post](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_ellipsis-menu-sign-in-redirect)

![The best resource does not exist.]()

### [The best resource does not exist.](https://www.linkedin.com/pulse/best-resource-does-exist-arpit-bhayani-pmnic?trk=news-guest_share-article)

#### Arpit Bhayani on LinkedIn

[4 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_social-action-counts_comments-text)

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_like-cta)
[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_comment-cta)
[Share](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_share-cta)

- [Report this post](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_ellipsis-menu-sign-in-redirect)

![It's not about what you know, but about how you think]()

### [It's not about what you know, but about how you think](https://www.linkedin.com/pulse/its-what-you-know-how-think-arpit-bhayani-we1hc?trk=news-guest_share-article)

#### Arpit Bhayani on LinkedIn

[1 Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_social-action-counts_comments-text)

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_like-cta)
[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_comment-cta)
[Share](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_share-cta)

- [Report this post](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_ellipsis-menu-sign-in-redirect)

![Roadmaps are just satisfying your urge to follow a syllabus]()

### [Roadmaps are just satisfying your urge to follow a syllabus](https://www.linkedin.com/pulse/roadmaps-just-satisfying-your-urge-follow-syllabus-arpit-bhayani-kp65c?trk=news-guest_share-article)

#### Arpit Bhayani on LinkedIn

[3 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_social-action-counts_comments-text)

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_like-cta)
[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_comment-cta)
[Share](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fnewsletters%2Farpit-s-newsletter-6921728898456072192&trk=news-guest_share-update_social-details_share-cta)

### More from this author

[![]()

### Always negotiate the offer you get

5mo](https://www.linkedin.com/pulse/always-negotiate-offer-you-get-arpit-bhayani-nvftc?trk=news-guest)
[![]()

### Proving your Culture Fit

6mo](https://www.linkedin.com/pulse/proving-your-culture-fit-arpit-bhayani-7crpc?trk=news-guest)
[![]()

### Premature Abstractions

6mo](https://www.linkedin.com/pulse/premature-abstractions-arpit-bhayani-ijmvc?trk=news-guest)

- LinkedIn

  © 2025

- [About](https://about.linkedin.com?trk=d_news-guest-series_entity_footer-about)
- [Accessibility](https://www.linkedin.com/accessibility?trk=d_news-guest-series_entity_footer-accessibility)
- [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=d_news-guest-series_entity_footer-user-agreement)
- [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=d_news-guest-series_entity_footer-privacy-policy)
- [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=d_news-guest-series_entity_footer-cookie-policy)
- [Copyright Policy](https://www.linkedin.com/legal/copyright-policy?trk=d_news-guest-series_entity_footer-copyright-policy)
- [Brand Policy](https://brand.linkedin.com/policies?trk=d_news-guest-series_entity_footer-brand-policy)
- [Guest Controls](https://www.linkedin.com/psettings/guest-controls?trk=d_news-guest-series_entity_footer-guest-controls)
- [Community Guidelines](https://www.linkedin.com/legal/professional-community-policies?trk=d_news-guest-series_entity_footer-community-guide)
- - العربية (Arabic)
  - বাংলা (Bangla)
  - Čeština (Czech)
  - Dansk (Danish)
  - Deutsch (German)
  - Ελληνικά (Greek)
  - **English (English)**
  - Español (Spanish)
  - فارسی (Persian)
  - Suomi (Finnish)
  - Français (French)
  - हिंदी (Hindi)
  - Magyar (Hungarian)
  - Bahasa Indonesia (Indonesian)
  - Italiano (Italian)
  - עברית (Hebrew)
  - 日本語 (Japanese)
  - 한국어 (Korean)
  - मराठी (Marathi)
  - Bahasa Malaysia (Malay)
  - Nederlands (Dutch)
  - Norsk (Norwegian)
  - ਪੰਜਾਬੀ (Punjabi)
  - Polski (Polish)
  - Português (Portuguese)
  - Română (Romanian)
  - Русский (Russian)
  - Svenska (Swedish)
  - తెలుగు (Telugu)
  - ภาษาไทย (Thai)
  - Tagalog (Tagalog)
  - Türkçe (Turkish)
  - Українська (Ukrainian)
  - Tiếng Việt (Vietnamese)
  - 简体中文 (Chinese (Simplified))
  - 正體中文 (Chinese (Traditional))
    Language

# https://arpit.substack.com

Arpit’s Newsletter | Arpit Bhayani | Substack

![](https://substackcdn.com/image/fetch/w_264,c_limit,f_auto,q_auto:best,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1bfb1b0-50c2-4552-b7f7-c93c84ad0f14_1280x1280.png)

# Arpit’s Newsletter

A newsletter to help software engineers become better engineers and grow faster in their careers.

By Arpit Bhayani · Over 14,000 subscribersSubscribeNo thanksBy registering you agree to Substack's [Terms of Service](https://substack.com/tos), our [Privacy Policy](https://substack.com/privacy), and our [Information Collection Notice](https://substack.com/ccpa#personal-data-collected)

This site requires JavaScript to run correctly. Please [turn on JavaScript](https://enable-javascript.com/) or unblock scripts

# https://youtube.com/c/ArpitBhayani

# https://twitter.com/arpit_bhayani

x.com

# https://linkedin.com/in/arpitbhayani

# https://github.com/arpitbbhayani

arpitbbhayani (Arpit Bhayani) · GitHub

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Farpitbbhayani)

- Product

  - [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  - [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  - [Actions
    Automate any workflow](https://github.com/features/actions)
  - [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  - [Issues
    Plan and track work](https://github.com/features/issues)
  - [Code Review
    Manage code changes](https://github.com/features/code-review)
  - [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  - [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore

  - [All features](https://github.com/features)
  - [Documentation](https://docs.github.com)
  - [GitHub Skills](https://skills.github.com)
  - [Blog](https://github.blog)

- Solutions

  By company size

  - [Enterprises](https://github.com/enterprise)
  - [Small and medium teams](https://github.com/team)
  - [Startups](https://github.com/enterprise/startups)
  - [Nonprofits](/solutions/industry/nonprofits)
    By use case
  - [DevSecOps](/solutions/use-case/devsecops)
  - [DevOps](/solutions/use-case/devops)
  - [CI/CD](/solutions/use-case/ci-cd)
  - [View all use cases](/solutions/use-case)

  By industry

  - [Healthcare](/solutions/industry/healthcare)
  - [Financial services](/solutions/industry/financial-services)
  - [Manufacturing](/solutions/industry/manufacturing)
  - [Government](/solutions/industry/government)
  - [View all industries](/solutions/industry)

  [View all solutions](/solutions)

- Resources

  Topics

  - [AI](/resources/articles/ai)
  - [DevOps](/resources/articles/devops)
  - [Security](/resources/articles/security)
  - [Software Development](/resources/articles/software-development)
  - [View all](/resources/articles)

  Explore

  - [Learning Pathways](https://resources.github.com/learn/pathways)
  - [White papers, Ebooks, Webinars](https://resources.github.com)
  - [Customer Stories](https://github.com/customer-stories)
  - [Partners](https://partner.github.com)
  - [Executive Insights](https://github.com/solutions/executive-insights)

- Open Source

  - [GitHub Sponsors
    Fund open source developers](/sponsors)
  - [The ReadME Project
    GitHub community articles](https://github.com/readme)
    Repositories
  - [Topics](https://github.com/topics)
  - [Trending](https://github.com/trending)
  - [Collections](https://github.com/collections)

- Enterprise

  - [Enterprise platform
    AI-powered developer platform](/enterprise)
    Available add-ons
  - [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  - [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  - [Premium Support
    Enterprise-grade 24/7 support](/premium-support)

- [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

Cancel

Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

Cancel

Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Farpitbbhayani)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E&source=header)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

![@arpitbbhayani](https://avatars.githubusercontent.com/u/4745789?s=64&v=4)

**arpitbbhayani**
[Follow](/login?return_to=https%3A%2F%2Fgithub.com%2Farpitbbhayani)

[Overview](/arpitbbhayani)
[Repositories
191](/arpitbbhayani?tab=repositories)
[Projects
0](/arpitbbhayani?tab=projects)
[Packages
0](/arpitbbhayani?tab=packages)
[Stars
135](/arpitbbhayani?tab=stars)

More

- [Overview](/arpitbbhayani)
- [Repositories](/arpitbbhayani?tab=repositories)
- [Projects](/arpitbbhayani?tab=projects)
- [Packages](/arpitbbhayani?tab=packages)
- [Stars](/arpitbbhayani?tab=stars)

![@arpitbbhayani](https://avatars.githubusercontent.com/u/4745789?s=64&v=4)

**arpitbbhayani**

[Follow](/login?return_to=https%3A%2F%2Fgithub.com%2Farpitbbhayani)

[![View arpitbbhayani's full-sized avatar](https://avatars.githubusercontent.com/u/4745789?v=4)](https://avatars.githubusercontent.com/u/4745789?v=4)
🎲

building [@DiceDB](https://github.com/DiceDB)

Arpit Bhayani

# arpitbbhayani

🎲

building [@DiceDB](https://github.com/DiceDB)

[Follow](/login?return_to=https%3A%2F%2Fgithub.com%2Farpitbbhayani)

Creator of [@DiceDB](https://github.com/DiceDB) • ex-Google Dataproc, ex-Amazon Fast Data
[5.3k
followers](https://github.com/arpitbbhayani?tab=followers) · [3
following](https://github.com/arpitbbhayani?tab=following)

- [@DiceDB](https://github.com/DiceDB)
- Bangalore
- <https://arpitbhayani.me>
- X
  [@arpit_bhayani](https://twitter.com/arpit_bhayani)

## [Achievements](/arpitbbhayani?tab=achievements)

[![Achievement: Starstruck](https://github.githubassets.com/assets/starstruck-default-b6610abad518.png)x4](/arpitbbhayani?achievement=starstruck&tab=achievements)[![Achievement: Pull Shark](https://github.githubassets.com/assets/pull-shark-default-498c279a747d.png)x4](/arpitbbhayani?achievement=pull-shark&tab=achievements)[![Achievement: Pair Extraordinaire](https://github.githubassets.com/assets/pair-extraordinaire-default-579438a20e01.png)](/arpitbbhayani?achievement=pair-extraordinaire&tab=achievements)[![Achievement: Quickdraw](https://github.githubassets.com/assets/quickdraw-default-39c6aec8ff89.png)](/arpitbbhayani?achievement=quickdraw&tab=achievements)[![Achievement: YOLO](https://github.githubassets.com/assets/yolo-default-be0bbff04951.png)](/arpitbbhayani?achievement=yolo&tab=achievements)[![Achievement: Arctic Code Vault Contributor](https://github.githubassets.com/assets/arctic-code-vault-contributor-default-df8d74122a06.png)](/arpitbbhayani?achievement=arctic-code-vault-contributor&tab=achievements)

## [Achievements](/arpitbbhayani?tab=achievements)

[![Achievement: Starstruck](https://github.githubassets.com/assets/starstruck-default-b6610abad518.png)x4](/arpitbbhayani?achievement=starstruck&tab=achievements)[![Achievement: Pull Shark](https://github.githubassets.com/assets/pull-shark-default-498c279a747d.png)x4](/arpitbbhayani?achievement=pull-shark&tab=achievements)[![Achievement: Pair Extraordinaire](https://github.githubassets.com/assets/pair-extraordinaire-default-579438a20e01.png)](/arpitbbhayani?achievement=pair-extraordinaire&tab=achievements)[![Achievement: Quickdraw](https://github.githubassets.com/assets/quickdraw-default-39c6aec8ff89.png)](/arpitbbhayani?achievement=quickdraw&tab=achievements)[![Achievement: YOLO](https://github.githubassets.com/assets/yolo-default-be0bbff04951.png)](/arpitbbhayani?achievement=yolo&tab=achievements)[![Achievement: Arctic Code Vault Contributor](https://github.githubassets.com/assets/arctic-code-vault-contributor-default-df8d74122a06.png)](/arpitbbhayani?achievement=arctic-code-vault-contributor&tab=achievements)

Block or Report

# Block or report arpitbbhayani

**Block user**

Prevent this user from interacting with your repositories and sending you notifications.
Learn more about [blocking users](https://docs.github.com/articles/blocking-a-user-from-your-personal-account).

You must be logged in to block users.

Add an optional note:

Please don't include any personal information such as legal names or email addresses. Maximum 100 characters, markdown supported. This note will be visible to only you.

Block user

**Report abuse**

Contact GitHub support about this user’s behavior.
Learn more about [reporting abuse](https://docs.github.com/articles/reporting-abuse-or-spam).

[Report abuse](/contact/report-abuse?report=arpitbbhayani+%28user%29)

[Overview](/arpitbbhayani)
[Repositories
191](/arpitbbhayani?tab=repositories)
[Projects
0](/arpitbbhayani?tab=projects)
[Packages
0](/arpitbbhayani?tab=packages)
[Stars
135](/arpitbbhayani?tab=stars)

More

- [Overview](/arpitbbhayani)
- [Repositories](/arpitbbhayani?tab=repositories)
- [Projects](/arpitbbhayani?tab=projects)
- [Packages](/arpitbbhayani?tab=packages)
- [Stars](/arpitbbhayani?tab=stars)

[arpitbbhayani](/arpitbbhayani/arpitbbhayani)/README.md

I am a software engineer and an engineering leader passionate about System Architecture, Databases Internals, Language Internals, and Advanced Algorithms. I am the creator and maintainer of [DiceDB](https://github.com/dicedb/dice) - an open source, reactive, multi-tenant, cache optimized for modern hardware.

In 2024, I took the leap of faith and co-founded [Profile.fyi](https://profile.fyi), where I took care of all things tech and product. Before this, I was a Staff Engineer at [Google](https://cloud.google.com/) leading the [Dataproc](https://cloud.google.com/dataproc) India team in providing a managed big data ecosystem to GCP customers. I was also part of [Amazon's](https://www.amazon.com/) Fast Data Team and took care of cold tiering of hot data and providing a seamless query interface across all tiers.

I held engineering leadership positions (both IC and management) at [Unacademy](https://unacademy.com/), where I built, grew, and led Search, Site Reliability Engineering (SRE) teams, and Data Engineering teams. I hold a total of 10+ years of experience in scaling backend services, taking products and teams from 0 to 1, and beyond.

I keep diving deep into engineering details and share my learnings by across my [socials](https://twitter.com/arpit_bhayani) and videos on [YouTube](https://youtube.com/c/ArpitBhayani).

Pinned

## Loading

1. [DiceDB/dice](/DiceDB/dice) DiceDB/dice Public

   DiceDB is an open source, redis-compliant, reactive, scalable, highly-available, unified cache optimized for modern hardware.

   Go

   [7.6k](/DiceDB/dice/stargazers)
   [1.2k](/DiceDB/dice/forks)

2. [system-design-questions](/arpitbbhayani/system-design-questions) system-design-questions Public

   Problem statements on System Design and Software Architecture as part of Arpit's System Design Masterclass

   Python

   [2.2k](/arpitbbhayani/system-design-questions/stargazers)
   [469](/arpitbbhayani/system-design-questions/forks)

3. [obsidian-hackernews](/arpitbbhayani/obsidian-hackernews) obsidian-hackernews Public

   Periodically fetches and displays top stories from HackerNews.

   TypeScript

   [95](/arpitbbhayani/obsidian-hackernews/stargazers)
   [7](/arpitbbhayani/obsidian-hackernews/forks)

4. [flasksr](/arpitbbhayani/flasksr) flasksr Public

   Make flask pages load faster and better by streaming partial HTTP Responses 💥

   Python

   [155](/arpitbbhayani/flasksr/stargazers)
   [9](/arpitbbhayani/flasksr/forks)

5. [WikiSe](/arpitbbhayani/WikiSe) WikiSe Public

   A wikipedia search engine that is completely built in Java and works on Wikipedia XML dumps

   Java

   [73](/arpitbbhayani/WikiSe/stargazers)
   [6](/arpitbbhayani/WikiSe/forks)

6. [experimental](/arpitbbhayani/experimental) experimental Public

   Bunch of quick experiements and prototypes for build a deeper understanding of anything and everything.

   Jupyter Notebook

   [22](/arpitbbhayani/experimental/stargazers)

Something went wrong, please refresh the page to try again.

If the problem persists, check the [GitHub status page](https://www.githubstatus.com/)
or [contact support](/contact).

## Footer

© 2025 GitHub, Inc.

### Footer navigation

- [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
- [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
- [Security](https://github.com/security)
- [Status](https://www.githubstatus.com/)
- [Docs](https://docs.github.com/)
- [Contact](https://support.github.com?tags=dotcom-footer)
- Manage cookies
- Do not share my personal information

You can’t perform that action at this time.
