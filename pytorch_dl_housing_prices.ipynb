{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "    load_dotenv()\n",
    "\n",
    "    variables_to_check = []\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "# Adjust the random seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "class HousingPriceDataset:\n",
    "    \"\"\"Class to load and preprocess the housing dataset.\"\"\"\n",
    "    def __init__(self, csv_path, target_column, scale_factor=1_000_000):\n",
    "        # Load the dataset\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "\n",
    "        # Separate features and target\n",
    "        self.X = self.data.drop(columns=[target_column])\n",
    "        self.y = self.data[target_column] / scale_factor  # Scale target\n",
    "\n",
    "        # Identify categorical and numeric columns\n",
    "        categorical_cols = self.X.select_dtypes(include=['object']).columns\n",
    "        numeric_cols = self.X.select_dtypes(include=['number']).columns\n",
    "\n",
    "        # Preprocess the data\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numeric_cols),\n",
    "                ('cat', OneHotEncoder(), categorical_cols)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Fit and transform the training data, transform the testing data\n",
    "        self.X_train = torch.tensor(preprocessor.fit_transform(X_train), dtype=torch.float32)\n",
    "        self.X_test = torch.tensor(preprocessor.transform(X_test), dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "        self.y_test = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    def get_input_size(self):\n",
    "        return self.X_train.shape[1]\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        return self.X_train, self.y_train\n",
    "\n",
    "class HousingPriceModel(nn.Module):\n",
    "    \"\"\"Class to define the neural network model for predicting housing prices.\"\"\"\n",
    "    def __init__(self, dataset: HousingPriceDataset, training_config=None):\n",
    "        if training_config is None:\n",
    "            training_config = {\n",
    "                \"epochs\": 500,\n",
    "                \"learning_rate\": 0.01,\n",
    "                \"dropout_rate\": 0.5,\n",
    "                \"l2_lambda\": 0.01,\n",
    "                \"layers\": [64, 32],\n",
    "                \"activation\": \"ReLU\"\n",
    "            }\n",
    "        input_size = dataset.get_input_size()\n",
    "        self.X_train, self.y_train = dataset.get_train_data()\n",
    "        self.training_config = training_config\n",
    "        super(HousingPriceModel, self).__init__()\n",
    "\n",
    "        # Build the model dynamically based on config\n",
    "        layers = []\n",
    "        previous_size = input_size\n",
    "        for size in training_config[\"layers\"]:\n",
    "            layers.append(nn.Linear(previous_size, size))\n",
    "            if training_config[\"activation\"] == \"ReLU\":\n",
    "                layers.append(nn.ReLU())\n",
    "            elif training_config[\"activation\"] == \"Tanh\":\n",
    "                layers.append(nn.Tanh())\n",
    "            elif training_config[\"activation\"] == \"Sigmoid\":\n",
    "                layers.append(nn.Sigmoid())\n",
    "            layers.append(nn.Dropout(training_config[\"dropout_rate\"]))\n",
    "            previous_size = size\n",
    "        layers.append(nn.Linear(previous_size, 1))  # Output layer\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.cached_model = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def set_training_config(self, config):\n",
    "        self.training_config = deepcopy(config)\n",
    "\n",
    "    def get_training_config(self):\n",
    "        return deepcopy(self.training_config)\n",
    "\n",
    "    def train_model(self):\n",
    "        training_config = self.get_training_config()\n",
    "        epochs = training_config[\"epochs\"]\n",
    "        learning_rate = training_config[\"learning_rate\"]\n",
    "        l2_lambda = training_config[\"l2_lambda\"]\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            outputs = self(self.X_train)\n",
    "            loss = criterion(outputs, self.y_train)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print loss every 50 epochs\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Cache the trained model\n",
    "        self.cached_model = self.state_dict()\n",
    "\n",
    "    def _assert_model_state(self):\n",
    "        if self.cached_model is None:\n",
    "            print(\"Model is not trained. Training the model now...\")\n",
    "            self.train_model()\n",
    "\n",
    "    def predict_price(self, X):\n",
    "        self._assert_model_state()\n",
    "        self.eval() \n",
    "        with torch.no_grad():\n",
    "            prediction = self(X)\n",
    "        return prediction.item()\n",
    "\n",
    "    def batch_predict(self, X, y=None):\n",
    "        self._assert_model_state()\n",
    "        self.eval()  \n",
    "        metrics = {\n",
    "            \"loss\": None,\n",
    "            \"rms\": None,\n",
    "            \"mape\": None,\n",
    "            \"mae\": None\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            predictions = self(X)\n",
    "            if y is not None:\n",
    "                loss = nn.MSELoss()(predictions, y).item()\n",
    "                rms = torch.sqrt(torch.tensor(loss)).item()\n",
    "                mape = torch.mean(torch.abs((predictions - y) / y) * 100).item()\n",
    "\n",
    "                metrics[\"loss\"] = loss\n",
    "                metrics[\"rms\"] = rms\n",
    "                metrics[\"mape\"] = mape\n",
    "                metrics[\"mae\"] = nn.L1Loss()(predictions, y).item()\n",
    "        return predictions, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    csv_path,\n",
    "    target_column,\n",
    "    training_config={\n",
    "        \"epochs\": 500,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"l2_lambda\": 0.01,\n",
    "        \"layers\": [128, 64, 32],\n",
    "        \"activation\": \"ReLU\",\n",
    "    },\n",
    "):\n",
    "    \"\"\"Method to run the experiment\"\"\"\n",
    "    dataset = HousingPriceDataset(csv_path, target_column)\n",
    "    model = HousingPriceModel(dataset, training_config)\n",
    "    model.train_model()\n",
    "    predictions, metrics = model.batch_predict(dataset.X_test, dataset.y_test)\n",
    "    clear_output()\n",
    "\n",
    "    print(\"METRICS:\")\n",
    "    print(f\"MSE: {metrics['loss']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rms']:.4f}\")\n",
    "    print(f\"MAPE: {metrics['mape']:.4f}%\")\n",
    "    print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "\n",
    "    # Print sample predictions\n",
    "    print(\"Sample predictions vs actual values:\")\n",
    "    for i in range(5):\n",
    "        print(\n",
    "            f\"Predicted: {predictions[i].item():.2f}, Actual: {dataset.y_test[i].item():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Basic stuff.\n",
    "\n",
    "training_config = {\n",
    "        \"epochs\": 1000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"l2_lambda\": 0.01,\n",
    "        \"layers\": [128, 64],\n",
    "        \"activation\": \"ReLU\"\n",
    "    }\n",
    "\n",
    "csv_path = \"data/housing_prices/housing_prices.csv\"\n",
    "target_column = \"price\"  \n",
    "run_experiment(csv_path, target_column, training_config=training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: 1000 epochs, 3 layers, ReLU activation\n",
    "# This works the best. \n",
    "\n",
    "training_config = {\n",
    "        \"epochs\": 1000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"l2_lambda\": 0.01,\n",
    "        \"layers\": [128, 64, 32],\n",
    "        \"activation\": \"ReLU\"\n",
    "    }\n",
    "\n",
    "csv_path = \"data/housing_prices/housing_prices.csv\"\n",
    "target_column = \"price\"  \n",
    "run_experiment(csv_path, target_column, training_config=training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: 5x the epochs and add more hidden layers\n",
    "\n",
    "training_config = {\n",
    "    \"epochs\": 5000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"l2_lambda\": 0.01,\n",
    "    \"layers\": [128, 64, 32, 64, 128],\n",
    "    \"activation\": \"ReLU\",\n",
    "}\n",
    "\n",
    "csv_path = \"data/housing_prices/housing_prices.csv\"\n",
    "target_column = \"price\"\n",
    "run_experiment(csv_path, target_column, training_config=training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: 1000 epochs, 3 layers, ReLU activation, synthetic data with 1000 samples\n",
    "# This works the best. \n",
    "# Note: Adding synthetic data doesn't seem to help much.\n",
    "\n",
    "training_config = {\n",
    "        \"epochs\": 5000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"l2_lambda\": 0.01,\n",
    "        \"layers\": [128, 64, 32],\n",
    "        \"activation\": \"ReLU\"\n",
    "    }\n",
    "\n",
    "csv_path = \"data/housing_prices/augmented_housing_prices_large.csv\"\n",
    "target_column = \"price\"  \n",
    "run_experiment(csv_path, target_column, training_config=training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
