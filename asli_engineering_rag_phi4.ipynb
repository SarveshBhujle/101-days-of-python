{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Web Scraping and Knowledge Base Generation\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://files.oaiusercontent.com/file-Kr7R2nnbT8LsxigKKcNG5e?se=2025-01-20T11%3A43%3A46Z&sp=r&sv=2024-08-04&sr=b&rscc=max-age%3D604800%2C%20immutable%2C%20private&rscd=attachment%3B%20filename%3Daf8131b8-9867-4984-ba3c-c458804c136e.webp&sig=2Vv8Uze4TKWm%2BiHNE3hNxmusBCdJr1PcCwr6N719nzc%3D\"> \n",
    "</p>\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://files.oaiusercontent.com/file-ByUJDMwhofGn5MMgMCdAw2?se=2025-01-20T11%3A42%3A38Z&sp=r&sv=2024-08-04&sr=b&rscc=max-age%3D604800%2C%20immutable%2C%20private&rscd=attachment%3B%20filename%3Df9b97315-b42b-45e8-8b67-1b974115012f.webp&sig=agXVl/OOxnBRx9BYolHV66LQ/hLlJt/OtOyEIR2QIQ8%3D\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "- Automates the process of scraping `blog` and `knowledge base` links.\n",
    "\n",
    "- Fetches and formats the `content`, compiling it into a `markdown` based knowledge base.\n",
    "\n",
    "- Manages `environment setup`, installs required `dependencies`, and c\n",
    "onfigures necessary\n",
    " `environment variables`.\n",
    "\n",
    "- Fetches content from multiple URLs, including `YouTube transcripts`.\n",
    "\n",
    "- Converts content into `markdown` and stores it in a file.\n",
    "\n",
    "- Implements `caching` to optimize performance.\n",
    "\n",
    "- Includes `error handling` to manage issues during the scraping process.\n",
    "\n",
    "- Outlines future steps for optimization and integration with `AI systems`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Requirements Installation Script\n",
    "\n",
    "This script sets up the environment for the notebook by installing necessary dependencies and checking for required environment variables. It installs the dependencies from a `requirements.txt file`, retries up to three times in case of failure, and ensures that essential environment variables are set. If any variable is missing, the script exits and prompts the user to set it. Once the setup is complete, a success message is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = []\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv(override=True)\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "setup_env()\n",
    "print(\"🚀 Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Web Scraping and Content Extraction with Caching and YouTube Transcript Support\n",
    "\n",
    "This script scrapes web pages to extract links and content, converting the HTML into Markdown format. It caches the fetched links and content to optimize performance by preventing redundant requests. It also supports YouTube video transcription, retrieving and formatting the transcript using the `YouTubeTranscriptApi`. The script handles errors, retries failed requests, and processes multiple URLs in batches for efficient content extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Library Imports and Initial Setup\n",
    "The code starts by importing necessary libraries:\n",
    "\n",
    "- `BeautifulSoup (from bs4)`: A library used to parse and extract data from HTML documents.\n",
    "\n",
    "- `Requests`: Used to send HTTP requests to retrieve web pages.\n",
    "\n",
    "- `Typing (List)`: A hint for function arguments to indicate that they accept a list of items.\n",
    "\n",
    "- `Markdownify`: Converts HTML content to Markdown format.\n",
    "\n",
    "- `YouTube Transcript API`: Used to fetch and format YouTube video transcripts.\n",
    "\n",
    "- `Traceback`: A module to handle error traces and provide detailed error logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from typing import List\n",
    "import traceback\n",
    "from markdownify import markdownify as md\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Global Variables and Caching\n",
    "The script uses global variables for caching and configuration:\n",
    "\n",
    "- `cache`: A dictionary that stores previously fetched data (links and page content) to avoid redundant web requests.\n",
    "\n",
    "- `formatter`: An instance of TextFormatter from youtube_transcript_api, used to format YouTube video transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = TextFormatter()\n",
    "\n",
    "cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get Base URL\n",
    "This function extracts the base URL (scheme and domain) from a provided URL to resolve relative links.\n",
    "\n",
    "For example:\n",
    "\n",
    "For `https://example.com/path/to/page`, it returns `https://example.com`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_url(url):\n",
    "    return \"/\".join(url.split(\"/\")[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get Links From Page\n",
    "This function fetches all links (a tags) from a given webpage:\n",
    "\n",
    "- `Cache Checking`: If links from the page have already been cached, they are returned immediately to optimize performance.\n",
    "\n",
    "- `Sending Request`: A requests.get call fetches the content of the URL.\n",
    "\n",
    "- `Parsing HTML`: BeautifulSoup is used to parse the HTML and extract all hyperlinks (href attributes).\n",
    "\n",
    "- `Handling Relative Links`: If the link is relative, it's resolved to an absolute URL by appending it to the base URL of the page.\n",
    "\n",
    "- `Caching`: The links are cached to avoid redundant requests the next time they are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_page(url):\n",
    "    global cache\n",
    "    try:\n",
    "        cached_item = cache.get(url)\n",
    "        cached_urls = cached_item.get(\"urls\") if cached_item else None\n",
    "        if cached_urls:\n",
    "            print(f\"Returning cached links for {url}\")\n",
    "            return cached_urls, None\n",
    "        base_url = get_base_url(url)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = []\n",
    "        for link in soup.find_all(\"a\"):\n",
    "            cur_link = link.get(\"href\")\n",
    "            if not cur_link:\n",
    "                continue\n",
    "            if not cur_link.startswith(\"http\"):\n",
    "                links.append(f\"{base_url}{cur_link}\")\n",
    "            else:\n",
    "                links.append(cur_link)\n",
    "        if not cached_item:\n",
    "            cache[url] = {\n",
    "                \"urls\": links,\n",
    "                \"content\": None,\n",
    "                \"url\": url,\n",
    "            }\n",
    "        else:\n",
    "            cached_item[\"urls\"] = links\n",
    "            cache[url] = cached_item\n",
    "        return links, None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get links from {url}. Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return [], str(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Get Page Content\n",
    "This function retrieves the content of a given webpage and converts it to Markdown format:\n",
    "\n",
    "- `Cache Checking`: It first checks if the content for the page is already cached. If it is, it returns the cached content.\n",
    "\n",
    "- `YouTube Video Handling`: If the URL corresponds to a YouTube video, the function extracts the video ID, fetches the transcript using the YouTubeTranscriptApi, and formats the transcript using the TextFormatter.\n",
    "\n",
    "- `HTML Parsing`: For other URLs, it parses the HTML using BeautifulSoup and converts the HTML to Markdown format with the markdownify library.\n",
    "\n",
    "- `Caching`: The content is cached for future requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(url):\n",
    "    try:\n",
    "        cached_item = cache.get(url)\n",
    "        cached_content = cached_item.get(\"content\") if cached_item else None\n",
    "        if cached_content:\n",
    "            print(f\"Returning cached content for {url}\")\n",
    "            return cached_content, None\n",
    "        if \"youtube\" in url or \"youtu.be\" in url or \"youtube.com\" in url:\n",
    "            video_id = url.split(\"=\")[-1]\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            text_transcript = formatter.format_transcript(transcript)\n",
    "            return md(text_transcript), None\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        result = md(str(soup))\n",
    "        if not cached_item:\n",
    "            cache[url] = {\n",
    "                \"urls\": None,\n",
    "                \"content\": result,\n",
    "                \"url\": url,\n",
    "            }\n",
    "        else:\n",
    "            cached_item[\"content\"] = result\n",
    "            cache[url] = cached_item\n",
    "        return result, None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get content from {url}. Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"\", str(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Get Page Content Batch\n",
    "This function processes multiple URLs in a batch:\n",
    "\n",
    "It checks if each URL’s content is cached. If cached, it is returned; otherwise, the content is fetched and processed.\n",
    "It appends the results to a list, which is returned after processing all URLs in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content_batch(urls: List[str]):\n",
    "    results = []\n",
    "    for url in urls:\n",
    "        cached_item = cache.get(url)\n",
    "        cached_content = cached_item.get(\"content\") if cached_item else None\n",
    "        if cached_content:\n",
    "            print(f\"Returning cached content for {url}\")\n",
    "            results.append({\"url\": url, \"content\": cached_content, \"error\": None})\n",
    "            continue\n",
    "        print(f\"Getting content from {url}\")\n",
    "        content, error = get_page_content(url)\n",
    "        results.append({\"url\": url, \"content\": content, \"error\": error})\n",
    "        if not cached_item:\n",
    "            cache[url] = {\n",
    "                \"urls\": None,\n",
    "                \"content\": content,\n",
    "                \"url\": url,\n",
    "            }\n",
    "        else:\n",
    "            cached_item[\"content\"] = content\n",
    "            cache[url] = cached_item\n",
    "        result_bytes_count = len(content.encode(\"utf-8\"))\n",
    "        print(f\"Content from {url} fetched. Size: {result_bytes_count} bytes!\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Process of Fetching, Cleaning, and Storing Knowledge Base Content\n",
    "\n",
    "### 1. Fetching AE Blog Links\n",
    "\n",
    "- The function get_ae_blog_links() fetches the links of blogs from the base URL https://arpitbhayani.me/blogs.\n",
    "\n",
    "- It calls get_links_from_page() to fetch all the links from the page.\n",
    "\n",
    "- If successful, it returns the list of blog links; otherwise, it returns an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "knowledge_base_cache = {}\n",
    "\n",
    "\n",
    "def get_ae_blog_links():\n",
    "    blogs_base_url = \"https://arpitbhayani.me/blogs\"\n",
    "    blog_links = []\n",
    "    links, error = get_links_from_page(blogs_base_url)\n",
    "    if error:\n",
    "        return blog_links, error\n",
    "    blog_links.extend(links)\n",
    "    return blog_links, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2. Whitespace Cleaning\n",
    "\n",
    "- The function clean_whitespace() is designed to clean up unnecessary whitespaces in the text.\n",
    "\n",
    "- It performs the following tasks:\n",
    "    - Removes leading and trailing whitespaces.\n",
    "\n",
    "    - Replaces multiple whitespaces with a sing\n",
    "    le whitespace.\n",
    "\n",
    "    - Replaces multiple newlines with a single newline.\n",
    "\n",
    "    - Removes leading or trailing newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the whitespace as per following rules;\n",
    "    - Removes leading and trailing whitespaces.\n",
    "    - Replaces multiple whitespaces with a single whitespace.\n",
    "    - Replaces multiple newlines with a single newline.\n",
    "    - Removes any leading or trailing newlines.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    multiple_whitespaces_remover = \" \".join(text.split())\n",
    "    return (\n",
    "        multiple_whitespaces_remover.replace(\"\\n \", \"\\n\").replace(\" \\n\", \"\\n\").strip()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fetching Knowledge Base\n",
    "\n",
    "- The function fetch_knowledge_base() retrieves links from the knowledge-base page (https://arpitbhayani.me/knowledge-base).\n",
    "\n",
    "- It filters out links related to Google Drive and other irrelevant links.\n",
    "\n",
    "- For each valid link, it retrieves blog links and collects them in a list.\n",
    "\n",
    "- Finally, it calls get_page_content_batch() to fetch content from the collected links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_knowledge_base():\n",
    "    knowledge_base_link = \"https://arpitbhayani.me/knowledge-base\"\n",
    "    links_to_fetch = []\n",
    "    sub_page_links, error = get_links_from_page(knowledge_base_link)\n",
    "    if error:\n",
    "        return [], error\n",
    "    for link in sub_page_links:\n",
    "        is_knowledge_base_link = \"knowledge-base\" in link\n",
    "        if not is_knowledge_base_link:\n",
    "            continue\n",
    "        is_google_drive_link = \"drive.google.com\" in link\n",
    "        if is_google_drive_link:\n",
    "            continue\n",
    "        result_links = []\n",
    "        try:\n",
    "            blog_links, error = get_links_from_page(link)\n",
    "            if error:\n",
    "                print(f\"Failed to get links from {link}. Error: {error}\")\n",
    "                continue\n",
    "            result_links.extend(blog_links)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get links from {link}. Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "        links_to_fetch.extend(result_links)\n",
    "    result = get_page_content_batch(links_to_fetch)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building Knowledge Base and Saving to File\n",
    "\n",
    "- The build_knowledge_base() function assembles the complete knowledge base:\n",
    "\n",
    "    - It generates a timestamped output file name.\n",
    "\n",
    "    - It checks if the file exists, and if not, creates it and writes a header.\n",
    "\n",
    "    - It collects blog links and content by calling get_ae_blog_links() and fetch_knowledge_base().\n",
    "\n",
    "    - The collected content is written into the output file in Markdown format.\n",
    "\n",
    "    - If an error occurs, it logs the error and returns a message indicating the failure.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_base(output_file=\"ae_knowledge_base.md\"):\n",
    "    try:\n",
    "        now = datetime.now()\n",
    "        now_human_formtted = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "        output_file_name, extension = os.path.splitext(output_file)\n",
    "        output_file = f\"{output_file_name}_{now_human_formtted}{extension}\"\n",
    "        file_exists = os.path.exists(output_file)\n",
    "        if not file_exists:\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(\"# AE Knowledge Base\\n\\n\")\n",
    "        blog_links, error = get_ae_blog_links()\n",
    "        if error:\n",
    "            return [], error\n",
    "        all_content = []\n",
    "        content = get_page_content_batch(blog_links)\n",
    "        knowledge_base_content = fetch_knowledge_base()\n",
    "        all_content.extend(knowledge_base_content)\n",
    "        with open(output_file, \"w\") as f:\n",
    "            for c in all_content:\n",
    "                # c['content'] = clean_whitespace(c['content'])\n",
    "                f.write(f\"# {c['url']}\\n\")\n",
    "                f.write(c[\"content\"])\n",
    "                f.write(\"\\n\\n\")\n",
    "        return content, None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build knowledge base. Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return [], str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the Knowledge Base and Handle Errors:\n",
    "\n",
    "- ### Fetch AE Blog Links:\n",
    "\n",
    "    - get_ae_blog_links() fetches the links for AE blogs.\n",
    "\n",
    "    - Print the number of links found.\n",
    "\n",
    "- ### Fetch Knowledge Base Content:\n",
    "\n",
    "    - fetch_knowledge_base() fetches the content from the knowledge base.\n",
    "    \n",
    "    - Print the number of content sections fetched.\n",
    "\n",
    "- ### Build the Knowledge Base:\n",
    "\n",
    "    - build_knowledge_base() builds the knowledge base, writing the data into a markdown file (ae_knowledge_base_v0_0_2.md).\n",
    "\n",
    "    - Print success or error messages based on the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_links, error = get_ae_blog_links()\n",
    "\n",
    "print(f\"Found {len(ae_links)} links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_content = fetch_knowledge_base()\n",
    "\n",
    "print(f\"Fetched {len(knowledge_base_content)} links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"ae_knowledge_base_v0_0_2.md\"\n",
    "contents, error = build_knowledge_base(output_file=output_file)\n",
    "\n",
    "\n",
    "if error:\n",
    "    print(f\"Failed to build knowledge base. Error: {error}\")\n",
    "else:\n",
    "    print(f\"Knowledge base built successfully. Output file: {output_file}\")\n",
    "    print(f\"Content: {len(contents)} sections. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: `Things to do till this is complete`.\n",
    "- ##### Load and clean the knowledge base content.\n",
    "- ##### Load the knowledge base content into a vector store.\n",
    "- ##### Implement a simple RAG using Phi4 and the vector store.\n",
    "- ##### Implement any optimizations on the RAG to improve response quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! 🌐\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
